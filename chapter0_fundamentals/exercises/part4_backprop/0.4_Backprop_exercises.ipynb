{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6p2NBslqDjO"
      },
      "source": [
        "# [0.4] - Build Your Own Backpropagation Framework (exercises)\n",
        "\n",
        "> **ARENA [Streamlit Page](https://arena-chapter0-fundamentals.streamlit.app/04_[0.4]_Backprop)**\n",
        ">\n",
        "> **Colab: [exercises](https://colab.research.google.com/github/callummcdougall/ARENA_3.0/blob/main/chapter0_fundamentals/exercises/part4_backprop/0.4_Backprop_exercises.ipynb?t=20250218) | [solutions](https://colab.research.google.com/github/callummcdougall/ARENA_3.0/blob/main/chapter0_fundamentals/exercises/part4_backprop/0.4_Backprop_solutions.ipynb?t=20250218)**\n",
        "\n",
        "Please send any problems / bugs on the `#errata` channel in the [Slack group](https://join.slack.com/t/arena-uk/shared_invite/zt-2zick19fl-6GY1yoGaoUozyM3wObwmnQ), and ask any questions on the dedicated channels for this chapter of material.\n",
        "\n",
        "You can collapse each section so only the headers are visible, by clicking the arrow symbol on the left hand side of the markdown header cells.\n",
        "\n",
        "Links to all other chapters: [(0) Fundamentals](https://arena-chapter0-fundamentals.streamlit.app/), [(1) Transformer Interpretability](https://arena-chapter1-transformer-interp.streamlit.app/), [(2) RL](https://arena-chapter2-rl.streamlit.app/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79y6JAKIqDjS"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/headers/header-04.png\" width=\"350\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FqWmDQlqDjS"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVXMld5vqDjT"
      },
      "source": [
        "Today you're going to build your very own system that can run the backpropagation algorithm in essentially the same way as PyTorch does. By the end of the day, you'll be able to train a multi-layer perceptron neural network, using your own backprop system!\n",
        "\n",
        "The main differences between the full PyTorch and our version are:\n",
        "\n",
        "* We will focus on CPU only, as all the ideas are the same on GPU.\n",
        "* We will use NumPy arrays internally instead of ATen, the C++ array type used by PyTorch. Backpropagation works independently of the array type.\n",
        "* A real `torch.Tensor` has about 700 fields and methods. We will only implement a subset that are particularly instructional and/or necessary to train the MLP.\n",
        "\n",
        "Note - for today, I'd lean a lot more towards being willing to read the solutions, and even move on from some of them if you don't fully understand them (especially in the first half of section 3). The low-level messy implementation details for today are much less important than the high-level conceptual takeaways."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVRhgAKAqDjT"
      },
      "source": [
        "## Content & Learning Objectives\n",
        "\n",
        "### 1️⃣ Introduction to backprop\n",
        "\n",
        "This takes you through what a **computational graph** is, and the basics of how gradients can be backpropagated through such a graph. You'll also implement the backwards versions of some basic functions: if we have tensors `output = func(input)`, then the backward function of `func` can calculate the grad of `input` as a function of the grad of `output`.\n",
        "\n",
        "> ##### Learning Objectives\n",
        ">\n",
        "> * Understand what a computational graph is, and how it can be used to calculate gradients.\n",
        "> * Start to implement backwards versions of some basic functions.\n",
        "\n",
        "### 2️⃣ Autograd\n",
        "\n",
        "This section goes into more detail on the backpropagation methodology. In order to find the `grad` of each tensor in a computational graph, we first have to perform a **topological sort** of the tensors in the graph, so that each time we try to calculate `tensor.grad`, we've already computed all the other gradients which are used in this calculation. We end this section by writing a `backprop` function, which works just like the `tensor.backward()` method you're already used to in PyTorch.\n",
        "\n",
        "> ##### Learning Objectives\n",
        ">\n",
        "> * Perform a topological sort of a computational graph (and understand why this is important).\n",
        "> * Implement a the `backprop` function, to calculate and store gradients for all tensors in a computational graph.\n",
        "\n",
        "### 3️⃣ Training on MNIST from scratch\n",
        "\n",
        "In this section, we build your own equivalents of `torch.nn` features like `nn.Parameter`, `nn.Module`, and `nn.Linear`. We can then use these to build our own neural network to classify MINST data.\n",
        "\n",
        "This completes the chain which starts at basic numpy arrays, and ends with us being able to build essentially any neural network architecture we want!\n",
        "\n",
        "> ##### Learning Objectives\n",
        ">\n",
        "> * Implement more forward and backward functions, including for indexing, summing, and matrix multiplication\n",
        "> * Learn how to build higher-level abstractions like parameters and modules on top of individual functions and tensors\n",
        "> * Complete the process of building up a neural network from scratch and training it via gradient descent.\n",
        "\n",
        "### 4️⃣ Bonus\n",
        "\n",
        "A few bonus exercises are suggested, for pushing your understanding of backpropagation further."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqk71a_3qDjU"
      },
      "source": [
        "## Setup code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hivPJEr3qDjU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "\n",
        "chapter = \"chapter0_fundamentals\"\n",
        "repo = \"ARENA_3.0\"\n",
        "branch = \"main\"\n",
        "\n",
        "# Install dependencies\n",
        "try:\n",
        "    import jaxtyping\n",
        "except:\n",
        "    %pip install einops jaxtyping\n",
        "\n",
        "# Get root directory, handling 3 different cases: (1) Colab, (2) notebook not in ARENA repo, (3) notebook in ARENA repo\n",
        "root = (\n",
        "    \"/content\"\n",
        "    if IN_COLAB\n",
        "    else \"/root\"\n",
        "    if repo not in os.getcwd()\n",
        "    else str(next(p for p in Path.cwd().parents if p.name == repo))\n",
        ")\n",
        "\n",
        "if Path(root).exists() and not Path(f\"{root}/{chapter}\").exists():\n",
        "    if not IN_COLAB:\n",
        "        !sudo apt-get install unzip\n",
        "        %pip install jupyter ipython --upgrade\n",
        "\n",
        "    if not os.path.exists(f\"{root}/{chapter}\"):\n",
        "        !wget -P {root} https://github.com/callummcdougall/ARENA_3.0/archive/refs/heads/{branch}.zip\n",
        "        !unzip {root}/{branch}.zip '{repo}-{branch}/{chapter}/exercises/*' -d {root}\n",
        "        !mv {root}/{repo}-{branch}/{chapter} {root}/{chapter}\n",
        "        !rm {root}/{branch}.zip\n",
        "        !rmdir {root}/{repo}-{branch}\n",
        "\n",
        "\n",
        "if f\"{root}/{chapter}/exercises\" not in sys.path:\n",
        "    sys.path.append(f\"{root}/{chapter}/exercises\")\n",
        "\n",
        "os.chdir(f\"{root}/{chapter}/exercises\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4k5_HKiEqDjW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import sys\n",
        "import time\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Any, Callable, Iterable, Iterator\n",
        "\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "Arr = np.ndarray\n",
        "grad_tracking_enabled = True\n",
        "\n",
        "# Make sure exercises are in the path\n",
        "chapter = \"chapter0_fundamentals\"\n",
        "section = \"part4_backprop\"\n",
        "root_dir = next(p for p in Path.cwd().parents if (p / chapter).exists())\n",
        "exercises_dir = root_dir / chapter / \"exercises\"\n",
        "section_dir = exercises_dir / section\n",
        "if str(exercises_dir) not in sys.path:\n",
        "    sys.path.append(str(exercises_dir))\n",
        "\n",
        "\n",
        "import part4_backprop.tests as tests\n",
        "from part4_backprop.utils import get_mnist, visualize\n",
        "from plotly_utils import line"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4yi7yO_qDjW"
      },
      "source": [
        "<details>\n",
        "<summary>Help - I get a NumPy-related error</summary>\n",
        "\n",
        "This is an annoying colab-related issue which I haven't been able to find a satisfying fix for. If you restart runtime (but don't delete runtime), and run just the imports cell above again (but not the `%pip install` cell), the problem should go away.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-o5LsvP0qDjW"
      },
      "source": [
        "# 1️⃣ Introduction to backprop\n",
        "\n",
        "> ##### Learning Objectives\n",
        ">\n",
        "> * Understand what a computational graph is, and how it can be used to calculate gradients.\n",
        "> * Start to implement backwards versions of some basic functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZVXpX_4qDjX"
      },
      "source": [
        "## Reading\n",
        "\n",
        "* [Calculus on Computational Graphs: Backpropagation (Chris Olah)](https://colah.github.io/posts/2015-08-Backprop/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_teR3F5qDjX"
      },
      "source": [
        "## Computing Gradients with Backpropagation\n",
        "\n",
        "This section will briefly review the backpropagation algorithm, but focus mainly on the concrete implementation in software.\n",
        "\n",
        "To train a neural network, we want to know how the loss would change if we slightly adjust one of the learnable parameters.\n",
        "\n",
        "One obvious and straightforward way to do this would be just to add a small value  to the parameter, and run the forward pass again. This is called finite differences, and the main issue is we need to run a forward pass for every single parameter that we want to adjust. This method is infeasible for large networks, but it's important to know as a way of sanity checking other methods.\n",
        "\n",
        "A second obvious way is to write out the function for the entire network, and then symbolically take the gradient to obtain a symbolic expression for the gradient. This also works and is another thing to check against, but the expression gets quite complicated.\n",
        "\n",
        "Suppose that you have some **computational graph**, and you want to determine the derivative of the some scalar loss L with respect to NumPy arrays a, b, and c:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zT52fO5yqDjX"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/callummcdougall/Fundamentals/main/images/abc_de_L.png\" width=\"400\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psxcaRcvqDjX"
      },
      "source": [
        "This graph corresponds to the following Python:\n",
        "\n",
        "```python\n",
        "d = a * b\n",
        "e = b + c\n",
        "L = d + e\n",
        "```\n",
        "\n",
        "The goal of our system is that users can write ordinary looking Python code like this and have all the book-keeping needed to perform backpropagation happen behind the scenes. To do this, we're going to wrap each array and each function in special objects from our library that do the usual thing plus build up this graph structure that we need."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhWvlkSyqDjX"
      },
      "source": [
        "### Backward Functions\n",
        "\n",
        "We've drawn our computation graph from left to right and the arrows pointing to the right, so that in the forward pass, boxes to the right depend on boxes to the left. In the backwards pass, the opposite is true: the gradient of boxes on the left depends on the gradient of boxes on the right.\n",
        "\n",
        "If we want to compute the derivative of $L$ wrt all other variables (as was described in the reading), we should traverse the graph from right to left. Each time we encounter an instance of function application, we can use the chain rule from calculus to proceed one step further to the left. For example, if we have $d = a \\times b$, then:\n",
        "\n",
        "$$\n",
        "\\frac{dL}{da} = \\frac{dL}{dd}\\times \\frac{dd}{da} = \\frac{dL}{dd}\\times b\n",
        "$$\n",
        "\n",
        "Suppose we are working from right to left, trying to calculate $\\frac{dL}{da}$. If we already know the values of the variables $a$, $b$ and $d$, as well as the value of $\\frac{dL}{dd}$, then we can use the following function to find $\\frac{dL}{da}$:\n",
        "\n",
        "$$\n",
        "F\\left(a, b, d, \\frac{dL}{dd}\\right) = \\frac{dL}{dd}\\times b\n",
        "$$\n",
        "\n",
        "and we can do something similar when trying to calculate $\\frac{dL}{db}$.\n",
        "\n",
        "In other words, we can take the **\"forward function\"** $(a, b) \\to a \\cdot b$, and for each of its parameters, we can define an associated **\"backwards function\"** which tells us how to compute the gradient wrt this argument using only known quantities as inputs.\n",
        "\n",
        "Ignoring issues of unbroadcasting (which we'll cover later), we could write the backward with respect to the first argument as:\n",
        "\n",
        "```python\n",
        "def multiply_back(grad_out, out, a, b):\n",
        "    '''\n",
        "    Inputs:\n",
        "        grad_out = dL/d(out)\n",
        "        out = a * b\n",
        "\n",
        "    Returns:\n",
        "        dL/da\n",
        "    '''\n",
        "    return grad_out * b\n",
        "```\n",
        "\n",
        "where `grad_out` is the gradient of the output of the function with respect to the loss (i.e. $\\frac{dL}{dd}$), `out` is the output of the function (i.e. $d$), and `a` and `b` are our inputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zr3Xpv1TqDjY"
      },
      "source": [
        "### Topological Ordering\n",
        "\n",
        "When we're actually doing backprop, how do we guarantee that we'll always know the value of our backwards functions' inputs? For instance, in the example above we couldn't have computed $\\frac{dL}{da}$ without first knowing $\\frac{dL}{dd}$.\n",
        "\n",
        "The answer is that we sort all our nodes using an algorithm called [topological sorting](https://en.wikipedia.org/wiki/Topological_sorting), and then do our computations in this order. After each computation, we store the gradients in our nodes for use in subsequent calculations.\n",
        "\n",
        "When described in terms of the diagram above, topological sort can be thought of as an ordering of nodes from right to left. Crucially, this sorting has the following property: if there is a directed path in the computational graph going from node `x` to node `y`, then `x` must follow `y` in the sorting.\n",
        "\n",
        "There are many ways of proving that a cycle-free directed graph contains a topological ordering. You can try and prove this for yourself, or click on the expander below to reveal the outline of a simple proof."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKYicfsdqDjY"
      },
      "source": [
        "<details>\n",
        "<summary>Click to reveal proof</summary>\n",
        "\n",
        "We can prove by induction on the number of nodes $N$.\n",
        "    \n",
        "If $N=1$, the problem is trivial.\n",
        "\n",
        "If $N>1$, then pick any node, and follow the arrows until you reach a node with no directed arrows going out of it. Such a node must exist, or else you would be following the arrows forever, and you'd eventually return to a node you previously visited, but this would be a cycle, which is a contradiction. Once you've found this \"root node\", you can put it first in your topological ordering, then remove it from the graph and apply the topological sort on the subgraph created by removing this node. By induction, your topological sorting algorithm on this smaller graph should return a valid ordering. If you append the root node to the start of this ordering, you have a topological ordering for the whole graph.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4zHb1-wqDjY"
      },
      "source": [
        "A quick note on some potentially confusing terminology. We will refer to the \"end node\" as the **root node**, and the \"starting nodes\" as **leaf nodes**. For instance, in the diagram at the top of the section, the left nodes `a`, `b` and `c` are the leaf nodes, and `L` is the root node. This might seem odd given it makes the leaf nodes come before the root nodes, but the reason is as follows: *when we're doing the backpropagation algorithm, we start at `L` and work our way back through the graph*. So, by our notation, we start at the root node and work our way out to the leaves.\n",
        "\n",
        "Another important piece of terminology here is **parent node**. This means the same thing as it does in most other contexts - the parents of node `x` are all the nodes `y` with connections `y -> x` (so in the diagram, `L`'s parents are `d` and `e`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2XukyZcqDjY"
      },
      "source": [
        "<details>\n",
        "<summary>Question - can you think of a reason it might be important for a node to store a list of all of its parent nodes?</summary>\n",
        "\n",
        "During backprop, we're moving from right to left in the diagram. If a node doesn't store its parent, then there will be no way to get access to that parent node during backprop, so we can't propagate gradients to it.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GEY_2IuqDjY"
      },
      "source": [
        "The very first node in our topological sort will be $L$, the root node."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wodtTNRZqDjY"
      },
      "source": [
        "### Backpropagation\n",
        "\n",
        "After all this setup, the backpropagation mechanism becomes pretty straightforward. We sort the nodes topologically, then we iterate over them and call each backward function exactly once in order to accumulate the gradients at each node.\n",
        "\n",
        "It's important that the grads be accumulated instead of overwritten in a case like value $b$ which has two outgoing edges, since $\\frac{dL}{db}$ will then be the sum of two terms. Since addition is commutative it doesn't matter whether we `backward()` the Mul or the Add that depend on $b$ first.\n",
        "\n",
        "During backpropagation, for each forward function in our computational graph we need to find the partial derivative of the output with respect to each of its inputs. Each partial is then multiplied by the gradient of the loss with respect to the forward functions output (`grad_out`) to find the gradient of the loss with respect to each input. We'll handle these calculations using backward functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erQanxAwqDjY"
      },
      "source": [
        "## Backward function of log\n",
        "\n",
        "First, we'll write the backward function for `x -> out = log(x)`. This should be a function which, when fed the values `x, out, grad_out = dL/d(out)` returns the value of `dL/dx` just from this particular computational path."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htkKY3jHqDjZ"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/callummcdougall/Fundamentals/main/images/x_log_out.png\" width=\"400\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZYlBRwBqDjZ"
      },
      "source": [
        "Note - it might seem strange at first why we need `x` and `out` to be inputs, `out` can be calculated directly from `x`. The answer is that sometimes it is computationally cheaper to express the derivative in terms of `out` than in terms of `x`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OS8s8C_AqDjZ"
      },
      "source": [
        "<details>\n",
        "<summary>Question - can you think of an example function where it would be computationally cheaper to use 'out' than to use 'x'?</summary>\n",
        "\n",
        "The most obvious answer is the exponential function, `out = e ^ x`. Here, the gradient `d(out)/dx` is equal to `out`. We'll see this when we implement a backward version of `torch.exp` later today.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNfuFCCgqDjZ"
      },
      "source": [
        "### Exercise - implement `log_back`\n",
        "\n",
        "> ```yaml\n",
        "> Difficulty: 🔴🔴⚪⚪⚪\n",
        "> Importance: 🔵🔵🔵⚪⚪\n",
        ">\n",
        "> You should spend up to 5-10 minutes on this exercise.\n",
        "> ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kx3KS7J-qDjZ"
      },
      "source": [
        "You should fill in this function below. Don't worry about division by zero or other edge cases - the goal here is just to see how the pieces of the system fit together.\n",
        "\n",
        "*This should just be a short, one-line function.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uLLVbMqqDjZ",
        "outputId": "75b57a7e-de4d-4b07-c623-686097cd8e6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests in `test_log_back` passed!\n"
          ]
        }
      ],
      "source": [
        "def log_back(grad_out: Arr, out: Arr, x: Arr) -> Arr:\n",
        "    \"\"\"Backwards function for f(x) = log(x)\n",
        "\n",
        "    grad_out: Gradient of some loss wrt out\n",
        "    out: the output of np.log(x).\n",
        "    x: the input of np.log.\n",
        "\n",
        "    Return: gradient of the given loss wrt x\n",
        "    \"\"\"\n",
        "    return grad_out / x\n",
        "\n",
        "\n",
        "tests.test_log_back(log_back)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suWATbrlqDjZ"
      },
      "source": [
        "<details>\n",
        "<summary>Help - I'm not sure what the output of this backward function for log should be.</summary>\n",
        "\n",
        "By the chain rule, we have:\n",
        "\n",
        "$$\n",
        "\\frac{dL}{dx} = \\frac{dL}{d(\\text{out})} \\cdot \\frac{d(\\text{out})}{dx} = \\frac{dL}{d(\\text{out})} \\cdot \\frac{d(\\log{x})}{dx} = \\frac{dL}{d(\\text{out})} \\cdot \\frac{1}{x}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "(Note - technically, $\\frac{d(\\text{out})}{dx}$ is a tensor containing the derivatives of each element of $\\text{out}$ with respect to each element of $x$, and we should matrix multiply when we use the chain rule. However, since $\\text{out} = \\log x$ is an elementwise function of $x$, our application of the chain rule here will also be an elementwise multiplication: $\\frac{dL}{dx_{ij}} = \\frac{dL}{d(\\text{out}_{ij})} \\cdot \\frac{d(\\text{out}_{ij})}{dx_{ij}}$. When we get to things like matrix multiplication later, we'll have to be a bit more careful!)\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Help - I get <code>ImportError: numpy.core.multiarray failed to import</code></summary>\n",
        "\n",
        "This is an annoying colab-related error which I haven't been able to find a satisfying fix for. The setup code at the top of this notebook should have installed a version of numpy which works, although you'll have to click \"Restart Runtime\" (from the Runtime menu) to make this work. Make sure you don't re-run the cell with `pip install`s.\n",
        "\n",
        "If this still doesn't work, please flag it in the Slack channel errata.\n",
        "</details>\n",
        "\n",
        "\n",
        "<details><summary>Solution</summary>\n",
        "\n",
        "```python\n",
        "def log_back(grad_out: Arr, out: Arr, x: Arr) -> Arr:\n",
        "    \"\"\"Backwards function for f(x) = log(x)\n",
        "\n",
        "    grad_out: Gradient of some loss wrt out\n",
        "    out: the output of np.log(x).\n",
        "    x: the input of np.log.\n",
        "\n",
        "    Return: gradient of the given loss wrt x\n",
        "    \"\"\"\n",
        "    return grad_out / x\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8tgt1bCqDja"
      },
      "source": [
        "## Backward functions of two tensors\n",
        "\n",
        "Now we'll implement backward functions for multiple tensors. From this point onwards, we'll be working with vector-valued derivatives, i.e. terms like $\\frac{\\partial x}{\\partial y}$ where $y$ (and possibly $x$) are tensors. To recap what this notation means - these terms are tensors where each value is the scalar derivative of one of the elements of $x$ wrt one of the elements of $y$. For example:\n",
        "\n",
        "- If $x$ is a scalar and $y$ is a tensor with shape `(3, 4)`, then $\\frac{\\partial x}{\\partial y}$ is a tensor with shape `(3, 4)` where the `[i, j]`-th element is $\\frac{\\partial x}{\\partial y[i, j]}$, i.e. the derivative of $x$ wrt a particular element of $y$.\n",
        "- If $x$ is a length-5 vector and $y$ is a tensor with shape `(3, 4)` then $\\frac{\\partial x}{\\partial y}$ is a tensor with shape `(5, 3, 4)` where the `[i, j, k]`-th element is $\\frac{\\partial x[i]}{\\partial y[j, k]}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgVLCJEEqDja"
      },
      "source": [
        "### Broadcasting\n",
        "\n",
        "Before we go through the next exercises, we'll need to address one important topic in tensor operations - **broadcasting**. We've discussed it in earlier exercises, but we're reviewing it here because it's very important for backprop. If you're comfortable with the topic, feel free to jump forwards to the section \"Why do we need broadcasting for backprop?\".\n",
        "\n",
        "Both NumPy and PyTorch have the same rules for broadcasting. When two tensors are involved in an elementwise operation, NumPy/PyTorch tries to broadcast them (i.e. copying them along dimensions) so that they both have the same shape. The rules of broadcasting are as follows:\n",
        "\n",
        "1. You can prepend dummy dimensions (of size 1) to the start of a tensor until both have the same number of dimensions\n",
        "2. After this point, if some dimension has size 1 in one of the tensors, it can be repeated until it matches the size of the corresponding dimension in the other tensor\n",
        "\n",
        "To give a simple example - suppose we have a 2D batch of data, of shape `data.shape = (N, k)` (i.e. we have `N` separate datapoints, each being a vector of length `k`). Suppose we want to add a vector `vec` of length `k` to each datapoint. This is a valid operation, because when we try and add these two objects together:\n",
        "\n",
        "1. `vec` gets prepended with a dummy dimension so it has shape `(1, k)` and both are 2D\n",
        "2. `vec` gets repeated along the first dimension so it has shape `(N, k)`, matching the shape of `data`\n",
        "\n",
        "Then, our output has shape `(N, k)`, and elements `output[i, j] = data[i, j] + vec[j]`.\n",
        "\n",
        "Broadcasting can be a very easy place to make mistakes, because it's easy to lose track of the exact shape of your tensors involved. As a warm-up exercise, below are some examples of broadcasting. Can you figure out which are valid, and which will raise errors?\n",
        "\n",
        "\n",
        "```python\n",
        "x = np.ones((3, 1, 5))\n",
        "y = np.ones((1, 4, 5))\n",
        "\n",
        "z = x + y\n",
        "```\n",
        "\n",
        "<details>\n",
        "<summary>Answer</summary>\n",
        "\n",
        "This is valid, because the 0th dimension of `y` and the 1st dimension of `x` can both be copied so that `x` and `y` have the same shape: `(3, 4, 5)`. The resulting array `z` will also have shape `(3, 4, 5)`.\n",
        "\n",
        "This example illustrates an important point - it's not always the case that one of the tensors is strictly smaller and the other is strictly bigger. Sometimes, both tensors will get expanded.\n",
        "\n",
        "</details>\n",
        "\n",
        "```python\n",
        "x = np.ones((8, 2, 6))\n",
        "y = np.ones((8, 2))\n",
        "\n",
        "z = x + y\n",
        "```\n",
        "\n",
        "<details>\n",
        "<summary>Answer</summary>\n",
        "\n",
        "This is not valid. We first need to expand `y` by appending a dimension to the front, and the last two dimensions of `x` are `(2, 6)`, which won't broadcast with `y`'s `(8, 2)`.\n",
        "</details>\n",
        "\n",
        "```python\n",
        "x = np.ones((8, 2, 6))\n",
        "y = np.ones((2, 6))\n",
        "\n",
        "z = x + y\n",
        "```\n",
        "\n",
        "<details>\n",
        "<summary>Answer</summary>\n",
        "\n",
        "This is valid. Once NumPy expands `y` by appending a single dimension to the front, it can then be broadcast with `x`.\n",
        "</details>\n",
        "\n",
        "```python\n",
        "x = np.ones((10, 20, 30))\n",
        "y = np.ones((20, 1))\n",
        "\n",
        "z = x + y\n",
        "```\n",
        "\n",
        "<details>\n",
        "<summary>Answer</summary>\n",
        "\n",
        "This is valid. Once NumPy expands `y` by appending a single dimension to the front, it can then be broadcast with `x` (this will involve copying along the first and last dimensions).\n",
        "</details>\n",
        "\n",
        "```python\n",
        "x = np.ones((4, 1))\n",
        "y = np.ones((4,))\n",
        "\n",
        "z = x + y\n",
        "```\n",
        "\n",
        "<details>\n",
        "<summary>Answer</summary>\n",
        "\n",
        "This is valid. Numpy will expand the second one to `(1, 4)` then broadcast them both to `(4, 4)`.\n",
        "\n",
        "Although this won't raise an error, it's very possible that this isn't what the person adding these two arrays intended. A common source of mistakes is when you add 2 tensors thinking they're the same shape, but one actually has a dummy dimension you forgot about. Sadly this is something you'll just have to be vigilant for (e.g. adding assert statements where necessary, or making sure you aren't combining too many different tensor operations in a single line), because PyTorch doesn't have built-in ways of statically checking your tensor shapes.\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shfYWG-2qDja"
      },
      "source": [
        "### Why do we need broadcasting for backprop?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khDk4Yq9qDja"
      },
      "source": [
        "Often, operations like `out = f(x, y)` involve an implicit broadcasting step. For example, if `out = x + y` with `x.shape = (4,)` and `y.shape = (3, 4)`, then really our operation has 2 steps:\n",
        "\n",
        "1. Broadcast `x` to a broadcasted version `x_b` which has the shape of `y`, i.e. copy it along the zeroth dimension to have shape `(3, 4)`,\n",
        "2. Define `out = x_b + y`, which is an operation that doesn't involve broadcasting.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/broadcast-last-1.png\" width=\"600\">\n",
        "\n",
        "How can we go from the gradient `dL/d(x_b)` (which might be easy to compute given it involves no broadcasting) to the gradient of `dL/dx`? The answer is that **we take `dL/d(x_b)` and sum it over the dimensions along which `x` was broadcasted to get `dL/dx`.**\n",
        "\n",
        "For the mathematically inclined this can be proved fairly easily (we provide a sketch of the proof below). But let's focus on the intuition here. When we copy `x` along axes to create `x_b`, we're essentially creating multiple pathways for each element of `x` to affect the final loss: one path for each time the element of `x` was copied. So when we change some element of `x`, this causes a first-order change in the loss from all of these different pathways, and we have to sum these changes to get the total first-order change (i.e. the derivative). For example:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/broadcast-last-2.png\" width=\"540\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfhULbniqDja"
      },
      "source": [
        "> ##### Summary\n",
        ">\n",
        "> If we know $\\frac{\\partial L}{\\partial (\\mathbf{out})}$, and want to know $\\frac{\\partial L}{\\partial \\mathbf{x}}$ (where $x$ was broadcasted to produce $\\text{out}$) then there are two steps:\n",
        ">\n",
        "> 1. Compute $\\frac{\\partial L}{\\partial \\mathbf{x_b}}$ in the standard way, i.e. using one of your backward functions (assuming no broadcasting happened).\n",
        "> 2. ***Unbroadcast*** $\\frac{\\partial L}{\\partial \\mathbf{x_b}}$, by summing it over the dimensions along which $\\mathbf{x}$ was broadcasted.\n",
        "\n",
        "\n",
        "<details>\n",
        "<summary>Mathematical derivation (sketch)</summary>\n",
        "\n",
        "We start with:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial \\mathbf{x}} = \\sum_{i_1, i_2, ...} \\frac{\\partial L}{\\partial x_b[i_1, i_2, ...]} \\cdot \\frac{\\partial x_b[i_1, i_2, ...]}{\\partial \\mathbf{x}}\n",
        "$$\n",
        "\n",
        "using the chain rule. Next, we can use the fact that $\\frac{\\partial x_b[i_1, i_2, ...]}{\\partial x[j_1, j_2, ...]}$ equals 0 everywhere except the indices where $x$ was broadcasted to produce $x_b$, where it equals 1. Therefore, this sum over indices $i_1, i_2, ...$ is equivalent to just summing $\\frac{\\partial L}{\\partial x_b[i_1, i_2, ...]}$ over the repeated elements of $x$.\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LaRflUlqDja"
      },
      "source": [
        "We used the term \"unbroadcast\" because the way that our tensor's shape changes will be the reverse of how it changed during broadcasting. If `x` was broadcasted from `(4,) -> (3, 4)`, then unbroadcasting will have to take a tensor of shape `(3, 4)` and sum over it to return a tensor of shape `(4,)`. Similarly, if `x` was broadcasted from `(1, 4) -> (3, 4)`, then we need to sum over the zeroth dimension, but leave it as a 2D tensor with a leading dummy dimension of size 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHP9n8wcqDjb"
      },
      "source": [
        "### Exercise - implement `unbroadcast`\n",
        "\n",
        "> ```yaml\n",
        "> Difficulty: 🔴🔴🔴⚪⚪\n",
        "> Importance: 🔵🔵🔵⚪⚪\n",
        ">\n",
        "> You should spend up to 10-25 minutes on this exercise.\n",
        "> This function can be quite fiddly - if you're stuck, read and understand the solution code instead.\n",
        "> ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_TlfCq-qDjb"
      },
      "source": [
        "The `unbroadcast` function below should take any value like $\\frac{\\partial L}{\\partial x_b}$ and return $\\frac{\\partial L}{\\partial x}$ (where the arguments `original` and `broadcasted` represent $x$ and $x_b$ respectively). In the same way that broadcasting `original` to the shape of `broadcasted` can be described as a 2-step process:\n",
        "\n",
        "1. Extend dummy dimensions (size 1) of `original` to match the last dimensions of `broadcasted`,\n",
        "2. Prepend dimensions to `original` until it has the same ndims as `broadcasted`,\n",
        "\n",
        "the unbroadcasting process is the same process in reverse:\n",
        "\n",
        "1. Sum over prepended dimensions of `broadcasted` until it has the same ndims as `original`,\n",
        "2. Now they have the same ndims, sum over dimensions of `broadcasted` where `original` had size 1.\n",
        "\n",
        "For example, if `original.shape = (3, 1)` and `broadcasted.shape = (1, 2, 3, 4)` then the broadcasting steps look like `(3, 1) -> (3, 4) -> (1, 2, 3, 4)` (copying along those dimensions at each step) and the unbroadcasting steps look like `(1, 2, 3, 4) -> (3, 4) -> (3, 1)` (summing over those dimensions at each step)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOcbT5yjqDjb",
        "outputId": "d8ab4afa-685d-4836-ce9b-8576cf083adf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests in `test_unbroadcast` passed!\n"
          ]
        }
      ],
      "source": [
        "def unbroadcast(broadcasted: Arr, original: Arr) -> Arr:\n",
        "    \"\"\"\n",
        "    Sum 'broadcasted' until it has the shape of 'original'.\n",
        "\n",
        "    broadcasted: An array that was formerly of the same shape of 'original' and was expanded by broadcasting rules.\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE: sum over `broadcasted` until it has the shape of `original`\n",
        "\n",
        "    # Step 1: sum and remove prepended dims, so both arrays have same number of dims\n",
        "    n_dims_to_sum = len(broadcasted.shape) - len(original.shape)\n",
        "    broadcasted = broadcasted.sum(axis=tuple(range(n_dims_to_sum)))\n",
        "\n",
        "    # Step 2: sum over dims which were originally 1 (but don't remove them)\n",
        "    dims_to_sum = tuple([i for i, (o, b) in enumerate(zip(original.shape, broadcasted.shape)) if o == 1 and b > 1])\n",
        "    broadcasted = broadcasted.sum(axis=dims_to_sum, keepdims=True)\n",
        "    assert broadcasted.shape == original.shape\n",
        "    return broadcasted\n",
        "\n",
        "\n",
        "tests.test_unbroadcast(unbroadcast)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yoh6FZwqDjb"
      },
      "source": [
        "<details><summary>Solution</summary>\n",
        "\n",
        "```python\n",
        "def unbroadcast(broadcasted: Arr, original: Arr) -> Arr:\n",
        "    \"\"\"\n",
        "    Sum 'broadcasted' until it has the shape of 'original'.\n",
        "\n",
        "    broadcasted: An array that was formerly of the same shape of 'original' and was expanded by broadcasting rules.\n",
        "    \"\"\"\n",
        "    # Step 1: sum and remove prepended dims, so both arrays have same number of dims\n",
        "    n_dims_to_sum = len(broadcasted.shape) - len(original.shape)\n",
        "    broadcasted = broadcasted.sum(axis=tuple(range(n_dims_to_sum)))\n",
        "\n",
        "    # Step 2: sum over dims which were originally 1 (but don't remove them)\n",
        "    dims_to_sum = tuple([i for i, (o, b) in enumerate(zip(original.shape, broadcasted.shape)) if o == 1 and b > 1])\n",
        "    broadcasted = broadcasted.sum(axis=dims_to_sum, keepdims=True)\n",
        "\n",
        "    assert broadcasted.shape == original.shape\n",
        "    return broadcasted\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCH8G5TWqDjg"
      },
      "source": [
        "### Backward Function for Elementwise Multiply\n",
        "\n",
        "Functions that are differentiable with respect to more than one input tensor are straightforward given that we already know how to handle broadcasting.\n",
        "\n",
        "- We're going to have two backwards functions, one for each input argument.\n",
        "- If the input arguments were broadcasted together to create a larger output, the incoming `grad_out` will be of the larger common broadcasted shape and we need to make use of `unbroadcast` from earlier to match the shape to the appropriate input argument.\n",
        "- We'll want our backward function to work when one of the inputs is an float. We won't need to calculate the grad_in with respect to floats, so we only need to consider when y is an float for `multiply_back0` and when x is an float for `multiply_back1`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiIPYEuzqDjh"
      },
      "source": [
        "### Exercise - implement both `multiply_back` functions\n",
        "\n",
        "> ```yaml\n",
        "> Difficulty: 🔴🔴🔴⚪⚪\n",
        "> Importance: 🔵🔵⚪⚪⚪\n",
        ">\n",
        "> You should spend up to 10-15 minutes on these exercises.\n",
        "> ```\n",
        "\n",
        "Below, you should implement both `multiply_back0` and `multiply_back1`.\n",
        "\n",
        "You might be wondering why we need two different functions, rather than just having a single function to serve both purposes. This will become more important later on, once we deal with functions with more than one argument, which is not symmetric in its arguments. For instance, the derivative of $x / y$ wrt $x$ is not the same as the expression you get after differentiating this wrt $y$ then swapping the labels around.\n",
        "\n",
        "The first part of each function has been provided for you (this makes sure that both inputs are arrays, since we want to support multiplication by floats or scalars)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qD4eTXoPqDjh",
        "outputId": "2defc205-12ff-45ee-dcf0-1c059b3a1c74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests in `test_multiply_back` passed!\n",
            "All tests in `test_multiply_back_float` passed!\n"
          ]
        }
      ],
      "source": [
        "def multiply_back0(grad_out: Arr, out: Arr, x: Arr, y: Arr | float) -> Arr:\n",
        "    \"\"\"Backwards function for x * y wrt argument 0 aka x.\"\"\"\n",
        "    if not isinstance(y, Arr):\n",
        "        y = np.array(y)\n",
        "    return unbroadcast(y * grad_out, x)\n",
        "\n",
        "\n",
        "def multiply_back1(grad_out: Arr, out: Arr, x: Arr | float, y: Arr) -> Arr:\n",
        "    \"\"\"Backwards function for x * y wrt argument 1 aka y.\"\"\"\n",
        "    if not isinstance(x, Arr):\n",
        "        x = np.array(x)\n",
        "\n",
        "    return unbroadcast(x * grad_out, y)\n",
        "\n",
        "\n",
        "tests.test_multiply_back(multiply_back0, multiply_back1)\n",
        "tests.test_multiply_back_float(multiply_back0, multiply_back1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xB4-AmawqDjh"
      },
      "source": [
        "<details>\n",
        "<summary>Hint</summary>\n",
        "\n",
        "Using the chain rule for scalar functions, we can find that:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial (xy)} \\cdot \\frac{\\partial (xy)}{\\partial x} = \\frac{\\partial L}{\\partial (xy)} \\cdot y\n",
        "$$\n",
        "\n",
        "An equivalent result\\* holds for vector-valued inputs:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial \\mathbf{x}_b} = \\frac{\\partial L}{\\partial (\\mathbf{x}_b \\cdot \\mathbf{y}_b)} \\cdot \\mathbf{y}_b\n",
        "$$\n",
        "\n",
        "where $\\cdot$ is the elementwise multiplication operator, and $\\mathbf{x}_b, \\mathbf{y}_b$ are the broadcasted versions of the input tensors. This allows us to compute $\\frac{\\partial L}{\\partial \\mathbf{x}_b}$ based on the inputs to our `multiply_back0` function. Can you use `unbroadcast` to finish the calculation?\n",
        "\n",
        "\\*You can derive this result for yourself using the chain rule. Note that application of the chain rule here would require you to sum over the elements of the intermediate tensor $\\mathbf{x}_b \\cdot \\mathbf{y}_b$, but we're working with elementwise operations so $\\partial (\\mathbf{x}_b \\cdot \\mathbf{y}_b)[i_1, i_2, ...] / \\partial \\mathbf{x}_b$ will have all elements zero except for one and the sum falls out, leaving us with the result above.\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Solution (and explanation)</summary>\n",
        "\n",
        "Based on the discussion in hint 1, we can conclude that the value $\\partial L / \\partial \\mathbf{x}_b$ is equal to `y_b * grad_out`. Note that this is just equivalent to `y * grad_out` because `y` will be broadcasted to the shape of `y_b` when we multiply it with `grad_out` (since we know `grad_out` has the same shape as the broadcasted `x * y` tensor).\n",
        "\n",
        "Now, we need to go from this broadcasted derivative to the shape of `x`, and we do this by applying `unbroadcast`, summing `y * grad_out` over dimensions so that it's the same shape as `x` - in other words, we return `unbroadcast(y * grad_out, x)`.\n",
        "\n",
        "The output of `multiply_back1` is the same as `multiply_back0`, except the roles of `x` and `y` are reversed.\n",
        "\n",
        "```python\n",
        "def multiply_back0(grad_out: Arr, out: Arr, x: Arr, y: Arr | float) -> Arr:\n",
        "    \"\"\"Backwards function for x * y wrt argument 0 aka x.\"\"\"\n",
        "    if not isinstance(y, Arr):\n",
        "        y = np.array(y)\n",
        "\n",
        "    return unbroadcast(y * grad_out, x)\n",
        "\n",
        "\n",
        "def multiply_back1(grad_out: Arr, out: Arr, x: Arr | float, y: Arr) -> Arr:\n",
        "    \"\"\"Backwards function for x * y wrt argument 1 aka y.\"\"\"\n",
        "    if not isinstance(x, Arr):\n",
        "        x = np.array(x)\n",
        "\n",
        "    return unbroadcast(x * grad_out, y)\n",
        "```\n",
        "\n",
        "<!-- Derivation: first we compute the gradient of the broadcasted tensor, using the chain rule:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial x_b[i_1, i_2, ...]} = \\frac{\\partial L}{\\partial (x_b \\cdot y_b)[i_1, i_2, ...]} \\cdot \\frac{\\partial (x_b \\cdot y_b)[i_1, i_2, ...]}{\\partial x_b[i_1, i_2, ...]}\n",
        "$$\n",
        "\n",
        "where we've avoided the sum over indices of $x_b \\cdot y_b$ because this is an elementwise operation, and so those gradients must be zero everywhere except where all $i_k = j_k$. We can further write this as:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial x_b[i_1, i_2, ...]} = \\text{grad\\_out}[i_1, i_2, ...] \\cdot y[i_1, i_2, ...]\n",
        "$$\n",
        "\n",
        "which gives us the result for broadcasted tensors! Note that although this tensor is `grad_out * y_b`, we can compute it as `grad_out * y` because we know that `y` is broadcastable to the shape of `grad_out` (since this is also the same as the shape of `out`).\n",
        "\n",
        "Once we have this result `grad_out * y`, we just use `unbroadcast` to sum over the dimensions `x` was broadcasted along.  -->\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEuKctWRqDji"
      },
      "source": [
        "Now we'll use our backward functions to do backpropagation manually, for the following computational graph:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/callummcdougall/Fundamentals/main/images/abcdefg.png\" width=550>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZMGoowEqDji"
      },
      "source": [
        "### Exercise - implement `forward_and_back`\n",
        "\n",
        "> ```yaml\n",
        "> Difficulty: 🔴🔴🔴⚪⚪\n",
        "> Importance: 🔵🔵🔵🔵⚪\n",
        ">\n",
        "> You should spend up to 15-20 minutes on these exercises.\n",
        "> This function is very useful for getting a sense for what backprop looks like in practice.\n",
        "> ```\n",
        "\n",
        "Below, you should implement the `forward_and_back` function. This is an opportunity for you to practice using the backward functions you've written so far, and should hopefully give you a better sense of how the full backprop function will eventually work. Note, we assume\n",
        "\n",
        "Note - you might be wondering what `grad_out` should be the first time you call a backward function. We've so far assumed that the final node `L` in the computational graph is a scalar, and all gradients (i.e. the `grad_out` input to our backward functions and the output of our backward functions) are gradients of `L`. In this case, it would make sense to set `grad_out = [1]` for the first backward function call, since we're computing $\\frac{\\partial L}{\\partial L} = 1$. But what about cases like the one above, when our final node `g` might not be a scalar?\n",
        "\n",
        "The answer is that we effectively add a final scalar node into our graph, defined as `L = (g * v).sum()` for some tensor `v` of the same shape as `g`. This is called the **directional derivative**. We then compute all tensor gradients using `L` as our final node. For example, this means that in our first gradient calculation (where `g` is the output and `f` is the input) we'll use `grad_out = dL/dg = v`. You don't really need to worry about this, because most of the time (including in the exercise below) we'll just be using the default behaviour where `v` is a tensor of 1s - this is equivalent to `L = g.sum()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6FFh_gPqDji",
        "outputId": "2880bb9f-cf90-4814-8994-3d9030be030e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests in `test_forward_and_back` passed!\n"
          ]
        }
      ],
      "source": [
        "def forward_and_back(a: Arr, b: Arr, c: Arr) -> tuple[Arr, Arr, Arr]:\n",
        "    \"\"\"\n",
        "    Calculates the output of the computational graph above (g), then backpropogates the gradients and returns dg/da,\n",
        "    dg/db, and dg/dc.\n",
        "    \"\"\"\n",
        "    d = a * b\n",
        "    e = np.log(c)\n",
        "    f = d * e\n",
        "    g = np.log(f)\n",
        "    final_grad_out = np.ones_like(g)\n",
        "\n",
        "    # YOUR CODE HERE - use your backward functions to compute the gradients of g wrt a, b, and c\n",
        "    dg_df = log_back(grad_out=final_grad_out, out=g, x=f)\n",
        "    dg_dd = multiply_back0(dg_df, f, d, e)\n",
        "    dg_de = multiply_back1(dg_df, f, d, e)\n",
        "    dg_da = multiply_back0(dg_dd, d, a, b)\n",
        "    dg_db = multiply_back1(dg_dd, d, a, b)\n",
        "    dg_dc = log_back(dg_de, e, c)\n",
        "    return (dg_da, dg_db, dg_dc)\n",
        "\n",
        "\n",
        "tests.test_forward_and_back(forward_and_back)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dso5eVlqDji"
      },
      "source": [
        "<details><summary>Solution</summary>\n",
        "\n",
        "```python\n",
        "def forward_and_back(a: Arr, b: Arr, c: Arr) -> tuple[Arr, Arr, Arr]:\n",
        "    \"\"\"\n",
        "    Calculates the output of the computational graph above (g), then backpropogates the gradients and returns dg/da,\n",
        "    dg/db, and dg/dc.\n",
        "    \"\"\"\n",
        "    d = a * b\n",
        "    e = np.log(c)\n",
        "    f = d * e\n",
        "    g = np.log(f)\n",
        "    final_grad_out = np.ones_like(g)\n",
        "\n",
        "    dg_df = log_back(grad_out=final_grad_out, out=g, x=f)\n",
        "    dg_dd = multiply_back0(dg_df, f, d, e)\n",
        "    dg_de = multiply_back1(dg_df, f, d, e)\n",
        "    dg_da = multiply_back0(dg_dd, d, a, b)\n",
        "    dg_db = multiply_back1(dg_dd, d, a, b)\n",
        "    dg_dc = log_back(dg_de, e, c)\n",
        "\n",
        "    return (dg_da, dg_db, dg_dc)\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9xk9WsJqDji"
      },
      "source": [
        "In the next section, you'll build up to full automation of this backpropagation process, in a way that's similar to PyTorch's `autograd`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzUl6Pu8qDji"
      },
      "source": [
        "# 2️⃣ Autograd\n",
        "\n",
        "> ##### Learning Objectives\n",
        ">\n",
        "> * Perform a topological sort of a computational graph (and understand why this is important).\n",
        "> * Implement a the `backprop` function, to calculate and store gradients for all tensors in a computational graph."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IVkJH0AqDjj"
      },
      "source": [
        "Now, rather than figuring out which backward functions to call, in what order, and what their inputs should be, we'll write code that takes care of that for us. We'll implement this with a few major components:\n",
        "\n",
        "- `Tensor`, which is a wrapper around numpy arrays which is equivalent to PyTorch's `Tensor` class\n",
        "- `Recipe`, which tracks the extra information needed to run backpropagation (mainly how this tensor was created from other tensors)\n",
        "- `wrap_forward_fn`, which takes a numpy function mapping arrays to arrays (e.g. `np.log`) and returns a new function that maps tensors to tensors (while also creating the recipe for the new tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fGFo4kyqDjj"
      },
      "source": [
        "## Wrapping Arrays (Tensor)\n",
        "\n",
        "We're going to wrap each array with a wrapper object from our library which we'll call `Tensor` because it's going to behave similarly to a `torch.Tensor`.\n",
        "\n",
        "Each Tensor that is created by one of our forward functions will have a `Recipe`, which tracks the extra information need to run backpropagation.\n",
        "\n",
        "`wrap_forward_fn` will take a forward function and return a new forward function that does the same thing while recording the info we need to do backprop in the `Recipe`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsHUzpANqDjj"
      },
      "source": [
        "## Recipe\n",
        "\n",
        "Let's start by taking a look at `Recipe`.\n",
        "\n",
        "`@dataclass` is a handy class decorator that sets up an `__init__` function for the class that takes the provided attributes as arguments and sets them as you'd expect.\n",
        "\n",
        "The class `Recipe` is designed to track the forward functions in our computational graph, so that gradients can be calculated during backprop. Each tensor created by a forward function has its own `Recipe`. We're naming it this because it is a set of instructions that tell us which ingredients went into making our tensor: what the function was, and what tensors were used as input to the function to produce this one as output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "i7phVGBgqDjj"
      },
      "outputs": [],
      "source": [
        "@dataclass(frozen=True)\n",
        "class Recipe:\n",
        "    \"\"\"Extra information necessary to run backpropagation. You don't need to modify this.\"\"\"\n",
        "\n",
        "    func: Callable\n",
        "    \"The 'inner' NumPy function that does the actual forward computation.\"\n",
        "    \"Note, we call it 'inner' to distinguish it from the wrapper we'll create for it later on.\"\n",
        "\n",
        "    args: tuple\n",
        "    \"The input arguments passed to func.\"\n",
        "    \"For instance, if func was np.sum then args would be a length-1 tuple containing the tensor to be summed.\"\n",
        "\n",
        "    kwargs: dict[str, Any]\n",
        "    \"Keyword arguments passed to func.\"\n",
        "    \"For instance, if func was np.sum then kwargs might contain 'dim' and 'keepdims'.\"\n",
        "\n",
        "    parents: dict[int, \"Tensor\"]\n",
        "    \"Map from positional argument index to the Tensor at that position, in order to be able to pass gradients back along the computational graph.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LbwYUGIqDjj"
      },
      "source": [
        "Note that `args` just stores the values of the underlying arrays, but `parents` stores the actual tensors. This is because they serve two different purposes: `args` is required for computing the value of gradients during backpropagation, and `parents` is required to infer the structure of the computational graph (i.e. which tensors were used to produce which other tensors).\n",
        "\n",
        "Here are some examples, to build intuition for what the four fields of `Recipe` are, and why we need all four of them to fully describe a tensor in our graph and how it was created. Make sure you understand each of these examples before moving on, because it'll really help you progress quickly through the following exercises.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/recipe-v2.png\" width=\"800\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfWBcFHPqDjj"
      },
      "source": [
        "## Registering backwards functions\n",
        "\n",
        "The `Recipe` takes care of tracking the forward functions in our computational graph, but we still need a way to find the backward function corresponding to a given forward function when we do backprop (or possibly the set of backward functions, if the forward function takes more than one argument)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jk-pZhk4qDjk"
      },
      "source": [
        "### Exercise - implement `BackwardFuncLookup`\n",
        "\n",
        "> ```yaml\n",
        "> Difficulty: 🔴🔴⚪⚪⚪\n",
        "> Importance: 🔵🔵🔵⚪⚪\n",
        ">\n",
        "> You should spend up to 10-15 minutes on these exercises.\n",
        "> These exercises should be very short, once you understand what is being asked.\n",
        "> ```\n",
        "\n",
        "We will define a class `BackwardFuncLookup` in order to find the backward function for a given forward function. The implementation details are left up to you - all that matters is that you pass the test code in the cell below. Reading this test code should explain how the `BackwardFuncLookup` class needs to be used - for any given forward function e.g. `np.log`, we need to be able to add a set of backward functions for each of its positional arguments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsM0QXTeqDjk",
        "outputId": "da545636-d52b-40cf-ec87-0aa6a07181db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tests passed - BackwardFuncLookup class is working as expected!\n"
          ]
        }
      ],
      "source": [
        "class BackwardFuncLookup:\n",
        "    def __init__(self) -> None:\n",
        "        self.back_funcs = {}  # each entry is a tuple of (forward_fn, arg_position) -> back_fn\n",
        "\n",
        "    def add_back_func(self, forward_fn: Callable, arg_position: int, back_fn: Callable) -> None:\n",
        "        self.back_funcs[(forward_fn, arg_position)] = back_fn\n",
        "\n",
        "    def get_back_func(self, forward_fn: Callable, arg_position: int) -> Callable:\n",
        "        return self.back_funcs[(forward_fn, arg_position)]\n",
        "\n",
        "\n",
        "BACK_FUNCS = BackwardFuncLookup()\n",
        "\n",
        "BACK_FUNCS.add_back_func(np.log, 0, log_back)\n",
        "BACK_FUNCS.add_back_func(np.multiply, 0, multiply_back0)\n",
        "BACK_FUNCS.add_back_func(np.multiply, 1, multiply_back1)\n",
        "\n",
        "assert BACK_FUNCS.get_back_func(np.log, 0) == log_back\n",
        "assert BACK_FUNCS.get_back_func(np.multiply, 0) == multiply_back0\n",
        "assert BACK_FUNCS.get_back_func(np.multiply, 1) == multiply_back1\n",
        "\n",
        "print(\"Tests passed - BackwardFuncLookup class is working as expected!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqUywIS9qDjk"
      },
      "source": [
        "<details>\n",
        "<summary>Help - I'm stuck on this implementation</summary>\n",
        "\n",
        "You can define a dict like `self.back_funcs` in the `__init__` method. When you add / retrieve a function, you can use the tuple `(forward_fn, arg_position)` as a key, and the backward function as the value.\n",
        "\n",
        "</details>\n",
        "\n",
        "\n",
        "<details><summary>Solution</summary>\n",
        "\n",
        "```python\n",
        "class BackwardFuncLookup:\n",
        "    def __init__(self) -> None:\n",
        "        self.back_funcs = {}  # each entry is a tuple of (forward_fn, arg_position) -> back_fn\n",
        "\n",
        "    def add_back_func(self, forward_fn: Callable, arg_position: int, back_fn: Callable) -> None:\n",
        "        self.back_funcs[(forward_fn, arg_position)] = back_fn\n",
        "\n",
        "    def get_back_func(self, forward_fn: Callable, arg_position: int) -> Callable:\n",
        "        return self.back_funcs[(forward_fn, arg_position)]\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8o0e7kN9qDjk"
      },
      "source": [
        "## Tensors\n",
        "\n",
        "Our Tensor object has these fields:\n",
        "\n",
        "- An `array` field of type `np.ndarray`. These are the actual tensor values.\n",
        "- A `requires_grad` field of type `bool`. This determines whether we need to compute gradients for this tensor (note this doesn't necessarily mean we need to store them, see below).\n",
        "- A `grad` field of the same size and type as the value. This is where gradients are stored.\n",
        "- A `recipe` field, as we've already seen. A tensor has a recipe if and only if it was created by some operation on other tensors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9vKacfGqDjk"
      },
      "source": [
        "### `requires_grad` and `is_leaf`\n",
        "\n",
        "The meaning of `requires_grad` is that when doing operations using this tensor, the recipe will be stored and it and any descendents will be included in the computational graph. Note that `requires_grad` does not necessarily mean that we will save the accumulated gradients to this tensor's `.grad` parameter when doing backprop - for example we require gradients to propagate through the hidden activations of a neural network to get back to grads for our model weights, but we don't need to actually store the gradients of the hidden activations.\n",
        "\n",
        "We use `is_leaf` to differentiate between these cases (see the method `Tensor.is_leaf` defined below) - a leaf tensor is one that represents the end of a backprop path, either because it doesn't require gradients or it has no nodes further back in the computational graph which require gradients. So our backprop algorithm will always terminate at a leaf node, then store that leaf node's gradient as `.grad` only if `requires_grad` is true.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/is_leaf.png\" width=\"900\">\n",
        "\n",
        "You can investigate this by running the following code:\n",
        "\n",
        "```python\n",
        "layer = torch.nn.Linear(3, 4)\n",
        "input = torch.ones(3)\n",
        "output = layer(input)\n",
        "\n",
        "print(layer.weight.is_leaf)       # -> True\n",
        "print(layer.weight.requires_grad) # -> True\n",
        "\n",
        "print(output.is_leaf)             # -> False\n",
        "print(output.requires_grad)       # -> True\n",
        "\n",
        "print(input.is_leaf)              # -> True\n",
        "print(input.requires_grad)        # -> False\n",
        "```\n",
        "\n",
        "When creating tensors, we can set `requires_grad` explicitly (e.g. it's false by default for most tensors, but is true by default if that tensor is wrapped in `torch.nn.Parameter` - we'll create our own version of this later). When creating a tensor from another tensor or tensors, `requires_grad` is true if and only if both of the following 3 conditions hold:\n",
        "\n",
        "1. Global grad tracking is enabled. In this notebook we've represented this with the global variable `grad_tracking_enabled`, but in PyTorch this is done with `torch.set_grad_enabled(False)`. This is useful because when you're looking at a model in inference mode, gradient tracking can waste memory and it's useful to disable it (we'll do this a lot next week, when we study transformer interpretability).\n",
        "2. At least one of the input tensors requires grad (since this is equivalent to \"there are other tensors further upstream which we need to get gradients for\").\n",
        "3. The function is differentiable (if not, obviously we can't compute gradients)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSKcKsWYqDjl"
      },
      "source": [
        "Now, we're giving you the full `Tensor` class. Most of these methods are currently undefined, and you'll go on to define them in later exercises (so you won't need to write any code this class). For now, just pay attention to the docstring & `__init__` methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "RJAk4WzgqDjl"
      },
      "outputs": [],
      "source": [
        "Arr = np.ndarray\n",
        "\n",
        "\n",
        "class Tensor:\n",
        "    \"\"\"\n",
        "    A drop-in replacement for torch.Tensor supporting a subset of features.\n",
        "    \"\"\"\n",
        "\n",
        "    array: Arr\n",
        "    \"The underlying array. Can be shared between multiple Tensors.\"\n",
        "    requires_grad: bool\n",
        "    \"If True, calling functions or methods on this tensor will track relevant data for backprop.\"\n",
        "    grad: \"Tensor | None\"\n",
        "    \"Backpropagation will accumulate gradients into this field.\"\n",
        "    recipe: \"Recipe | None\"\n",
        "    \"Extra information necessary to run backpropagation.\"\n",
        "\n",
        "    def __init__(self, array: Arr | list, requires_grad=False):\n",
        "        self.array = array if isinstance(array, Arr) else np.array(array)\n",
        "        if self.array.dtype == np.float64:\n",
        "            self.array = self.array.astype(np.float32)\n",
        "        self.requires_grad = requires_grad\n",
        "        self.grad = None\n",
        "        self.recipe = None\n",
        "        \"If not None, this tensor's array was created via recipe.func(*recipe.args, **recipe.kwargs).\"\n",
        "\n",
        "    def __neg__(self) -> \"Tensor\":\n",
        "        return negative(self)\n",
        "\n",
        "    def __add__(self, other) -> \"Tensor\":\n",
        "        return add(self, other)\n",
        "\n",
        "    def __radd__(self, other) -> \"Tensor\":\n",
        "        return add(other, self)\n",
        "\n",
        "    def __sub__(self, other) -> \"Tensor\":\n",
        "        return subtract(self, other)\n",
        "\n",
        "    def __rsub__(self, other) -> \"Tensor\":\n",
        "        return subtract(other, self)\n",
        "\n",
        "    def __mul__(self, other) -> \"Tensor\":\n",
        "        return multiply(self, other)\n",
        "\n",
        "    def __rmul__(self, other):\n",
        "        return multiply(other, self)\n",
        "\n",
        "    def __truediv__(self, other):\n",
        "        return true_divide(self, other)\n",
        "\n",
        "    def __rtruediv__(self, other):\n",
        "        return true_divide(other, self)\n",
        "\n",
        "    def __matmul__(self, other):\n",
        "        return matmul(self, other)\n",
        "\n",
        "    def __rmatmul__(self, other):\n",
        "        return matmul(other, self)\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        return eq(self, other)\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return f\"Tensor({repr(self.array)}, requires_grad={self.requires_grad})\"\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        if self.array.ndim == 0:\n",
        "            raise TypeError\n",
        "        return self.array.shape[0]\n",
        "\n",
        "    def __hash__(self) -> int:\n",
        "        return id(self)\n",
        "\n",
        "    def __getitem__(self, index) -> \"Tensor\":\n",
        "        return getitem(self, index)\n",
        "\n",
        "    def add_(self, other: \"Tensor\", alpha: float = 1.0) -> \"Tensor\":\n",
        "        add_(self, other, alpha=alpha)\n",
        "        return self\n",
        "\n",
        "    def sub_(self, other: \"Tensor\", alpha: float = 1.0) -> \"Tensor\":\n",
        "        sub_(self, other, alpha=alpha)\n",
        "        return self\n",
        "\n",
        "    def __iadd__(self, other: \"Tensor\") -> \"Tensor\":\n",
        "        self.add_(other)\n",
        "        return self\n",
        "\n",
        "    def __isub__(self, other: \"Tensor\") -> \"Tensor\":\n",
        "        self.sub_(other)\n",
        "        return self\n",
        "\n",
        "    @property\n",
        "    def T(self) -> \"Tensor\":\n",
        "        return permute(self, axes=(-1, -2))\n",
        "\n",
        "    def item(self):\n",
        "        return self.array.item()\n",
        "\n",
        "    def sum(self, dim=None, keepdim=False) -> \"Tensor\":\n",
        "        return sum(self, dim=dim, keepdim=keepdim)\n",
        "\n",
        "    def log(self) -> \"Tensor\":\n",
        "        return log(self)\n",
        "\n",
        "    def exp(self) -> \"Tensor\":\n",
        "        return exp(self)\n",
        "\n",
        "    def reshape(self, new_shape) -> \"Tensor\":\n",
        "        return reshape(self, new_shape)\n",
        "\n",
        "    def permute(self, dims) -> \"Tensor\":\n",
        "        return permute(self, dims)\n",
        "\n",
        "    def maximum(self, other) -> \"Tensor\":\n",
        "        return maximum(self, other)\n",
        "\n",
        "    def relu(self) -> \"Tensor\":\n",
        "        return relu(self)\n",
        "\n",
        "    def argmax(self, dim=None, keepdim=False) -> \"Tensor\":\n",
        "        return argmax(self, dim=dim, keepdim=keepdim)\n",
        "\n",
        "    def uniform_(self, low: float, high: float) -> \"Tensor\":\n",
        "        self.array[:] = np.random.uniform(low, high, self.array.shape)\n",
        "        return self\n",
        "\n",
        "    def backward(self, end_grad: \"Arr | Tensor | None\" = None) -> None:\n",
        "        if isinstance(end_grad, Arr):\n",
        "            end_grad = Tensor(end_grad)\n",
        "        return backprop(self, end_grad)\n",
        "\n",
        "    def size(self, dim: int | None = None):\n",
        "        if dim is None:\n",
        "            return self.shape\n",
        "        return self.shape[dim]\n",
        "\n",
        "    @property\n",
        "    def shape(self):\n",
        "        return self.array.shape\n",
        "\n",
        "    @property\n",
        "    def ndim(self):\n",
        "        return self.array.ndim\n",
        "\n",
        "    @property\n",
        "    def is_leaf(self):\n",
        "        \"\"\"Same as https://pytorch.org/docs/stable/generated/torch.Tensor.is_leaf.html\"\"\"\n",
        "        if self.requires_grad and self.recipe and self.recipe.parents:\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    def __bool__(self):\n",
        "        if np.array(self.shape).prod() != 1:\n",
        "            raise RuntimeError(\"bool value of Tensor with more than one value is ambiguous\")\n",
        "        return bool(self.item())\n",
        "\n",
        "\n",
        "def empty(*shape: int) -> Tensor:\n",
        "    \"\"\"Like torch.empty.\"\"\"\n",
        "    return Tensor(np.empty(shape))\n",
        "\n",
        "\n",
        "def zeros(*shape: int) -> Tensor:\n",
        "    \"\"\"Like torch.zeros.\"\"\"\n",
        "    return Tensor(np.zeros(shape))\n",
        "\n",
        "\n",
        "def arange(start: int, end: int, step=1) -> Tensor:\n",
        "    \"\"\"Like torch.arange(start, end).\"\"\"\n",
        "    return Tensor(np.arange(start, end, step=step))\n",
        "\n",
        "\n",
        "def tensor(array: Arr, requires_grad=False) -> Tensor:\n",
        "    \"\"\"Like torch.tensor.\"\"\"\n",
        "    return Tensor(array, requires_grad=requires_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zY1ILtNqDjl"
      },
      "source": [
        "## Forward Pass: Building the Computational Graph\n",
        "\n",
        "Let's start with a simple case: our `log` function. `log_forward` is a wrapper, which should implement the functionality of `np.log` but work with tensors rather than arrays."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XHk7uAxqDjl"
      },
      "source": [
        "### Exercise - implement `log_forward`\n",
        "\n",
        "> ```yaml\n",
        "> Difficulty: 🔴🔴🔴⚪⚪\n",
        "> Importance: 🔵🔵🔵⚪⚪\n",
        ">\n",
        "> You should spend up to 15-20 minutes on this exercise.\n",
        "> ```\n",
        "\n",
        "Your `log` function should be a wrapper around `np.log`, which takes and returns a `Tensor` object rather than numpy arrays. You can refer to the first of the five diagrams at the start of the \"Recipe\" section if you're stuck.\n",
        "\n",
        "Some more hints / tips:\n",
        "\n",
        "- As a reminder, `requires_grad` is true if both global gradient tracking is enabled (i.e. `grad_tracking_enabled` is true) and at least one of the inputs has `requires_grad` true.\n",
        "- You should also set the recipe for the new tensor, if `requires_grad` is true (if not then you can just set the recipe to None).\n",
        "\n",
        "Later we'll write code to wrap numpy functions in a generic and reusable way, but for now we just want to get this working for `np.log`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2r9lXQ5ZqDjm",
        "outputId": "23a29506-df36-4d3c-873b-c52a6a99f34c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests in `test_log` passed!\n",
            "All tests in `test_log_no_grad` passed!\n"
          ]
        }
      ],
      "source": [
        "def log_forward(x: Tensor) -> Tensor:\n",
        "    \"\"\"Performs np.log on a Tensor object.\"\"\"\n",
        "    # Get the function output (as a numpy array)\n",
        "    array = np.log(x.array)\n",
        "\n",
        "    # Find whether the tensor requires grad\n",
        "    requires_grad = grad_tracking_enabled and x.requires_grad\n",
        "\n",
        "    # Create the tensor\n",
        "    out = Tensor(array, requires_grad)\n",
        "\n",
        "    # Set the recipe (if we need it)\n",
        "    if requires_grad:\n",
        "        out.recipe = Recipe(func=np.log, args=(x.array,), kwargs={}, parents={0: x})\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "log = log_forward\n",
        "tests.test_log(Tensor, log_forward)\n",
        "tests.test_log_no_grad(Tensor, log_forward)\n",
        "a = Tensor([1], requires_grad=True)\n",
        "grad_tracking_enabled = False\n",
        "b = log_forward(a)\n",
        "grad_tracking_enabled = True\n",
        "assert not b.requires_grad, \"should not require grad if grad tracking globally disabled\"\n",
        "assert b.recipe is None, \"should not create recipe if grad tracking globally disabled\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYgM0ebTqDjm"
      },
      "source": [
        "<details><summary>Solution</summary>\n",
        "\n",
        "```python\n",
        "def log_forward(x: Tensor) -> Tensor:\n",
        "    \"\"\"Performs np.log on a Tensor object.\"\"\"\n",
        "    # Get the function output (as a numpy array)\n",
        "    array = np.log(x.array)\n",
        "\n",
        "    # Find whether the tensor requires grad\n",
        "    requires_grad = grad_tracking_enabled and x.requires_grad\n",
        "\n",
        "    # Create the tensor\n",
        "    out = Tensor(array, requires_grad)\n",
        "\n",
        "    # Set the recipe (if we need it)\n",
        "    if requires_grad:\n",
        "        out.recipe = Recipe(func=np.log, args=(x.array,), kwargs={}, parents={0: x})\n",
        "\n",
        "    return out\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3RX88hdqDjm"
      },
      "source": [
        "Now let's do the same for multiply, to see how to handle functions with multiple arguments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcJGaXeFqDjm"
      },
      "source": [
        "### Exercise - implement `multiply_forward`\n",
        "\n",
        "> ```yaml\n",
        "> Difficulty: 🔴🔴🔴🔴⚪\n",
        "> Importance: 🔵🔵🔵⚪⚪\n",
        ">\n",
        "> You should spend up to 15-20 minutes on this exercise.\n",
        "> ```\n",
        "\n",
        "There are a few differences between this and log:\n",
        "\n",
        "- The actual function to be called is different\n",
        "- We need more than one argument in `args` and `parents`, when defining `Recipe`\n",
        "- `requires_grad` should be true if `grad_tracking_enabled=True`, and ANY of the input tensors require grad\n",
        "- One of the inputs may be an int, so you'll need to deal with this case before calculating `out`\n",
        "\n",
        "If you're confused, you can scroll up to the diagram at the top of the page (which tells you how to construct the recipe for functions like multiply or add when they are both arrays, or when one is an array and the other is a scalar)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXHjOhkcqDjm",
        "outputId": "789a213a-f9dd-4823-d409-ee4cd8e9d1c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests in `test_multiply` passed!\n",
            "All tests in `test_multiply_no_grad` passed!\n",
            "All tests in `test_multiply_float` passed!\n"
          ]
        }
      ],
      "source": [
        "def multiply_forward(a: Tensor | int, b: Tensor | int) -> Tensor:\n",
        "    \"\"\"Performs np.multiply on a Tensor object.\"\"\"\n",
        "    assert isinstance(a, Tensor) or isinstance(b, Tensor)\n",
        "\n",
        "    # Get all function arguments as non-tensors (i.e. either ints or arrays)\n",
        "    arg_a = a.array if isinstance(a, Tensor) else a\n",
        "    arg_b = b.array if isinstance(b, Tensor) else b\n",
        "\n",
        "    # Calculate the output (which is a numpy array)\n",
        "    out_arr = arg_a * arg_b\n",
        "    assert isinstance(out_arr, np.ndarray)\n",
        "\n",
        "    # Find whether the tensor requires grad (need to check if ANY of the inputs do)\n",
        "    requires_grad = grad_tracking_enabled and any([isinstance(x, Tensor) and x.requires_grad for x in (a, b)])\n",
        "\n",
        "    # Create the output tensor from the underlying data and the requires_grad flag\n",
        "    out = Tensor(out_arr, requires_grad)\n",
        "\n",
        "    # If requires_grad, then create a recipe\n",
        "    if requires_grad:\n",
        "        parents = {idx: arr for idx, arr in enumerate([a, b]) if isinstance(arr, Tensor)}\n",
        "        out.recipe = Recipe(np.multiply, (arg_a, arg_b), {}, parents)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "multiply = multiply_forward\n",
        "tests.test_multiply(Tensor, multiply_forward)\n",
        "tests.test_multiply_no_grad(Tensor, multiply_forward)\n",
        "tests.test_multiply_float(Tensor, multiply_forward)\n",
        "a = Tensor([2], requires_grad=True)\n",
        "b = Tensor([3], requires_grad=True)\n",
        "grad_tracking_enabled = False\n",
        "b = multiply_forward(a, b)\n",
        "grad_tracking_enabled = True\n",
        "assert not b.requires_grad, \"should not require grad if grad tracking globally disabled\"\n",
        "assert b.recipe is None, \"should not create recipe if grad tracking globally disabled\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFbRHXqcqDjm"
      },
      "source": [
        "<details>\n",
        "<summary>Help - I get <code>AttributeError: 'int' object has no attribute 'array'</code>.</summary>\n",
        "\n",
        "Remember that your multiply function should also accept integers. You need to separately deal with the cases where `a` and `b` are integers or Tensors.\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Help - I get <code>AssertionError: assert len(c.recipe.parents) == 1 and c.recipe.parents[0] is a</code> in the \"test_multiply_float\" test.</summary>\n",
        "\n",
        "This is probably because you've stored the inputs to `multiply` as integers when one of the is an integer. Remember, `parents` should just be a list of the **Tensors** that were inputs to `multiply`, so you shouldn't add ints.\n",
        "</details>\n",
        "\n",
        "\n",
        "<details><summary>Solution</summary>\n",
        "\n",
        "```python\n",
        "def multiply_forward(a: Tensor | int, b: Tensor | int) -> Tensor:\n",
        "    \"\"\"Performs np.multiply on a Tensor object.\"\"\"\n",
        "    assert isinstance(a, Tensor) or isinstance(b, Tensor)\n",
        "\n",
        "    # Get all function arguments as non-tensors (i.e. either ints or arrays)\n",
        "    arg_a = a.array if isinstance(a, Tensor) else a\n",
        "    arg_b = b.array if isinstance(b, Tensor) else b\n",
        "\n",
        "    # Calculate the output (which is a numpy array)\n",
        "    out_arr = arg_a * arg_b\n",
        "    assert isinstance(out_arr, np.ndarray)\n",
        "\n",
        "    # Find whether the tensor requires grad (need to check if ANY of the inputs do)\n",
        "    requires_grad = grad_tracking_enabled and any([isinstance(x, Tensor) and x.requires_grad for x in (a, b)])\n",
        "\n",
        "    # Create the output tensor from the underlying data and the requires_grad flag\n",
        "    out = Tensor(out_arr, requires_grad)\n",
        "\n",
        "    # If requires_grad, then create a recipe\n",
        "    if requires_grad:\n",
        "        parents = {idx: arr for idx, arr in enumerate([a, b]) if isinstance(arr, Tensor)}\n",
        "        out.recipe = Recipe(np.multiply, (arg_a, arg_b), {}, parents)\n",
        "\n",
        "    return out\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hjuTvDQqDjn"
      },
      "source": [
        "## Forward Pass - Generic Version\n",
        "\n",
        "All our forward functions are going to look extremely similar to `log_forward` and `multiply_forward`.\n",
        "Implement the higher order function `wrap_forward_fn` that takes a `Arr -> Arr` function and returns a `Tensor -> Tensor` function. In other words, `wrap_forward_fn(np.multiply)` should evaluate to a callable that does the same thing as your `multiply_forward` (and same for `np.log`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDUzMFrGqDjn"
      },
      "source": [
        "### Exercise - implement `wrap_forward_fn`\n",
        "\n",
        "> ```yaml\n",
        "> Difficulty: 🔴🔴🔴🔴⚪\n",
        "> Importance: 🔵🔵🔵🔵⚪\n",
        ">\n",
        "> You should spend up to 20-25 minutes on this exercise.\n",
        "> This exercise is probably the 2nd most conceptually important today, after the backprop implementation at the end of the section.\n",
        "> ```\n",
        "\n",
        "If you're stuck, you can start with the same structure as the wrapped multiply function above (i.e. just copy and paste the code from solutions and use this as a stand in for `tensor_func` below, then modify it)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m62o1t_fqDjn",
        "outputId": "8a637de6-de88-4166-990f-428c5601bd66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests in `test_log` passed!\n",
            "All tests in `test_log_no_grad` passed!\n",
            "All tests in `test_multiply` passed!\n",
            "All tests in `test_multiply_no_grad` passed!\n",
            "All tests in `test_multiply_float` passed!\n",
            "All tests in `test_sum` passed!\n"
          ]
        }
      ],
      "source": [
        "def wrap_forward_fn(numpy_func: Callable, is_differentiable=True) -> Callable:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        numpy_func:\n",
        "            takes any number of positional arguments, some of which may be NumPy arrays, and any number of keyword#\n",
        "            arguments which we aren't allowing to be NumPy arrays at present. It returns a single NumPy array.\n",
        "\n",
        "        is_differentiable:\n",
        "            if True, numpy_func is differentiable with respect to some input argument, so we may need to track\n",
        "            information in a Recipe. If False, we definitely don't need to track information.\n",
        "\n",
        "    Returns:\n",
        "        tensor_func\n",
        "            It has the same signature as numpy_func, except it operates on Tensors instead of arrays.\n",
        "    \"\"\"\n",
        "\n",
        "    def tensor_func(*args: Any, **kwargs: Any) -> Tensor:\n",
        "        # Get all function arguments as non-tensors (i.e. either ints or arrays)\n",
        "        arg_arrays = tuple([(a.array if isinstance(a, Tensor) else a) for a in args])\n",
        "\n",
        "        # Calculate the output (which is a numpy array)\n",
        "        out_arr = numpy_func(*arg_arrays, **kwargs)\n",
        "\n",
        "        # Find whether the tensor requires grad (need to check if ANY of the inputs do)\n",
        "        requires_grad = (\n",
        "            grad_tracking_enabled\n",
        "            and is_differentiable\n",
        "            and any([(isinstance(a, Tensor) and a.requires_grad) for a in args])\n",
        "        )\n",
        "\n",
        "        # Create the output tensor from the underlying data and the requires_grad flag\n",
        "        out = Tensor(out_arr, requires_grad)\n",
        "\n",
        "        # If requires_grad, then create a recipe\n",
        "        if requires_grad:\n",
        "            parents = {idx: a for idx, a in enumerate(args) if isinstance(a, Tensor)}\n",
        "            out.recipe = Recipe(numpy_func, arg_arrays, kwargs, parents)\n",
        "\n",
        "        return out\n",
        "\n",
        "    return tensor_func\n",
        "\n",
        "\n",
        "def _sum(x: Arr, dim=None, keepdim=False) -> Arr:\n",
        "    # need to be careful with sum, because kwargs have different names in torch and numpy\n",
        "    return np.sum(x, axis=dim, keepdims=keepdim)\n",
        "\n",
        "\n",
        "log = wrap_forward_fn(np.log)\n",
        "multiply = wrap_forward_fn(np.multiply)\n",
        "eq = wrap_forward_fn(np.equal, is_differentiable=False)\n",
        "sum = wrap_forward_fn(_sum)\n",
        "\n",
        "tests.test_log(Tensor, log)\n",
        "tests.test_log_no_grad(Tensor, log)\n",
        "tests.test_multiply(Tensor, multiply)\n",
        "tests.test_multiply_no_grad(Tensor, multiply)\n",
        "tests.test_multiply_float(Tensor, multiply)\n",
        "tests.test_sum(Tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fhFSVvjqDjn"
      },
      "source": [
        "<details>\n",
        "<summary>Help - I'm getting <code>NameError: name 'getitem' is not defined</code>.</summary>\n",
        "\n",
        "This is probably because you're calling `numpy_func` on the args themselves. Recall that `args` will be a list of `Tensor` objects, and that you should call `numpy_func` on the underlying arrays.\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Help - I'm getting an AssertionError on <code>assert c.requires_grad == True</code> (or something similar).</summary>\n",
        "\n",
        "This is probably because you're not defining `requires_grad` correctly. Remember that the output of a forward function should have `requires_grad = True` if and only if all of the following hold:\n",
        "\n",
        "* Grad tracking is enabled\n",
        "* The function is differentiable\n",
        "* **Any** of the inputs are tensors with `requires_grad = True`\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Help - my function passes all tests up to <code>test_sum</code>, but then fails here.</summary>\n",
        "\n",
        "`test_sum`, unlike the previous tests, wraps a function that uses keyword arguments. So if you're failing here, it's probably because you didn't use `kwargs` correctly.\n",
        "\n",
        "`kwargs` should be used in two ways: once when actually calling the `numpy_func`, and once when defining the `Recipe` object for the output tensor.\n",
        "</details>\n",
        "\n",
        "\n",
        "<details><summary>Solution</summary>\n",
        "\n",
        "```python\n",
        "def wrap_forward_fn(numpy_func: Callable, is_differentiable=True) -> Callable:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        numpy_func:\n",
        "            takes any number of positional arguments, some of which may be NumPy arrays, and any number of keyword#\n",
        "            arguments which we aren't allowing to be NumPy arrays at present. It returns a single NumPy array.\n",
        "\n",
        "        is_differentiable:\n",
        "            if True, numpy_func is differentiable with respect to some input argument, so we may need to track\n",
        "            information in a Recipe. If False, we definitely don't need to track information.\n",
        "\n",
        "    Returns:\n",
        "        tensor_func\n",
        "            It has the same signature as numpy_func, except it operates on Tensors instead of arrays.\n",
        "    \"\"\"\n",
        "\n",
        "    def tensor_func(*args: Any, **kwargs: Any) -> Tensor:\n",
        "        # Get all function arguments as non-tensors (i.e. either ints or arrays)\n",
        "        arg_arrays = tuple([(a.array if isinstance(a, Tensor) else a) for a in args])\n",
        "\n",
        "        # Calculate the output (which is a numpy array)\n",
        "        out_arr = numpy_func(*arg_arrays, **kwargs)\n",
        "\n",
        "        # Find whether the tensor requires grad (need to check if ANY of the inputs do)\n",
        "        requires_grad = (\n",
        "            grad_tracking_enabled\n",
        "            and is_differentiable\n",
        "            and any([(isinstance(a, Tensor) and a.requires_grad) for a in args])\n",
        "        )\n",
        "\n",
        "        # Create the output tensor from the underlying data and the requires_grad flag\n",
        "        out = Tensor(out_arr, requires_grad)\n",
        "\n",
        "        # If requires_grad, then create a recipe\n",
        "        if requires_grad:\n",
        "            parents = {idx: a for idx, a in enumerate(args) if isinstance(a, Tensor)}\n",
        "            out.recipe = Recipe(numpy_func, arg_arrays, kwargs, parents)\n",
        "\n",
        "        return out\n",
        "\n",
        "    return tensor_func\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nw2jfE5PqDjn"
      },
      "source": [
        "Note - none of these functions involve keyword args, so the tests won't detect if you're handling kwargs incorrectly (or even failing to use them at all). If your code fails in later exercises, you might want to come back here and check that you're using the kwargs correctly. Alternatively, once you pass the tests, you can compare your code to the solutions and see how they handle kwargs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOyR_IAuqDjn"
      },
      "source": [
        "## Backpropagation\n",
        "\n",
        "Now all the pieces are in place to implement backpropagation. We need to loop over our nodes from right to left (i.e. starting with the tensors computed last and moving backwards chronologically). At each node, we:\n",
        "\n",
        "- Call the backward function to transform the grad wrt output to the grad wrt input.\n",
        "- If the node is a leaf, write the grad to the grad field.\n",
        "- Otherwise, accumulate the grad into temporary storage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnqF_Y0sqDjo"
      },
      "source": [
        "### Topological Sort\n",
        "\n",
        "As part of backprop, we need to sort the nodes of our graph so we can traverse the graph in the appropriate order."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huVuI6NoqDjo"
      },
      "source": [
        "### Exercise - implement `topological_sort`\n",
        "\n",
        "> ```yaml\n",
        "> Difficulty: 🔴🔴🔴🔴⚪\n",
        "> Importance: 🔵⚪⚪⚪⚪\n",
        ">\n",
        "> You should spend up to 20-25 minutes on this exercise.\n",
        "> Note, it's completely fine to skip this problem if you're not very interested in it.\n",
        "> It's more of a fun LeetCode-style challenge, and writing a solution for it isn't crucial for understanding today's content.\n",
        "> ```\n",
        "\n",
        "Write a general function `topological_sort` that return a list of node's children in topological order (beginning with the furthest descendants, ending with the starting node) using [depth-first search](https://en.wikipedia.org/wiki/Topological_sorting).\n",
        "\n",
        "We've given you a `Node` class, with a `children` attribute, and a `get_children` function. You shouldn't change any of these, and your `topological_sort` function should use `get_children` to access a node's children rather than calling `node.children` directly. In subsequent exercises, we'll replace the `Node` class with the `Tensor` class (and using a different `get_children` function), so this will ensure your code still works for this new case.\n",
        "\n",
        "If you're stuck, try looking at the pseudocode from some of [these examples](https://en.wikipedia.org/wiki/Topological_sorting#Kahn's_algorithm)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTHDxLEPqDjo",
        "outputId": "7b091634-9976-45df-94ed-94629cdbc2ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests in `test_topological_sort_linked_list` passed!\n",
            "All tests in `test_topological_sort_branching` passed!\n",
            "All tests in `test_topological_sort_rejoining` passed!\n",
            "All tests in `test_topological_sort_cyclic` passed!\n"
          ]
        }
      ],
      "source": [
        "class Node:\n",
        "    def __init__(self, *children):\n",
        "        self.children = list(children)\n",
        "\n",
        "\n",
        "def get_children(node: Node) -> list[Node]:\n",
        "    return node.children\n",
        "\n",
        "\n",
        "def topological_sort(node: Node, get_children: Callable) -> list[Node]:\n",
        "    \"\"\"\n",
        "    Return a list of node's descendants in reverse topological order from future\n",
        "    to past (i.e. `node` should be last).\n",
        "\n",
        "    Should raise an error if the graph with `node` as root is not in fact acyclic.\n",
        "    \"\"\"\n",
        "    result: list[Node] = []  # stores the list of nodes to be returned (in reverse topological order)\n",
        "    perm: set[Node] = set()  # same as `result`, but as a set (faster to check for membership)\n",
        "    temp: set[Node] = set()  # keeps track of previously visited nodes (to detect cyclicity)\n",
        "\n",
        "    def visit(cur: Node):\n",
        "        \"\"\"\n",
        "        Recursive function which visits all the children of the current node,\n",
        "        and appends them all to `result` in the order they were found.\n",
        "        \"\"\"\n",
        "        if cur in perm:\n",
        "            return\n",
        "        if cur in temp:\n",
        "            raise ValueError(\"Not a DAG!\")\n",
        "        temp.add(cur)\n",
        "\n",
        "        for next in get_children(cur):\n",
        "            visit(next)\n",
        "\n",
        "        result.append(cur)\n",
        "        perm.add(cur)\n",
        "        temp.remove(cur)\n",
        "\n",
        "    visit(node)\n",
        "    return result\n",
        "\n",
        "tests.test_topological_sort_linked_list(topological_sort)\n",
        "tests.test_topological_sort_branching(topological_sort)\n",
        "tests.test_topological_sort_rejoining(topological_sort)\n",
        "tests.test_topological_sort_cyclic(topological_sort)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6oT1eGSqDjo"
      },
      "source": [
        "<details>\n",
        "<summary>Help - my function is hanging without returning any values.</summary>\n",
        "\n",
        "This is probably because it's going around in cycles when fed a cyclic graph. You should add a way of raising an error if your function detects that the graph isn't cyclic. One way to do this is to create a set `temp`, which stores the nodes you've visited on a particular excursion into the graph, then you can raise an error if you come across an already visited node.\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Help - I'm completely stuck on how to implement this, and would like the template for some code.</summary>\n",
        "\n",
        "Here is the template for a depth-first search implementation:\n",
        "\n",
        "```python\n",
        "def topological_sort(node: Node, get_children: Callable) -> list[Node]:\n",
        "    \n",
        "    result: list[Node] = [] # stores the list of nodes to be returned (in reverse topological order)\n",
        "    perm: set[Node] = set() # same as `result`, but as a set (faster to check for membership)\n",
        "    temp: set[Node] = set() # keeps track of previously visited nodes (to detect cyclicity)\n",
        "\n",
        "    def visit(cur: Node):\n",
        "        '''\n",
        "        Recursive function which visits all the children of the current node, and appends them all\n",
        "        to `result` in the order they were found.\n",
        "        '''\n",
        "        pass\n",
        "\n",
        "    visit(node)\n",
        "    return result\n",
        "```\n",
        "</details>\n",
        "\n",
        "\n",
        "<details><summary>Solution</summary>\n",
        "\n",
        "```python\n",
        "def topological_sort(node: Node, get_children: Callable) -> list[Node]:\n",
        "    \"\"\"\n",
        "    Return a list of node's descendants in reverse topological order from future\n",
        "    to past (i.e. `node` should be last).\n",
        "\n",
        "    Should raise an error if the graph with `node` as root is not in fact acyclic.\n",
        "    \"\"\"\n",
        "    result: list[Node] = []  # stores the list of nodes to be returned (in reverse topological order)\n",
        "    perm: set[Node] = set()  # same as `result`, but as a set (faster to check for membership)\n",
        "    temp: set[Node] = set()  # keeps track of previously visited nodes (to detect cyclicity)\n",
        "\n",
        "    def visit(cur: Node):\n",
        "        \"\"\"\n",
        "        Recursive function which visits all the children of the current node,\n",
        "        and appends them all to `result` in the order they were found.\n",
        "        \"\"\"\n",
        "        if cur in perm:\n",
        "            return\n",
        "        if cur in temp:\n",
        "            raise ValueError(\"Not a DAG!\")\n",
        "        temp.add(cur)\n",
        "\n",
        "        for next in get_children(cur):\n",
        "            visit(next)\n",
        "\n",
        "        result.append(cur)\n",
        "        perm.add(cur)\n",
        "        temp.remove(cur)\n",
        "\n",
        "    visit(node)\n",
        "    return result\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZSa2aoHqDjo"
      },
      "source": [
        "Now, we've given you the function `sorted_computational_graph`. This calls `topological_sort` and returns the result in reverse order (because we want to start with the root node). The \"get children\" function we're using here is \"return all tensors in the recipe for this tensor\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCqEwMt9qDjo"
      },
      "source": [
        "<img src=\"https://github.com/callummcdougall/Fundamentals/blob/main/images/abcdefg.png?raw=true\" width=500>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmFKnZzUqDjo",
        "outputId": "9eaf554f-e665-431b-c7dd-b2106abe4d8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['g', 'f', 'e', 'c', 'd', 'b', 'a']\n"
          ]
        }
      ],
      "source": [
        "def sorted_computational_graph(tensor: Tensor) -> list[Tensor]:\n",
        "    \"\"\"\n",
        "    For a given tensor, return a list of Tensors that make up the nodes of the given Tensor's computational graph, in\n",
        "    reverse topological order (i.e. `tensor` should be first).\n",
        "    \"\"\"\n",
        "\n",
        "    def get_parents(tensor: Tensor) -> list[Tensor]:\n",
        "        if tensor.recipe is None:\n",
        "            return []\n",
        "        return list(tensor.recipe.parents.values())\n",
        "\n",
        "    return topological_sort(tensor, get_parents)[::-1]\n",
        "\n",
        "\n",
        "a = Tensor([1], requires_grad=True)\n",
        "b = Tensor([2], requires_grad=True)\n",
        "c = Tensor([3], requires_grad=True)\n",
        "d = a * b\n",
        "e = c.log()\n",
        "f = d * e\n",
        "g = f.log()\n",
        "name_lookup = {a: \"a\", b: \"b\", c: \"c\", d: \"d\", e: \"e\", f: \"f\", g: \"g\"}\n",
        "\n",
        "print([name_lookup[t] for t in sorted_computational_graph(g)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EC46DEdqDjp"
      },
      "source": [
        "Compare your output with the computational graph. You should never be printing `x` before `y` if there is an edge `x --> ... --> y` (this should result in approximately reverse alphabetical order)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9wCLrjrqDjp"
      },
      "source": [
        "### The `backward` method\n",
        "\n",
        "Now we're really ready for backprop!\n",
        "\n",
        "Recall that in the implementation of the class `Tensor`, we had:\n",
        "\n",
        "```python\n",
        "class Tensor:\n",
        "    ...\n",
        "    def backward(self, end_grad: \"Arr | Tensor | None\" = None) -> None:\n",
        "        if isinstance(end_grad, Arr):\n",
        "            end_grad = Tensor(end_grad)\n",
        "        return backprop(self, end_grad)\n",
        "```\n",
        "\n",
        "In other words, for a tensor `out`, calling `out.backward()` is equivalent to `backprop(out)`.\n",
        "\n",
        "Recall that in the last section, we said that calling `backward` on a scalar tensor is equivalent to backpropagating on the weighted sum of all the elements of the tensor, i.e. `L = (tensor * v).sum()`. By default `v` is a tensor of 1s of the same shape as the tensor you're calling `backward` from, meaning we're just backpropagating on `L.sum()`. Here, the `end_grad` argument you pass to `backward` gives you the option to override this default behaviour, in other words if it's supplied you should use it as the first input to your backward function instead of a tensor of 1s. The use case for this is pretty niche (used for things like **influence functions**), but it's still useful to understand!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hz9ZF7hUqDjp"
      },
      "source": [
        "### Exercise - implement `backprop`\n",
        "\n",
        "> ```yaml\n",
        "> Difficulty: 🔴🔴🔴🔴🔴\n",
        "> Importance: 🔵🔵🔵🔵🔵\n",
        ">\n",
        "> You should spend up to 30-45 minutes on this exercise.\n",
        ">\n",
        "> This exercise is the most conceptually important today, and probably the hardest. We've provided several dropdowns to help you.\n",
        "> ```\n",
        "\n",
        "Now, we get to the actual backprop function! Some code is provided below, which you should complete.\n",
        "\n",
        "If you want a challenge, you can try and implement it straight away, with out any help. However, because this is quite a challenging exercise, you can also use the dropdowns below. The first one gives you a sketch of the backpropagation algorithm, the second gives you a diagram which provides a bit more detail, and the third gives you the annotations for the function (so you just have to fill in the code). You are recommended to start by trying to implement it without help, but use the dropdowns (in order) if this is too difficult.\n",
        "\n",
        "We've also provided a few dropdowns to address specific technical errors that can arise from implementing this function. If you're having trouble, you can use these to help you debug. You should take some time with this function, as it's definitely the most important exercise to understand today."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsncX_U2qDjp",
        "outputId": "7c5b0131-248a-4fc0-8115-d57c4e58dddf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests in `test_backprop` passed!\n",
            "All tests in `test_backprop_branching` passed!\n",
            "All tests in `test_backprop_requires_grad_sum` passed!\n",
            "All tests in `test_backprop_requires_grad_false` passed!\n",
            "All tests in `test_backprop_float_arg` passed!\n"
          ]
        }
      ],
      "source": [
        "def backprop(end_node: Tensor, end_grad: Tensor | None = None) -> None:\n",
        "    \"\"\"Accumulates gradients in the grad field of each leaf node.\n",
        "\n",
        "    tensor.backward() is equivalent to backprop(tensor).\n",
        "\n",
        "    end_node:\n",
        "        The rightmost node in the computation graph. If it contains more than one element, end_grad must be provided.\n",
        "    end_grad:\n",
        "        A tensor of the same shape as end_node. Set to 1 if not specified and end_node has only one element.\n",
        "    \"\"\"\n",
        "    # Get value of end_grad_arr\n",
        "    end_grad_arr = np.ones_like(end_node.array) if end_grad is None else end_grad.array\n",
        "\n",
        "    # Create dict to store gradients\n",
        "    grads: dict[Tensor, Arr] = {end_node: end_grad_arr}\n",
        "\n",
        "    for node in sorted_computational_graph(end_node):\n",
        "        # Get the outgrad from the grads dict\n",
        "        outgrad = grads.pop(node)\n",
        "\n",
        "        # (1) If it's a leaf node, then set/update the gradient if requires_grad=True, then stop here\n",
        "        if node.is_leaf:\n",
        "            if node.requires_grad:\n",
        "                node.grad = Tensor(outgrad) if node.grad is None else node.grad + outgrad\n",
        "\n",
        "        # (2) If not a leaf node then it must have a recipe, so we iterate through its parents and update their grads\n",
        "        else:\n",
        "            for argnum, parent in node.recipe.parents.items():\n",
        "                # Get backward function, from the forward function that created `node` from `parent`\n",
        "                back_fn = BACK_FUNCS.get_back_func(node.recipe.func, argnum)\n",
        "\n",
        "                # Use it to compute the gradient we'll add onto parent from the path `parent -> node -> ... -> end_node`\n",
        "                in_grad = back_fn(outgrad, node.array, *node.recipe.args, **node.recipe.kwargs)\n",
        "\n",
        "                # Add this gradient to the grads dict (handling special case where parent is not in grads yet)\n",
        "                grads[parent] = in_grad if (parent not in grads) else grads[parent] + in_grad\n",
        "\n",
        "\n",
        "tests.test_backprop(Tensor)\n",
        "tests.test_backprop_branching(Tensor)\n",
        "tests.test_backprop_requires_grad_sum(Tensor)\n",
        "tests.test_backprop_requires_grad_false(Tensor)\n",
        "tests.test_backprop_float_arg(Tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gB_yU8nBqDjp"
      },
      "source": [
        "<details>\n",
        "<summary>Help - I get AttributeError: 'NoneType' object has no attribute 'func'</summary>\n",
        "\n",
        "This error is probably because you're trying to access `recipe.func` from the wrong node. Possibly, you're calling your backward functions using the parents nodes' `recipe.func`, rather than the node's `recipe.func`.\n",
        "        \n",
        "To explain further, suppose your computational graph is simply:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/Screenshot%202023-02-17%20174308.png\" width=320>\n",
        "\n",
        "When you reach `b` in your backprop iteration, you should calculate the gradient wrt `a` (the only parent of `b`) and store it in your `grads` dictionary, as `grads[a]`. In order to do this, you need the backward function for `func1`, which is stored in the node `b` (recall that the recipe of a tensor can be thought of as a set of instructions for how that tensor was created).\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Help - I get AttributeError: 'numpy.ndarray' object has no attribute 'array'</summary>\n",
        "\n",
        "This might be because you've set `node.grad` to be an array, rather than a tensor. You should store gradients as tensors (think of PyTorch, where `tensor.grad` will have type `torch.Tensor`).\n",
        "        \n",
        "It's fine to store numpy arrays in the `grads` dictionary, but when it comes time to set a tensor's grad attribute, you should use a tensor.\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Help - I get 'RuntimeError: bool value of Tensor with more than one value is ambiguous'.</summary>\n",
        "\n",
        "This error is probably because your computational graph function checks whether a tensor is in a list. The way these classes are compared for equality is a bit funky, and using sets rather than lists should make this error go away (i.e. checking whether a tensor is in a set should be fine).\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Help - I'm failing on the <code>test_backprop_requires_grad_sum</code> test and I don't know why.</summary>\n",
        "\n",
        "This test is designed to spot cases where you're accidentally **overwriting the gradient with each backward fn call**, rather than summing them. Remember that if a node has multiple paths from itself to the end node, then that node's grad attribute should be the sum of the gradients from all those\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Help - I'm stuck, and I need a template for the function.</summary>\n",
        "\n",
        "You just need to fill in the code below the comments labelled (1) and (2).\n",
        "\n",
        "```python\n",
        "def backprop(end_node: Tensor, end_grad: Tensor | None = None) -> None:\n",
        "\n",
        "    # Get value of end_grad_arr\n",
        "    end_grad_arr = np.ones_like(end_node.array) if end_grad is None else end_grad.array\n",
        "\n",
        "    # Create dict to store gradients\n",
        "    grads: dict[Tensor, Arr] = {end_node: end_grad_arr}\n",
        "\n",
        "    for node in sorted_computational_graph(end_node):\n",
        "        outgrad = grads.pop(node)\n",
        "\n",
        "        # (1) If this is a leaf node, then set/update the gradient if requires_grad\n",
        "\n",
        "        # (2) If this isn't a leaf node, then iterate through this node's parents and update their values in the `grads`\n",
        "        # dict, using the outgrad values returned from this node's backward function\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "\n",
        "<details><summary>Solution</summary>\n",
        "\n",
        "```python\n",
        "def backprop(end_node: Tensor, end_grad: Tensor | None = None) -> None:\n",
        "    \"\"\"Accumulates gradients in the grad field of each leaf node.\n",
        "\n",
        "    tensor.backward() is equivalent to backprop(tensor).\n",
        "\n",
        "    end_node:\n",
        "        The rightmost node in the computation graph. If it contains more than one element, end_grad must be provided.\n",
        "    end_grad:\n",
        "        A tensor of the same shape as end_node. Set to 1 if not specified and end_node has only one element.\n",
        "    \"\"\"\n",
        "    # Get value of end_grad_arr\n",
        "    end_grad_arr = np.ones_like(end_node.array) if end_grad is None else end_grad.array\n",
        "\n",
        "    # Create dict to store gradients\n",
        "    grads: dict[Tensor, Arr] = {end_node: end_grad_arr}\n",
        "\n",
        "    for node in sorted_computational_graph(end_node):\n",
        "        # Get the outgrad from the grads dict\n",
        "        outgrad = grads.pop(node)\n",
        "\n",
        "        # (1) If it's a leaf node, then set/update the gradient if requires_grad=True, then stop here\n",
        "        if node.is_leaf:\n",
        "            if node.requires_grad:\n",
        "                node.grad = Tensor(outgrad) if node.grad is None else node.grad + outgrad\n",
        "\n",
        "        # (2) If not a leaf node then it must have a recipe, so we iterate through its parents and update their grads\n",
        "        else:\n",
        "            for argnum, parent in node.recipe.parents.items():\n",
        "                # Get backward function, from the forward function that created `node` from `parent`\n",
        "                back_fn = BACK_FUNCS.get_back_func(node.recipe.func, argnum)\n",
        "\n",
        "                # Use it to compute the gradient we'll add onto parent from the path `parent -> node -> ... -> end_node`\n",
        "                in_grad = back_fn(outgrad, node.array, *node.recipe.args, **node.recipe.kwargs)\n",
        "\n",
        "                # Add this gradient to the grads dict (handling special case where parent is not in grads yet)\n",
        "                grads[parent] = in_grad if (parent not in grads) else grads[parent] + in_grad\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnQj_de2qDjq"
      },
      "source": [
        "# 3️⃣ Training on MNIST from scratch\n",
        "\n",
        "> ##### Learning Objectives\n",
        ">\n",
        "> * Implement more forward and backward functions, including for indexing, summing, and matrix multiplication\n",
        "> * Learn how to build higher-level abstractions like parameters and modules on top of individual functions and tensors\n",
        "> * Complete the process of building up a neural network from scratch and training it via gradient descent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3CP6eGEqDjq"
      },
      "source": [
        "Congrats on implementing backprop! Soon we'll be able to train a full model from scratch, but first we'll go through a bunch of backward functions which will be necessary for training (as well as ones that cover some interesting cases). These should be a lot like your `log_back` and `multiply_back0`, `multiplyback1` examples earlier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l188DSadqDjq"
      },
      "source": [
        "## More backward functions!\n",
        "\n",
        "> Note - some of these exercises can get a bit repetitive, and so **you're welcome to skip through many of these exercises if you don't find them interesting, and/or you're pressed for time.** The exercises in the section \"Parameters & Modules\" and beyond are much more conceptually valuable.\n",
        ">\n",
        "> Additionally, most of these functions can be implemented simply in 1 or 2 lines, so if you find yourself writing a lot more than that then you might want to look at the solution instead."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyCKpOXuqDjq"
      },
      "source": [
        "### Exercise - `negative`\n",
        "\n",
        "> ```yaml\n",
        "> Difficulty: 🔴⚪⚪⚪⚪\n",
        "> Importance: 🔵⚪⚪⚪⚪\n",
        ">\n",
        "> You should spend up to ~5 minutes on this exercise (it's not a trick question, it is as simple as it looks!).\n",
        "> ```\n",
        "\n",
        "`torch.negative` just performs `-x` elementwise. Make your own version `negative` using `wrap_forward_fn`. Note, you don't need to worry about unbroadcasting here because `np.negative` won't change the input shape (technically it can since `np.negative(x, out)` is actually the negative version of `x` broadcasted to the shape of `out`, but we won't be using it in this way during our exercises)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYVTbn5jqDjq",
        "outputId": "8835be5d-edaf-4e9a-9dca-42a996afce2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests in `test_negative_back` passed!\n"
          ]
        }
      ],
      "source": [
        "def negative_back(grad_out: Arr, out: Arr, x: Arr) -> Arr:\n",
        "    \"\"\"Backward function for f(x) = -x elementwise.\"\"\"\n",
        "    return -grad_out\n",
        "\n",
        "\n",
        "negative = wrap_forward_fn(np.negative)\n",
        "BACK_FUNCS.add_back_func(np.negative, 0, negative_back)\n",
        "\n",
        "tests.test_negative_back(Tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pax8NraEqDjq"
      },
      "source": [
        "<details><summary>Solution</summary>\n",
        "\n",
        "```python\n",
        "def negative_back(grad_out: Arr, out: Arr, x: Arr) -> Arr:\n",
        "    \"\"\"Backward function for f(x) = -x elementwise.\"\"\"\n",
        "    return -grad_out\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVR_LllIqDjq"
      },
      "source": [
        "### Exercise - `exp`\n",
        "\n",
        "> ```yaml\n",
        "> Difficulty: 🔴⚪⚪⚪⚪\n",
        "> Importance: 🔵🔵⚪⚪⚪\n",
        ">\n",
        "> You should spend up to 5-10 minutes on this exercise.\n",
        "> ```\n",
        "\n",
        "Make your own version of `torch.exp`. The backward function should express the result in terms of the `out` parameter - this more efficient than expressing it in terms of `x`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9udkgZiqDjr",
        "outputId": "62b32a26-78b9-4491-afa4-bbbf2a4ace4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests in `test_exp_back` passed!\n"
          ]
        }
      ],
      "source": [
        "def exp_back(grad_out: Arr, out: Arr, x: Arr) -> Arr:\n",
        "    \"\"\"Backward function for f(x) = exp(x) elementwise.\"\"\"\n",
        "    return out * grad_out\n",
        "\n",
        "\n",
        "exp = wrap_forward_fn(np.exp)\n",
        "BACK_FUNCS.add_back_func(np.exp, 0, exp_back)\n",
        "\n",
        "tests.test_exp_back(Tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIKi_YXrqDjr"
      },
      "source": [
        "<details><summary>Solution</summary>\n",
        "\n",
        "```python\n",
        "def exp_back(grad_out: Arr, out: Arr, x: Arr) -> Arr:\n",
        "    \"\"\"Backward function for f(x) = exp(x) elementwise.\"\"\"\n",
        "    return out * grad_out\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leRcA2mdqDjr"
      },
      "source": [
        "### Exercise - `reshape`\n",
        "\n",
        "> ```yaml\n",
        "> Difficulty: 🔴🔴⚪⚪⚪\n",
        "> Importance: 🔵⚪⚪⚪⚪\n",
        ">\n",
        "> You should spend up to 5-10 minutes on this exercise.\n",
        "> ```\n",
        "\n",
        "`reshape` is a bit different than the functions we've dealt with so far: it changes the shape of the tensor, not its values. In other words, the backward function needs to be able to map from the gradient $\\partial L / \\partial \\mathbf{x_r}$ to $\\partial L / \\partial \\mathbf{x}$, where $\\mathbf{x_r}$ is the reshaped version of $\\mathbf{x}$.\n",
        "\n",
        "Depending how you wrote `wrap_forward_fn` and `backprop`, you might need to go back and adjust them to handle this - if you're failing tests but think your implementation is correct, we recommend you go back to these functions and check them.\n",
        "\n",
        "This function should just be a single line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DswXWH7EqDjr",
        "outputId": "be572291-28a6-4e02-a2e9-4580cc993753"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests in `test_reshape_back` passed!\n"
          ]
        }
      ],
      "source": [
        "def reshape_back(grad_out: Arr, out: Arr, x: Arr, new_shape: tuple) -> Arr:\n",
        "    \"\"\"Backward function for torch.reshape.\"\"\"\n",
        "    return np.reshape(grad_out, x.shape)\n",
        "\n",
        "\n",
        "reshape = wrap_forward_fn(np.reshape)\n",
        "BACK_FUNCS.add_back_func(np.reshape, 0, reshape_back)\n",
        "\n",
        "tests.test_reshape_back(Tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpIgqVfTqDjr"
      },
      "source": [
        "<details>\n",
        "<summary>Solution (and explanation)</summary>\n",
        "\n",
        "Explanation: the reshape operation that takes us from the tensor $\\frac{\\partial L}{\\partial \\mathbf{x_r}}$ to $\\frac{\\partial L}{\\partial \\mathbf{x}}$ is exactly the inverse of the forward reshape operation that produced $\\mathbf{x_r}$ from $\\mathbf{x}$. In other words, we want to to take `grad_out` and reshape it back to the shape of `x`.\n",
        "\n",
        "```python\n",
        "def reshape_back(grad_out: Arr, out: Arr, x: Arr, new_shape: tuple) -> Arr:\n",
        "    \"\"\"Backward function for torch.reshape.\"\"\"\n",
        "    return np.reshape(grad_out, x.shape)\n",
        "```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhN4YT2kqDjr"
      },
      "source": [
        "### Exercise - `permute`\n",
        "\n",
        "> ```yaml\n",
        "> Difficulty: 🔴🔴🔴⚪⚪\n",
        "> Importance: 🔵⚪⚪⚪⚪\n",
        ">\n",
        "> You should spend up to 5-10 minutes on this exercise.\n",
        "> ```\n",
        "\n",
        "In NumPy, the equivalent of `torch.permute` is called `np.transpose`, so we will wrap that. Permute is somewhat similar to reshape, but the difference is that it does actually change the order of elements in the underlying array.\n",
        "\n",
        "Hint - just like with `reshape`, the inverse of a transposition is another transposition. You might find the function `np.argsort` useful for getting the inverse transposition.\n",
        "\n",
        "This function should also just be a single line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g35USfnIqDjr",
        "outputId": "0b293420-5d5c-45fb-8436-9b02253635b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests in `test_permute_back` passed!\n"
          ]
        }
      ],
      "source": [
        "def permute_back(grad_out: Arr, out: Arr, x: Arr, axes: tuple) -> Arr:\n",
        "    \"\"\"Backward function for torch.permute. Works by inverting the transposition in the forward function.\"\"\"\n",
        "    return np.transpose(grad_out, np.argsort(axes))\n",
        "\n",
        "\n",
        "BACK_FUNCS.add_back_func(np.transpose, 0, permute_back)\n",
        "permute = wrap_forward_fn(np.transpose)\n",
        "\n",
        "tests.test_permute_back(Tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-FRVXcPqDjs"
      },
      "source": [
        "<details>\n",
        "<summary>Solution (and explanation)</summary>\n",
        "\n",
        "The inverse of transposing with `axes` is transposing using `np.argsort(axes)`. To see this: the forward transpose will send axis `j` to axis `axes[j]`, and so we want the inverse transposition `axes_inv` to satisfy `axes_inv[axes[j]] = j`, in other words **the indices of `axes_inv` should sort `axes`** - this is exactly what `np.argsort` does.\n",
        "\n",
        "```python\n",
        "def permute_back(grad_out: Arr, out: Arr, x: Arr, axes: tuple) -> Arr:\n",
        "    \"\"\"Backward function for torch.permute. Works by inverting the transposition in the forward function.\"\"\"\n",
        "    return np.transpose(grad_out, np.argsort(axes))\n",
        "```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4mqz3drqDjs"
      },
      "source": [
        "### Exercise - `sum`\n",
        "\n",
        "> ```yaml\n",
        "> Difficulty: 🔴🔴🔴⚪⚪\n",
        "> Importance: 🔵🔵⚪⚪⚪\n",
        ">\n",
        "> You should spend up to 20-30 minutes on this exercise.\n",
        "> ```\n",
        "\n",
        "The output can also be smaller than the input, such as when calling `torch.sum`.\n",
        "\n",
        "Recall that when we looked at broadcasting, the backwards operation was summing over the broadcasted dimensions. This is because a broadcast operation effectively copies our tensor, giving it more gradient paths that we need to sum over. Similarly, the backwards operation for summing is broadcasting. We can intuitively see this as follows: if we have some value `L = L(x_summed)` where `x_summed` is the result of summing `x` over some number of dimensions, then editing `x_summed[i, j, ...] += delta` has the same downstream effect as editing any one of the `x` values which were summed over to get `x_summed[i, j, ...]`. So to get the gradient of `L` wrt `x`, we need to copy (broadcast) the gradient of `L` wrt `x_summed` up to the full size of `x`.\n",
        "\n",
        "Implementing `sum_back` should have 2 steps:\n",
        "\n",
        "1. **Adding new dims if they were summed over with `keepdim=False`**. You can do this with `np.expand_dims`, for example if `arr` has shape `(2, 3)` then `np.expand_dims(arr, (0, 2))` has shape `(1, 2, 1, 3)`, i.e. it's a new tensor with dummy dimensions created at indices 0 and 2.\n",
        "2. **Broadcasting along dims that were summed over**. Since after step (1) you've effectively reduced to the `keepdim=True` case, you can now use `np.broadcast_to` to get the correct shape.\n",
        "\n",
        "Note, if you get weird errors that you can't explain, and these exceptions don't even go away when you use the solutions provided, this could mean that your implementation of `wrap_forward_fn` was wrong in a way which wasn't picked up by the tests. You should return to this function and try to fix it (or just use the solution)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKFRHH0LqDjs",
        "outputId": "2a7b4c89-d122-4b53-f215-1a05588a09f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests in `test_sum_keepdim_false` passed!\n",
            "All tests in `test_sum_keepdim_true` passed!\n",
            "All tests in `test_sum_dim_none` passed!\n",
            "All tests in `test_sum_nonscalar_grad_out` passed!\n"
          ]
        }
      ],
      "source": [
        "def sum_back(grad_out: Arr, out: Arr, x: Arr, dim=None, keepdim=False):\n",
        "    \"\"\"Backward function for torch.sum\"\"\"\n",
        "    # Step (1): if keepdim=False, then we need to add back in dims, so grad_out and x have the\n",
        "    # same number of dims. We don't bother with the dim=None case, since then grad_out is a scalar\n",
        "    # and this will be handled by our broadcasting in step (2).\n",
        "    if (not keepdim) and (dim is not None):\n",
        "        grad_out = np.expand_dims(grad_out, dim)\n",
        "\n",
        "    # Step (2): repeat grad_out along the dims over which x was summed\n",
        "    return np.broadcast_to(grad_out, x.shape)\n",
        "\n",
        "\n",
        "def _sum(x: Arr, dim=None, keepdim=False) -> Arr:\n",
        "    \"\"\"Like torch.sum, calling np.sum internally.\"\"\"\n",
        "    return np.sum(x, axis=dim, keepdims=keepdim)\n",
        "\n",
        "\n",
        "sum = wrap_forward_fn(_sum)\n",
        "BACK_FUNCS.add_back_func(_sum, 0, sum_back)\n",
        "\n",
        "tests.test_sum_keepdim_false(Tensor)\n",
        "tests.test_sum_keepdim_true(Tensor)\n",
        "tests.test_sum_dim_none(Tensor)\n",
        "tests.test_sum_nonscalar_grad_out(Tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-c_mb--qDjs"
      },
      "source": [
        "<details>\n",
        "<summary>Help - I'm not sure how to handle the case where <code>dim=None</code>.</summary>\n",
        "\n",
        "You can actually handle this pretty easily - if `dim=None` then `grad_out` will be a scalar, so it's always fine to broadcast it along the dims that were summed over! This means you can skip step (1), i.e. this step only needs to handle the case where `keepdim=False` and `dim` is not `None`.\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Help - I get the error \"Encountered error when running `backward` in the test for nonscalar grad_out.\"</summary>\n",
        "\n",
        "This error is likely due to the fact that you're expanding your tensor in a way that doesn't refer to the dimensions being summed over (i.e. the `dim` argument).\n",
        "\n",
        "Remember that in the previous exercise we assumed that the tensors were broadcastable with each other, and our functions could just internally call `np.broadcast_to` as a result. But here, one tensor is the sum over another tensor's dimensions, and if `keepdim=False` then they might not broadcast. For instance, if `x.shape = (2, 5)`, `out = x.sum(dim=1)` has shape `(2,)` and `grad_out.shape = (2,)`, then the tensors `grad_out` and `x` are not broadcastable.\n",
        "\n",
        "How can you carefully handle the case where `keepdim=False` and `dim` doesn't just refer to dimensions at the start of the tensor? (Hint - try and use `np.expand_dims`).\n",
        "\n",
        "</details>\n",
        "\n",
        "\n",
        "<details><summary>Solution</summary>\n",
        "\n",
        "```python\n",
        "def sum_back(grad_out: Arr, out: Arr, x: Arr, dim=None, keepdim=False):\n",
        "    \"\"\"Backward function for torch.sum\"\"\"\n",
        "    # Step (1): if keepdim=False, then we need to add back in dims, so grad_out and x have the\n",
        "    # same number of dims. We don't bother with the dim=None case, since then grad_out is a scalar\n",
        "    # and this will be handled by our broadcasting in step (2).\n",
        "    if (not keepdim) and (dim is not None):\n",
        "        grad_out = np.expand_dims(grad_out, dim)\n",
        "\n",
        "    # Step (2): repeat grad_out along the dims over which x was summed\n",
        "    return np.broadcast_to(grad_out, x.shape)\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ETz7azFqDjs"
      },
      "source": [
        "### Elementwise add, subtract, divide\n",
        "\n",
        "> ```yaml\n",
        "> Difficulty: 🔴🔴🔴⚪⚪\n",
        "> Importance: 🔵🔵⚪⚪⚪\n",
        ">\n",
        "> You should spend up to 10-15 minutes on this exercise.\n",
        "> ```\n",
        "\n",
        "These are exactly analogous to the multiply case. Note that Python and NumPy have the notion of \"floor division\", which is a truncating integer division as in `7 // 3 = 2`. You can ignore floor division: - we only need the usual floating point division which is called \"true division\".\n",
        "\n",
        "Use lambda functions to define and register the backward functions each in one line. We've given you the first one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgOguRJFqDjs",
        "outputId": "5a7f4890-e53b-42dd-9bb4-03a6b5dbf109"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests in `test_add_broadcasted` passed!\n",
            "All tests in `test_subtract_broadcasted` passed!\n",
            "All tests in `test_truedivide_broadcasted` passed!\n"
          ]
        }
      ],
      "source": [
        "add = wrap_forward_fn(np.add)\n",
        "subtract = wrap_forward_fn(np.subtract)\n",
        "true_divide = wrap_forward_fn(np.true_divide)\n",
        "\n",
        "BACK_FUNCS.add_back_func(np.add, 0, lambda grad_out, out, x, y: unbroadcast(grad_out, x))\n",
        "# YOUR CODE HERE - continue adding to BACK_FUNCS, for each of the 3 functions & both argument orders\n",
        "BACK_FUNCS.add_back_func(np.add, 0, lambda grad_out, out, x, y: unbroadcast(grad_out, x))\n",
        "BACK_FUNCS.add_back_func(np.add, 1, lambda grad_out, out, x, y: unbroadcast(grad_out, y))\n",
        "BACK_FUNCS.add_back_func(np.subtract, 0, lambda grad_out, out, x, y: unbroadcast(grad_out, x))\n",
        "BACK_FUNCS.add_back_func(np.subtract, 1, lambda grad_out, out, x, y: unbroadcast(-grad_out, y))\n",
        "BACK_FUNCS.add_back_func(np.true_divide, 0, lambda grad_out, out, x, y: unbroadcast(grad_out / y, x))\n",
        "BACK_FUNCS.add_back_func(np.true_divide, 1, lambda grad_out, out, x, y: unbroadcast(grad_out * (-x / y**2), y))\n",
        "\n",
        "tests.test_add_broadcasted(Tensor)\n",
        "tests.test_subtract_broadcasted(Tensor)\n",
        "tests.test_truedivide_broadcasted(Tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwEuEoi-qDjt"
      },
      "source": [
        "<details><summary>Solution</summary>\n",
        "\n",
        "```python\n",
        "BACK_FUNCS.add_back_func(np.add, 0, lambda grad_out, out, x, y: unbroadcast(grad_out, x))\n",
        "BACK_FUNCS.add_back_func(np.add, 1, lambda grad_out, out, x, y: unbroadcast(grad_out, y))\n",
        "BACK_FUNCS.add_back_func(np.subtract, 0, lambda grad_out, out, x, y: unbroadcast(grad_out, x))\n",
        "BACK_FUNCS.add_back_func(np.subtract, 1, lambda grad_out, out, x, y: unbroadcast(-grad_out, y))\n",
        "BACK_FUNCS.add_back_func(np.true_divide, 0, lambda grad_out, out, x, y: unbroadcast(grad_out / y, x))\n",
        "BACK_FUNCS.add_back_func(np.true_divide, 1, lambda grad_out, out, x, y: unbroadcast(grad_out * (-x / y**2), y))\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmCQB-dXqDjt"
      },
      "source": [
        "### Indexing\n",
        "\n",
        "If we have the gradient of `L` wrt `x[index]`, what is the gradient of `L` wrt `x`? The answer is it'll be an array of zeros, filled in with the values of `dL/dx[index]` at the appropriate index positions. For example, if `x = [1, 2, 3]` and `L = x[0]`, then we trivially have `dL/dx[0] = 1`, and we can compute `dL/dx = [1, 0, 0]` in this way.\n",
        "\n",
        "In its full generality, exactly how you can index a `torch.Tensor` is really complicated and there are quite a few cases to handle separately. Our implementation only handles 2 cases:\n",
        "\n",
        "- The index is an integer or tuple of integers.\n",
        "- The index is a tuple of (array or Tensor) representing coordinates. Each array is 1D and of equal length. Some coordinates may be repeated. This is [Integer array indexing](https://numpy.org/doc/stable/user/basics.indexing.html#integer-array-indexing).\n",
        "\n",
        "This latter case is very important, because it describes how we index correct logprobs / probabilities. For example if we're training a classifier and we have tensors `logprobs.shape = (batch_size, n_classes)` and `targets.shape = (n_classes,)` then we index the correct logprobs using `logprobs[arange(batch_size), targets]` (note that the `arange` function has been given to you earlier, when we defined the `Tensor` class - it's a simple wrapper around `np.arange`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "WA3Tq0SzqDjt"
      },
      "outputs": [],
      "source": [
        "Index = int | tuple[int, ...] | tuple[Arr] | tuple[Tensor]\n",
        "\n",
        "\n",
        "def coerce_index(index: Index):\n",
        "    \"\"\"Helper function: converts array of tensors to array of numpy arrays.\"\"\"\n",
        "    if isinstance(index, tuple) and all(isinstance(i, Tensor) for i in index):\n",
        "        return tuple([i.array for i in index])\n",
        "    else:\n",
        "        return index\n",
        "\n",
        "\n",
        "def _getitem(x: Arr, index: Index) -> Arr:\n",
        "    \"\"\"Like x[index] when x is a torch.Tensor.\"\"\"\n",
        "    return x[coerce_index(index)]\n",
        "\n",
        "\n",
        "def getitem_back(grad_out: Arr, out: Arr, x: Arr, index: Index):\n",
        "    \"\"\"\n",
        "    Backwards function for _getitem.\n",
        "\n",
        "    Hint: use np.add.at(a, indices, b)\n",
        "    This function works just like a[indices] += b, except that it allows for repeated indices.\n",
        "    \"\"\"\n",
        "    new_grad_out = np.full_like(x, 0)\n",
        "    np.add.at(new_grad_out, coerce_index(index), grad_out)\n",
        "    return new_grad_out\n",
        "\n",
        "\n",
        "getitem = wrap_forward_fn(_getitem)\n",
        "BACK_FUNCS.add_back_func(_getitem, 0, getitem_back)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WodA0DNOqDjt"
      },
      "source": [
        "### Non-Differentiable Functions\n",
        "\n",
        "For functions like `torch.argmax` or `torch.eq`, there's no sensible way to define gradients with respect to the input tensor. For these, we will still use `wrap_forward_fn` because we still need to unbox the arguments and box the result, but by passing `is_differentiable=False` we can avoid doing any unnecessary computation.\n",
        "\n",
        "We've given you this one as an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "cLPjvf80qDjt"
      },
      "outputs": [],
      "source": [
        "def _argmax(x: Arr, dim=None, keepdim=False):\n",
        "    \"\"\"Like torch.argmax.\"\"\"\n",
        "    return np.expand_dims(np.argmax(x, axis=dim), axis=([] if dim is None else dim))\n",
        "\n",
        "\n",
        "argmax = wrap_forward_fn(_argmax, is_differentiable=False)\n",
        "\n",
        "a = Tensor([1.0, 0.0, 3.0, 4.0], requires_grad=True)\n",
        "b = a.argmax()\n",
        "assert not b.requires_grad\n",
        "assert b.recipe is None\n",
        "assert b.item() == 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZZ14JQvqDjt"
      },
      "source": [
        "### In-Place Operations\n",
        "\n",
        "Supporting in-place operations introduces substantial complexity and generally doesn't help performance that much. The problem is that if any of the inputs used in the backward function have been modified in-place since the forward pass, then the backward function will incorrectly calculate using the modified version. PyTorch will warn you when this causes a problem with the error \"RuntimeError: a leaf Variable that requires grad is being used in an in-place operation.\".\n",
        "\n",
        "Note - you don't have to fill anything in here; just run the cell. If you're curious, you can implement inplace operations as a bonus exercise at the end, but for now we just warn against inplace operations unless we specify otherwise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nDoJ-ijqDjt",
        "outputId": "8c6fd14f-08ae-45d5-ab9b-6cf1ae16a299"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grad wrt a is OK!\n",
            "Grad wrt b is WRONG!\n"
          ]
        }
      ],
      "source": [
        "def add_(x: Tensor, other: Tensor, alpha: float = 1.0) -> Tensor:\n",
        "    \"\"\"Like torch.add_. Compute x += other * alpha in-place and return tensor.\"\"\"\n",
        "    np.add(x.array, other.array * alpha, out=x.array)\n",
        "    return x\n",
        "\n",
        "\n",
        "def sub_(x: Tensor, other: Tensor, alpha: float = 1.0) -> Tensor:\n",
        "    \"\"\"Like torch.sub_. Compute x -= other * alpha in-place and return tensor.\"\"\"\n",
        "    np.subtract(x.array, other.array * alpha, out=x.array)\n",
        "    return x\n",
        "\n",
        "\n",
        "def safe_example():\n",
        "    \"\"\"This example should work properly.\"\"\"\n",
        "    a = Tensor([0.0, 1.0, 2.0, 3.0], requires_grad=True)\n",
        "    b = Tensor([2.0, 3.0, 4.0, 5.0], requires_grad=True)\n",
        "    a.add_(b)\n",
        "    c = a * b\n",
        "    c.sum().backward()\n",
        "    assert a.grad is not None and np.allclose(a.grad.array, [2.0, 3.0, 4.0, 5.0])\n",
        "    assert b.grad is not None and np.allclose(b.grad.array, [2.0, 4.0, 6.0, 8.0])\n",
        "\n",
        "\n",
        "def unsafe_example():\n",
        "    \"\"\"This example is expected to compute the wrong gradients, because dc/db is calculated using the modified a.\"\"\"\n",
        "    a = Tensor([0.0, 1.0, 2.0, 3.0], requires_grad=True)\n",
        "    b = Tensor([2.0, 3.0, 4.0, 5.0], requires_grad=True)\n",
        "    c = a * b\n",
        "    a.add_(b)\n",
        "    c.sum().backward()\n",
        "    if a.grad is not None and np.allclose(a.grad.array, [2.0, 3.0, 4.0, 5.0]):\n",
        "        print(\"Grad wrt a is OK!\")\n",
        "    else:\n",
        "        print(\"Grad wrt a is WRONG!\")\n",
        "    if b.grad is not None and np.allclose(b.grad.array, [0.0, 1.0, 2.0, 3.0]):\n",
        "        print(\"Grad wrt b is OK!\")\n",
        "    else:\n",
        "        print(\"Grad wrt b is WRONG!\")\n",
        "\n",
        "\n",
        "safe_example()\n",
        "unsafe_example()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVdPHduKqDju"
      },
      "source": [
        "### Mixed Scalar-Tensor Operations\n",
        "\n",
        "You may have been wondering why our `Tensor` class has to define both `__mul__` and `__rmul__` magic methods.\n",
        "\n",
        "Without `__rmul__` defined, executing `2 * a` when `a` is a `Tensor` would try to call `2.__mul__(a)`, and the built-in class `int` would be confused about how to handle this.\n",
        "\n",
        "Since we have defined `__rmul__` for you at the start, and you implemented multiply to work with floats as arguments, the following should \"just work\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "_vSaYb99qDju"
      },
      "outputs": [],
      "source": [
        "a = Tensor([0, 1, 2, 3], requires_grad=True)\n",
        "(a * 2).sum().backward()\n",
        "b = Tensor([0, 1, 2, 3], requires_grad=True)\n",
        "(2 * b).sum().backward()\n",
        "assert a.grad is not None\n",
        "assert b.grad is not None\n",
        "assert np.allclose(a.grad.array, b.grad.array)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2awvzOwEqDju"
      },
      "source": [
        "### Exercise - `max`\n",
        "\n",
        "> ```yaml\n",
        "> Difficulty: 🔴🔴🔴⚪⚪\n",
        "> Importance: 🔵🔵⚪⚪⚪\n",
        ">\n",
        "> You should spend up to 10-15 minutes on this exercise.\n",
        "> ```\n",
        "\n",
        "Since this is an elementwise function, we can think about the scalar case. For scalar $x$, $y$, the derivative for $\\max(x, y)$ wrt $x$ is 1 when $x > y$ and 0 when $x < y$. What should happen when $x = y$?\n",
        "\n",
        "Intuitively, since $\\max(x, x)$ is equivalent to the identity function which has a derivative of 1 wrt $x$, it makes sense for the sum of our partial derivatives wrt $x$ and $y$ to also therefore total 1. The convention used by PyTorch is to split the derivative evenly between the two arguments. We will follow this behavior for compatibility, but it's just as legitimate to say it's 1 wrt $x$ and 0 wrt $y$, or some other arbitrary combination that sums to one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LgZ-vxlqDju"
      },
      "source": [
        "<details>\n",
        "<summary>Help - I'm not sure how to implement this function.</summary>\n",
        "\n",
        "Try returning `grad_out * bool_sum`, where `bool_sum` is an array constructed from the sum of two boolean arrays.\n",
        "\n",
        "You can alternatively use `np.where`.\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Help - I'm passing the first test but not the second.</summary>\n",
        "\n",
        "This probably means that you haven't implemented `unbroadcast`. You'll need to do this, to get `grad_out` into the right shape before you use it in `np.where`.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoP_xPwnqDju",
        "outputId": "7888262c-3fcb-423d-fbb6-68302ace5e33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests in `test_maximum` passed!\n",
            "All tests in `test_maximum_broadcasted` passed!\n"
          ]
        }
      ],
      "source": [
        "def maximum_back0(grad_out: Arr, out: Arr, x: Arr, y: Arr):\n",
        "    \"\"\"Backwards function for max(x, y) wrt x.\"\"\"\n",
        "    bool_sum = (x > y) + 0.5 * (x == y)\n",
        "    return unbroadcast(grad_out * bool_sum, x)\n",
        "\n",
        "\n",
        "def maximum_back1(grad_out: Arr, out: Arr, x: Arr, y: Arr):\n",
        "    \"\"\"Backwards function for max(x, y) wrt y.\"\"\"\n",
        "    bool_sum = (x < y) + 0.5 * (x == y)\n",
        "    return unbroadcast(grad_out * bool_sum, y)\n",
        "\n",
        "maximum = wrap_forward_fn(np.maximum)\n",
        "BACK_FUNCS.add_back_func(np.maximum, 0, maximum_back0)\n",
        "BACK_FUNCS.add_back_func(np.maximum, 1, maximum_back1)\n",
        "\n",
        "tests.test_maximum(Tensor)\n",
        "tests.test_maximum_broadcasted(Tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M94ti5IRqDju"
      },
      "source": [
        "<details><summary>Solution</summary>\n",
        "\n",
        "```python\n",
        "def maximum_back0(grad_out: Arr, out: Arr, x: Arr, y: Arr):\n",
        "    \"\"\"Backwards function for max(x, y) wrt x.\"\"\"\n",
        "    bool_sum = (x > y) + 0.5 * (x == y)\n",
        "    return unbroadcast(grad_out * bool_sum, x)\n",
        "\n",
        "\n",
        "def maximum_back1(grad_out: Arr, out: Arr, x: Arr, y: Arr):\n",
        "    \"\"\"Backwards function for max(x, y) wrt y.\"\"\"\n",
        "    bool_sum = (x < y) + 0.5 * (x == y)\n",
        "    return unbroadcast(grad_out * bool_sum, y)\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvYI9s6dqDju"
      },
      "source": [
        "### Exercise - functional `ReLU`\n",
        "\n",
        "> ```yaml\n",
        "> Difficulty: 🔴⚪⚪⚪⚪\n",
        "> Importance: 🔵⚪⚪⚪⚪\n",
        ">\n",
        "> You should spend ~5 minutes on this exercise.\n",
        "> ```\n",
        "\n",
        "A simple and correct ReLU function can be defined in terms of your maximum function. Note the PyTorch version also supports in-place operation, which we are punting to the bonus section for now.\n",
        "\n",
        "Again, at $x = 0$ your derivative could reasonably be anything between 0 and 1 inclusive, but we've followed PyTorch in making it 0.5. This means you can just use the `maximum` function defined above!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BR2YnzkqDjv",
        "outputId": "9d2255b8-9771-47fb-d3b4-9eaf3514f0ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests in `test_relu` passed!\n"
          ]
        }
      ],
      "source": [
        "def relu(x: Tensor) -> Tensor:\n",
        "    \"\"\"Like torch.nn.function.relu(x, inplace=False).\"\"\"\n",
        "    return maximum(x, 0.0)\n",
        "\n",
        "\n",
        "tests.test_relu(Tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jZ9epvHqDjv"
      },
      "source": [
        "<details><summary>Solution</summary>\n",
        "\n",
        "```python\n",
        "def relu(x: Tensor) -> Tensor:\n",
        "    \"\"\"Like torch.nn.function.relu(x, inplace=False).\"\"\"\n",
        "    return maximum(x, 0.0)\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXY5h33zqDjv"
      },
      "source": [
        "### Exercise - 2D `matmul`\n",
        "\n",
        "> ```yaml\n",
        "> Difficulty: 🔴🔴🔴⚪⚪\n",
        "> Importance: 🔵🔵🔵⚪⚪\n",
        ">\n",
        "> You should spend up to 20-25 minutes on this exercise.\n",
        "> ```\n",
        "\n",
        "Implement your version of `torch.matmul`, restricting it to the simpler case where both inputs are 2D (this means we don't need to worry about unbroadcasting or anything).\n",
        "\n",
        "Note - althought the solution to this exercise is very short (just one line), you may find the actual mathematical derivation a bit tricky. We've given hints to help you, which we recommend using if you're stuck."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKKeFlsEqDjv",
        "outputId": "bf55b3c0-5ca4-4ffd-dfa9-e3338018ca32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests in `test_matmul2d` passed!\n"
          ]
        }
      ],
      "source": [
        "def _matmul2d(x: Arr, y: Arr) -> Arr:\n",
        "    \"\"\"Matrix multiply restricted to the case where both inputs are exactly 2D.\"\"\"\n",
        "    return x @ y\n",
        "\n",
        "\n",
        "def matmul2d_back0(grad_out: Arr, out: Arr, x: Arr, y: Arr) -> Arr:\n",
        "    return grad_out @ y.T\n",
        "\n",
        "\n",
        "def matmul2d_back1(grad_out: Arr, out: Arr, x: Arr, y: Arr) -> Arr:\n",
        "    return x.T @ grad_out\n",
        "\n",
        "\n",
        "matmul = wrap_forward_fn(_matmul2d)\n",
        "BACK_FUNCS.add_back_func(_matmul2d, 0, matmul2d_back0)\n",
        "BACK_FUNCS.add_back_func(_matmul2d, 1, matmul2d_back1)\n",
        "\n",
        "tests.test_matmul2d(Tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvuTRmz-qDjv"
      },
      "source": [
        "<details>\n",
        "<summary>Help - I need a hint about the math</summary>\n",
        "\n",
        "Let $X$, $Y$ and $M$ denote the variables `x`, `y` and `out`, so we have the matrix relation $M = XY$. The object `grad_out` is a tensor with elements `grad_out[p, q]` $ = \\frac{\\partial L}{\\partial M_{p q}}$.\n",
        "\n",
        "The output of `matmul2d_back0` should be the gradient of $L$ wrt $X$, i.e. it should have elements $\\frac{\\partial L}{\\partial X_{i j}}$. Can you write this in terms of the elements of `x`, `y`, `out` and `grad_out`?\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Help - I need the math explained</summary>\n",
        "\n",
        "We can write $\\frac{\\partial L}{\\partial X_{i j}}$ as:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\frac{\\partial L}{\\partial X_{i j}} &=\\sum_{pq} \\frac{\\partial L}{\\partial M_{p q}} \\frac{\\partial M_{p q}}{\\partial X_{i j}} \\\\\n",
        "&=\\sum_{pqr} \\left[\\text{ grad\\_out }\\right]_{p q} \\frac{\\partial (X_{p r} Y_{r q})}{\\partial X_{i j}} \\\\\n",
        "&=\\sum_{q} \\left[\\text{ grad\\_out }\\right]_{iq} Y_{j q} \\\\\n",
        "&= \\left[\\text{ grad\\_out } \\times Y^{\\top}\\right]_{ij}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "where the second line follows because $M_{pq} = \\sum_r X_{pr} Y_{rq}$ (and we can rearrange the summands), and the third line follows because $\\frac{\\partial{X_{pr}}}{X_{ij}} = 1 \\text{ if } (p, r) = (i, j), \\text{ else } 0$.\n",
        "\n",
        "\n",
        "In other words, the `x.grad` attribute should be is `grad_out @ y.T`.\n",
        "\n",
        "You can calculate the gradient wrt `y` in a similar way - we leave this as an exercise for the reader.\n",
        "</details>\n",
        "\n",
        "\n",
        "<details><summary>Solution</summary>\n",
        "\n",
        "```python\n",
        "def _matmul2d(x: Arr, y: Arr) -> Arr:\n",
        "    \"\"\"Matrix multiply restricted to the case where both inputs are exactly 2D.\"\"\"\n",
        "    return x @ y\n",
        "\n",
        "\n",
        "def matmul2d_back0(grad_out: Arr, out: Arr, x: Arr, y: Arr) -> Arr:\n",
        "    return grad_out @ y.T\n",
        "\n",
        "\n",
        "def matmul2d_back1(grad_out: Arr, out: Arr, x: Arr, y: Arr) -> Arr:\n",
        "    return x.T @ grad_out\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbcW3g3KqDjv"
      },
      "source": [
        "## Parameters & Modules\n",
        "\n",
        "We've now written enough backwards passes that we can go up a layer and write our own `nn.Parameter` and `nn.Module`. These are important abstractions that help us building up neural networks.\n",
        "\n",
        "Below is a simple implementation of `Parameter`. It is itself a `Tensor`, shares storage with the provided `Tensor` and requires_grad is `True` by default - that's it! Make sure you understand the code being run in this cell to test the functionality of this class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "OmYYebiZqDjw"
      },
      "outputs": [],
      "source": [
        "class Parameter(Tensor):\n",
        "    def __init__(self, tensor: Tensor, requires_grad=True):\n",
        "        \"\"\"Share the array with the provided tensor.\"\"\"\n",
        "        return super().__init__(tensor.array, requires_grad=requires_grad)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Parameter containing:\\n{super().__repr__()}\"\n",
        "\n",
        "\n",
        "x = Tensor([1.0, 2.0, 3.0])\n",
        "p = Parameter(x)\n",
        "assert p.requires_grad\n",
        "assert p.array is x.array\n",
        "assert repr(p) == \"Parameter containing:\\nTensor(array([1., 2., 3.], dtype=float32), requires_grad=True)\"\n",
        "x.add_(Tensor(np.array(2.0)))\n",
        "assert np.allclose(p.array, np.array([3.0, 4.0, 5.0])), (\n",
        "    \"in-place modifications to the original tensor should affect the parameter\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJsuozAKqDjw"
      },
      "source": [
        "Just like `torch.Tensor`, the `nn.Module` class has a lot of functionality which we mostly don't care about today. We will just implement enough to get our network training.\n",
        "\n",
        "Below is a simple implementation. We'll explain it a bit below (if you're already experienced in Python then this might be obvious to you and you can skip it).\n",
        "\n",
        "- **Single-underscore attributes** are a notational convention; they're not treated differently by Python but they're used to indicate that the attribute is private and shouldn't be accessed directly by anyone using the class.\n",
        "    - `_modules` is a dict mapping module names to module objects. The `modules` method returns an iterator over these modules.\n",
        "    - `_parameters` is similar, with added recursion to include submodule parameters.\n",
        "- **Double-underscore attributes** are special methods that determine how your class instance behaves when you use certain syntax.\n",
        "    - `__call__` determines what the module does when you call it like a function. In this case, `module(*args, **kwargs)` calls `module.forward(**args, **kwargs)` - this is why we only ever need to implement `forward` in the modules we've written so far.\n",
        "    - `__setattr__` manages attribute setting (i.e. running `module.attr = value` actually calls `module.__setattr__(\"attr\", value)`). The default behaviour is to add the attribute to `self.__dict__`, but we've added custom logic so modules & parameters are also added to `self._modules` and `self._parameters` respectively - this is basically how logic like `module.parameters()` can work.\n",
        "        - Note that there's a related method `__getattr__` which specifies attribute getting behaviour when lookup in `self.__dict__` fails."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "heGw64E5qDjw",
        "outputId": "1a78b30a-d597-479d-d5e3-e9a658820361"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Manually verify that the repr looks reasonable:\n",
            "TestModule(\n",
            "  (inner): TestInnerModule()\n",
            ")\n",
            "All tests for `Module` passed!\n"
          ]
        }
      ],
      "source": [
        "class Module:\n",
        "    _modules: dict[str, \"Module\"]\n",
        "    _parameters: dict[str, Parameter]\n",
        "\n",
        "    def __init__(self):\n",
        "        self._modules: dict[str, \"Module\"] = {}\n",
        "        self._parameters: dict[str, Parameter] = {}\n",
        "\n",
        "    def modules(self) -> Iterator[\"Module\"]:\n",
        "        \"\"\"Return the direct child modules of this module, not including self.\"\"\"\n",
        "        yield from self._modules.values()\n",
        "\n",
        "    def parameters(self, recurse: bool = True) -> Iterator[Parameter]:\n",
        "        \"\"\"\n",
        "        Return an iterator over Module parameters.\n",
        "\n",
        "        recurse: if True, the iterator includes parameters of submodules, recursively.\n",
        "        \"\"\"\n",
        "        yield from self._parameters.values()\n",
        "        if recurse:\n",
        "            for mod in self.modules():\n",
        "                yield from mod.parameters(recurse=True)\n",
        "\n",
        "    def __setattr__(self, key: str, val: Any) -> None:\n",
        "        \"\"\"\n",
        "        If val is a Parameter or Module, store it in the appropriate _parameters or _modules dict.\n",
        "        Otherwise, call __setattr__ from the superclass.\n",
        "        \"\"\"\n",
        "        if isinstance(val, Parameter):\n",
        "            self._parameters[key] = val\n",
        "        elif isinstance(val, Module):\n",
        "            self._modules[key] = val\n",
        "        super().__setattr__(key, val)\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        return self.forward(*args, **kwargs)\n",
        "\n",
        "    def forward(self):\n",
        "        raise NotImplementedError(\"Subclasses must implement forward!\")\n",
        "\n",
        "    def __repr__(self):\n",
        "        _indent = lambda s_, nSpaces: re.sub(\"\\n\", \"\\n\" + (\" \" * nSpaces), s_)\n",
        "        lines = [f\"({key}): {_indent(repr(module), 2)}\" for key, module in self._modules.items()]\n",
        "        return \"\".join([self.__class__.__name__ + \"(\", \"\\n  \" + \"\\n  \".join(lines) + \"\\n\" if lines else \"\", \")\"])\n",
        "\n",
        "\n",
        "class TestInnerModule(Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.param1 = Parameter(Tensor([1.0]))\n",
        "        self.param2 = Parameter(Tensor([2.0]))\n",
        "\n",
        "\n",
        "class TestModule(Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.inner = TestInnerModule()\n",
        "        self.param3 = Parameter(Tensor([3.0]))\n",
        "\n",
        "\n",
        "mod = TestModule()\n",
        "assert list(mod.modules()) == [mod.inner]\n",
        "assert list(mod.parameters()) == [mod.param3, mod.inner.param1, mod.inner.param2]\n",
        "print(\"Manually verify that the repr looks reasonable:\")\n",
        "print(mod)\n",
        "print(\"All tests for `Module` passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjlQzHiMqDjw"
      },
      "source": [
        "### Exercise - implement `Linear`\n",
        "\n",
        "> ```yaml\n",
        "> Difficulty: 🔴🔴🔴⚪⚪\n",
        "> Importance: 🔵🔵🔵🔵⚪\n",
        ">\n",
        "> You should spend up to 20-25 minutes on this exercise.\n",
        "> ```\n",
        "\n",
        "Now, let's go one level of abstraction higher and create a `Linear` module. This should inherit from `Module` and have `__init__` & `forward` methods just like your linear module inheriting from `nn.Module` in previous exercises. In fact, your code can probably be extremely similar to the time you implemented `Linear` in the earlier exercises, except you'll need to use methods we've defined already. You should be able to do everything you need in `forward` using just the matmul operator `@`, the transpose operator `.T` (which is equivalent to `.permute(-1, -2)` as you can see in the tensor class above) and standard tensor addition `+`.\n",
        "\n",
        "To restate the task in case you don't remember it from the previous exercises, you should:\n",
        "\n",
        "- Define `self.weight` and `self.bias` in `__init__`, with both tensors having a uniform distribution in the range `[-sf, sf]` where `sf = 1/sqrt(in_features)`,\n",
        "- Write the appropriate affine operation in `forward`, i.e. multiplying by `self.weight` and adding `self.bias` if it exists.\n",
        "\n",
        "Don't forget to wrap your weights as `Parameter(Tensor(...))`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30w61STWqDjw",
        "outputId": "619d0a91-9570-412f-8167-30229fe20937"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests for `Linear` passed!\n"
          ]
        }
      ],
      "source": [
        "class Linear(Module):\n",
        "    weight: Parameter\n",
        "    bias: Parameter | None\n",
        "\n",
        "    def __init__(self, in_features: int, out_features: int, bias=True):\n",
        "        \"\"\"\n",
        "        A simple linear (technically, affine) transformation.\n",
        "\n",
        "        The fields should be named `weight` and `bias` for compatibility with PyTorch.\n",
        "        If `bias` is False, set `self.bias` to None.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "\n",
        "        sf = in_features**-0.5\n",
        "        self.weight = Parameter(Tensor(sf * (2 * np.random.rand(out_features, in_features) - 1)))\n",
        "        self.bias = Parameter(Tensor(sf * (2 * np.random.rand(out_features) - 1))) if bias else None\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        x: shape (*, in_features)\n",
        "        Return: shape (*, out_features)\n",
        "        \"\"\"\n",
        "        out = x @ self.weight.T  # transpose has been defined as .permute(-1, -2), see the `Tensor` class\n",
        "        if self.bias is not None:\n",
        "            out = out + self.bias\n",
        "        return out\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        return f\"in_features={self.in_features}, out_features={self.out_features}, bias={self.bias is not None}\"\n",
        "\n",
        "\n",
        "linear = Linear(3, 4)\n",
        "assert isinstance(linear.weight, Tensor)\n",
        "assert linear.weight.requires_grad\n",
        "\n",
        "input = Tensor([[1.0, 2.0, 3.0]])\n",
        "output = linear(input)\n",
        "assert output.requires_grad\n",
        "\n",
        "expected_output = input @ linear.weight.T + linear.bias\n",
        "np.testing.assert_allclose(output.array, expected_output.array)\n",
        "\n",
        "print(\"All tests for `Linear` passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRRtXDHBqDjw"
      },
      "source": [
        "<details><summary>Solution</summary>\n",
        "\n",
        "```python\n",
        "class Linear(Module):\n",
        "    weight: Parameter\n",
        "    bias: Parameter | None\n",
        "\n",
        "    def __init__(self, in_features: int, out_features: int, bias=True):\n",
        "        \"\"\"\n",
        "        A simple linear (technically, affine) transformation.\n",
        "\n",
        "        The fields should be named `weight` and `bias` for compatibility with PyTorch.\n",
        "        If `bias` is False, set `self.bias` to None.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "\n",
        "        sf = in_features**-0.5\n",
        "        self.weight = Parameter(Tensor(sf * (2 * np.random.rand(out_features, in_features) - 1)))\n",
        "        self.bias = Parameter(Tensor(sf * (2 * np.random.rand(out_features) - 1))) if bias else None\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        x: shape (*, in_features)\n",
        "        Return: shape (*, out_features)\n",
        "        \"\"\"\n",
        "        out = x @ self.weight.T  # transpose has been defined as .permute(-1, -2), see the `Tensor` class\n",
        "        if self.bias is not None:\n",
        "            out = out + self.bias\n",
        "        return out\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        return f\"in_features={self.in_features}, out_features={self.out_features}, bias={self.bias is not None}\"\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MoXYcLSqDjw"
      },
      "source": [
        "Finally, for the sake of completeness, we'll define a `ReLU` module:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "ChwNThu5qDjx"
      },
      "outputs": [],
      "source": [
        "class ReLU(Module):\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return relu(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiIeKAbdqDjx"
      },
      "source": [
        "Now we can define a MLP suitable for classifying MNIST, with zero PyTorch dependency!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "rjVkcd8TqDjx"
      },
      "outputs": [],
      "source": [
        "class MLP(Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear1 = Linear(28 * 28, 64)\n",
        "        self.linear2 = Linear(64, 64)\n",
        "        self.relu1 = ReLU()\n",
        "        self.relu2 = ReLU()\n",
        "        self.output = Linear(64, 10)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = x.reshape((x.shape[0], 28 * 28))\n",
        "        x = self.relu1(self.linear1(x))\n",
        "        x = self.relu2(self.linear2(x))\n",
        "        x = self.output(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhXtMxi2qDjx"
      },
      "source": [
        "### Exercise - implement `cross_entropy`\n",
        "\n",
        "> ```yaml\n",
        "> Difficulty: 🔴🔴🟠⚪⚪\n",
        "> Importance: 🔵🔵🔵⚪⚪\n",
        ">\n",
        "> You should spend up to 10-15 minutes on this exercise.\n",
        "> ```\n",
        "\n",
        "Make use of your integer array indexing to implement `cross_entropy`. See the documentation page [here](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html).\n",
        "\n",
        "We discussed this briefly in the section on indexing earlier, but the kind of indexing you should be doing on your logprobs is `logprobs[range(batch_size), true_labels]`, since this is equivalent to returning the vector of length `batch_size` with elements `[logprobs[0, true_labels[0]], logprobs[1, true_labels[1]], ...]`. Rather than using `range`, you should be using the `arange` function we've provided for you (this is equivalent to torch's `torch.arange` function, and is defined just below the `Tensor` class).\n",
        "\n",
        "Note - if you're using the `exp` function, it's usually good to make your implementation numerically stable (since taking the exponential of large numbers is prone to overflow). The common solution here is to subtract the maximum value of the tensor from all elements. However, you don't need to worry about that here (consider it a bonus exercise)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tgnzdE6qDjx",
        "outputId": "4121aad6-3017-45f3-df7d-30f0ef84d929"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests in `test_cross_entropy` passed!\n"
          ]
        }
      ],
      "source": [
        "def cross_entropy(logits: Tensor, true_labels: Tensor) -> Tensor:\n",
        "    \"\"\"Like torch.nn.functional.cross_entropy with reduction='none'.\n",
        "\n",
        "    logits: shape (batch, classes)\n",
        "    true_labels: shape (batch,). Each element is the index of the correct label in the logits.\n",
        "\n",
        "    Return: shape (batch, ) containing the per-example loss.\n",
        "    \"\"\"\n",
        "    batch_size = logits.shape[0]\n",
        "    logprobs = logits - logits.exp().sum(-1, keepdim=True).log()\n",
        "    return -logprobs[arange(0, batch_size), true_labels]\n",
        "\n",
        "\n",
        "tests.test_cross_entropy(Tensor, cross_entropy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUIil6qwqDjx"
      },
      "source": [
        "<details>\n",
        "<summary>Help - I'm not sure how to get logprobs from logits.</summary>\n",
        "\n",
        "They're equal up to a constant: `logprobs = logits - log(sum(exp(logits)))` (where the sum is over the last dimension).\n",
        "\n",
        "To see why this is true: let's define `C = logits - logprobs`. We know `sum(exp(logits - C)) = sum(exp(logprobs)) = 1` (this is by definition of `logprobs`). Factoring out the `exp(-C)` term, we get `exp(C) = sum(exp(logits))`, hence `C = log(sum(exp(logits)))` as required.\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Solution</summary>\n",
        "\n",
        "```python\n",
        "def cross_entropy(logits: Tensor, true_labels: Tensor) -> Tensor:\n",
        "    \"\"\"Like torch.nn.functional.cross_entropy with reduction='none'.\n",
        "\n",
        "    logits: shape (batch, classes)\n",
        "    true_labels: shape (batch,). Each element is the index of the correct label in the logits.\n",
        "\n",
        "    Return: shape (batch, ) containing the per-example loss.\n",
        "    \"\"\"\n",
        "    batch_size = logits.shape[0]\n",
        "    logprobs = logits - logits.exp().sum(-1, keepdim=True).log()\n",
        "    return -logprobs[arange(0, batch_size), true_labels]\n",
        "```\n",
        "\n",
        "or alternatively we can solve a slightly different way, which is still equivalent:\n",
        "\n",
        "```python\n",
        "true = logits[arange(0, batch_size), true_labels]\n",
        "return -log(exp(true) / exp(logits).sum(1))\n",
        "```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRuGQRbnqDjx"
      },
      "source": [
        "## `NoGrad` context manager\n",
        "\n",
        "The last thing our backpropagation system needs is the ability to turn it off completely like `torch.no_grad` (or `torch.inference_mode`). We've given you an implementation below, which works by modifying the global `grad_tracking_enabled` variable.\n",
        "\n",
        "A few notes on the actual python here (again for people who are more familiar with Python and understand it can skip this):\n",
        "\n",
        "- The `global` keyword is required in order to modify the global `grad_tracking_enabled` variable. We can still reference its value without this keyword, but we wouldn't be able to change it.\n",
        "- The special `__enter__` and `__exit__` methods are part of the protocol for context managers, which is a more pythonic way of doing this kind of thing. If we have a context manager block like `with NoGrad(): ...`, then we'll run `NoGrad().__enter__()` before any of the code in this block, and `NoGrad().__exit__()` after the block finishes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58JzOmrHqDjy",
        "outputId": "6f4e56de-ef35-490c-d969-3aefeb9efd65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verified that we've disabled gradients inside `NoGrad`, then set back to its previous value once we exit.\n"
          ]
        }
      ],
      "source": [
        "class NoGrad:\n",
        "    \"\"\"Context manager that disables grad inside the block. Like torch.no_grad.\"\"\"\n",
        "\n",
        "    was_enabled: bool\n",
        "\n",
        "    def __enter__(self):\n",
        "        \"\"\"\n",
        "        Method which is called whenever the context manager is entered, i.e. at the start of the `with NoGrad():` block.\n",
        "        This disables gradient tracking (but stores the value it had before, so we can set it back to this on exit).\n",
        "        \"\"\"\n",
        "        global grad_tracking_enabled\n",
        "        self.was_enabled = grad_tracking_enabled\n",
        "        grad_tracking_enabled = False\n",
        "\n",
        "    def __exit__(self, type, value, traceback):\n",
        "        \"\"\"\n",
        "        Method which is called whenever we exit the context manager. This sets the global `grad_tracking_enabled`\n",
        "        variable back to the value it had before we entered the context manager.\n",
        "        \"\"\"\n",
        "        global grad_tracking_enabled\n",
        "        grad_tracking_enabled = self.was_enabled\n",
        "\n",
        "\n",
        "assert grad_tracking_enabled\n",
        "with NoGrad():\n",
        "    assert not grad_tracking_enabled\n",
        "assert grad_tracking_enabled\n",
        "print(\"Verified that we've disabled gradients inside `NoGrad`, then set back to its previous value once we exit.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7S7TnzmhqDjy"
      },
      "source": [
        "### Exercise - implement `SGD`\n",
        "\n",
        "> ```yaml\n",
        "> Difficulty: 🔴🔴🔴⚪⚪\n",
        "> Importance: 🔵🔵🔵⚪⚪\n",
        ">\n",
        "> You should spend up to 10-20 minutes on this exercise.\n",
        "> ```\n",
        "\n",
        "In today's final exercise, you should implement the `SGD` class methods `zero_grad` and `step`. This should be pretty familiar if you've gone through yesterday's exercises on optimizers (although without all the bells and whistles from those exercises, because we're literally just implementing plain SGD with no momentum, weight decay or anything).\n",
        "\n",
        "Important note - in yesterday's exercises it was important to use inplace operations, so we would actually modify the existing tensor data rather than creating new tensors, and this is also the case here. The inplace operation `+=` is supported, since under the hood this calls `__iadd__` which we've defined in our `Tensor` class (same for subtraction, the underlying method here is `__isub__`). Note that we did discuss earlier how inplace operations are very risky for backprop, this is generally true however here we're using it for parameter updates which aren't meant to be differentiated and which are performed just before zeroing all gradients - this makes it safe in this particular context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMLbboysqDjy",
        "outputId": "d1533383-1e16-461b-a42c-4844aa7b3e43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests for `SGD` passed!\n"
          ]
        }
      ],
      "source": [
        "class SGD:\n",
        "    def __init__(self, params: Iterable[Parameter], lr: float):\n",
        "        \"\"\"Vanilla SGD with no additional features.\"\"\"\n",
        "        self.params = list(params)\n",
        "        self.lr = lr\n",
        "        self.b = [None for _ in self.params]\n",
        "\n",
        "    def zero_grad(self) -> None:\n",
        "        \"\"\"Iterates through params, and sets all grads to None.\"\"\"\n",
        "        for p in self.params:\n",
        "            p.grad = None\n",
        "\n",
        "    def step(self) -> None:\n",
        "        \"\"\"Iterates through params, and updates each of them by subtracting `param.grad * lr`.\"\"\"\n",
        "        with NoGrad():\n",
        "            for p in self.params:\n",
        "                p -= p.grad * self.lr\n",
        "\n",
        "\n",
        "tests.test_sgd(Parameter, Tensor, SGD)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jR6rwAXYqDjy"
      },
      "source": [
        "<details><summary>Solution</summary>\n",
        "\n",
        "```python\n",
        "class SGD:\n",
        "    def __init__(self, params: Iterable[Parameter], lr: float):\n",
        "        \"\"\"Vanilla SGD with no additional features.\"\"\"\n",
        "        self.params = list(params)\n",
        "        self.lr = lr\n",
        "        self.b = [None for _ in self.params]\n",
        "\n",
        "    def zero_grad(self) -> None:\n",
        "        \"\"\"Iterates through params, and sets all grads to None.\"\"\"\n",
        "        for p in self.params:\n",
        "            p.grad = None\n",
        "\n",
        "    def step(self) -> None:\n",
        "        \"\"\"Iterates through params, and updates each of them by subtracting `param.grad * lr`.\"\"\"\n",
        "        with NoGrad():\n",
        "            for p in self.params:\n",
        "                p -= p.grad * self.lr\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1EwLTRKqDjy"
      },
      "source": [
        "## Training Your Network\n",
        "\n",
        "We've already looked at data loading and training loops earlier in the course, so we'll provide a minimal version of these today as well as the data loading code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 664
        },
        "id": "P1IhyStGqDjy",
        "outputId": "eeaba658-6855-4a14-b1b8-d3860835963c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:01<00:00, 5.04MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 134kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:06<00:00, 245kB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 5.75MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training data: 100%|██████████| 60000/60000 [00:09<00:00, 6064.51it/s]\n",
            "Test data: 100%|██████████| 10000/10000 [00:01<00:00, 7217.13it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"f27232dd-77f1-453e-9e0f-d201408e96c5\" class=\"plotly-graph-div\" style=\"height:525px; width:1000px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"f27232dd-77f1-453e-9e0f-d201408e96c5\")) {                    Plotly.newPlot(                        \"f27232dd-77f1-453e-9e0f-d201408e96c5\",                        [{\"z\":[[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[197,197,197],[253,253,253],[194,194,194],[84,84,84],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[74,74,74],[249,249,249],[253,253,253],[253,253,253],[173,173,173],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[13,13,13],[229,229,229],[253,253,253],[253,253,253],[205,205,205],[94,94,94],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[9,9,9],[205,205,205],[253,253,253],[253,253,253],[110,110,110],[7,7,7],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[59,59,59],[253,253,253],[253,253,253],[185,185,185],[13,13,13],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[83,83,83],[217,217,217],[253,253,253],[253,253,253],[57,57,57],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[195,195,195],[253,253,253],[253,253,253],[186,186,186],[2,2,2],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[88,88,88],[235,235,235],[253,253,253],[209,209,209],[54,54,54],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[196,196,196],[253,253,253],[243,243,243],[78,78,78],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[99,99,99],[247,247,247],[253,253,253],[157,157,157],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[35,35,35],[223,223,223],[253,253,253],[207,207,207],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[162,162,162],[253,253,253],[253,253,253],[96,96,96],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[71,71,71],[241,241,241],[253,253,253],[195,195,195],[17,17,17],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[209,209,209],[253,253,253],[239,239,239],[61,61,61],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[38,38,38],[253,253,253],[253,253,253],[100,100,100],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[158,158,158],[255,255,255],[241,241,241],[29,29,29],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[18,18,18],[221,221,221],[253,253,253],[233,233,233],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[107,107,107],[253,253,253],[253,253,253],[135,135,135],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[227,227,227],[253,253,253],[253,253,253],[39,39,39],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[159,159,159],[253,253,253],[155,155,155],[23,23,23],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]]],\"type\":\"image\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"z\":[[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[23,23,23],[253,253,253],[255,255,255],[145,145,145],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[145,145,145],[253,253,253],[253,253,253],[239,239,239],[25,25,25],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[45,45,45],[221,221,221],[253,253,253],[253,253,253],[253,253,253],[137,137,137],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[227,227,227],[253,253,253],[253,253,253],[253,253,253],[247,247,247],[88,88,88],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[231,231,231],[253,253,253],[253,253,253],[253,253,253],[249,249,249],[105,105,105],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[231,231,231],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[137,137,137],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[10,10,10],[165,165,165],[251,251,251],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[137,137,137],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[143,143,143],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[223,223,223],[37,37,37],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[185,185,185],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[84,84,84],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[185,185,185],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[189,189,189],[7,7,7],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[185,185,185],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[183,183,183],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[42,42,42],[217,217,217],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[183,183,183],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[85,85,85],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[141,141,141],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[188,188,188],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[245,245,245],[27,27,27],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[64,64,64],[245,245,245],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[186,186,186],[13,13,13],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[139,139,139],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[161,161,161],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[139,139,139],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[179,179,179],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[139,139,139],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[29,29,29],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[139,139,139],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[201,201,201],[45,45,45],[5,5,5],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[55,55,55],[150,150,150],[253,253,253],[253,253,253],[113,113,113],[29,29,29],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]]],\"type\":\"image\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"z\":[[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[2,2,2],[73,73,73],[237,237,237],[78,78,78],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[15,15,15],[253,253,253],[253,253,253],[243,243,243],[40,40,40],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[131,131,131],[253,253,253],[253,253,253],[253,253,253],[151,151,151],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[23,23,23],[233,233,233],[253,253,253],[253,253,253],[253,253,253],[155,155,155],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[127,127,127],[253,253,253],[247,247,247],[249,249,249],[253,253,253],[155,155,155],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[49,49,49],[231,231,231],[255,255,255],[162,162,162],[213,213,213],[253,253,253],[155,155,155],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[130,130,130],[255,255,255],[241,241,241],[10,10,10],[213,213,213],[253,253,253],[155,155,155],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[99,99,99],[217,217,217],[137,137,137],[0,0,0],[213,213,213],[253,253,253],[155,155,155],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[16,16,16],[0,0,0],[0,0,0],[213,213,213],[253,253,253],[68,68,68],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[213,213,213],[253,253,253],[52,52,52],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[51,51,51],[245,245,245],[233,233,233],[32,32,32],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[80,80,80],[253,253,253],[142,142,142],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[167,167,167],[253,253,253],[99,99,99],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[8,8,8],[213,213,213],[253,253,253],[87,87,87],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[15,15,15],[253,253,253],[249,249,249],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[15,15,15],[253,253,253],[241,241,241],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[44,44,44],[253,253,253],[217,217,217],[26,26,26],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[16,16,16],[144,144,144],[235,235,235],[253,253,253],[253,253,253],[233,233,233],[219,219,219],[159,159,159],[115,115,115],[115,115,115],[83,83,83],[1,1,1],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[130,130,130],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[25,25,25],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[88,88,88],[225,225,225],[150,150,150],[82,82,82],[49,49,49],[150,150,150],[150,150,150],[150,150,150],[229,229,229],[253,253,253],[221,221,221],[14,14,14],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]]],\"type\":\"image\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"z\":[[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[2,2,2],[48,48,48],[50,50,50],[215,215,215],[237,237,237],[47,47,47],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[3,3,3],[125,125,125],[197,197,197],[78,78,78],[177,177,177],[1,1,1],[82,82,82],[0,0,0],[28,28,28],[141,141,141],[30,30,30],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[3,3,3],[139,139,139],[151,151,151],[86,86,86],[128,128,128],[0,0,0],[3,3,3],[0,0,0],[3,3,3],[12,12,12],[123,123,123],[194,194,194],[108,108,108],[5,5,5],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[26,26,26],[253,253,253],[249,249,249],[231,231,231],[188,188,188],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[60,60,60],[239,239,239],[253,253,253],[113,113,113],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[26,26,26],[253,253,253],[253,253,253],[237,237,237],[130,130,130],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[133,133,133],[253,253,253],[140,140,140],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[7,7,7],[78,78,78],[78,78,78],[26,26,26],[0,0,0],[0,0,0],[0,0,0],[11,11,11],[74,74,74],[74,74,74],[20,20,20],[198,198,198],[253,253,253],[98,98,98],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[8,8,8],[121,121,121],[121,121,121],[185,185,185],[253,253,253],[253,253,253],[233,233,233],[251,251,251],[253,253,253],[140,140,140],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[25,25,25],[194,194,194],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[72,72,72],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[53,53,53],[247,247,247],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[225,225,225],[20,20,20],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[93,93,93],[225,225,225],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[213,213,213],[173,173,173],[249,249,249],[144,144,144],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[22,22,22],[116,116,116],[155,155,155],[138,138,138],[52,52,52],[23,23,23],[0,0,0],[229,229,229],[213,213,213],[13,13,13],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[12,12,12],[233,233,233],[243,243,243],[29,29,29],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[47,47,47],[243,243,243],[237,237,237],[26,26,26],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[43,43,43],[172,172,172],[251,251,251],[195,195,195],[16,16,16],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[60,60,60],[243,243,243],[253,253,253],[249,249,249],[60,60,60],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[17,17,17],[20,20,20],[105,105,105],[185,185,185],[247,247,247],[253,253,253],[253,253,253],[184,184,184],[42,42,42],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[48,48,48],[68,68,68],[136,136,136],[241,241,241],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[245,245,245],[113,113,113],[8,8,8],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[122,122,122],[219,219,219],[219,219,219],[243,243,243],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[213,213,213],[38,38,38],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[160,160,160],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[209,209,209],[95,95,95],[61,61,61],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[21,21,21],[227,227,227],[253,253,253],[255,255,255],[253,253,253],[229,229,229],[148,148,148],[46,46,46],[9,9,9],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]]],\"type\":\"image\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"},{\"z\":[[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[32,32,32],[170,170,170],[255,255,255],[182,182,182],[10,10,10],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[41,41,41],[223,223,223],[253,253,253],[253,253,253],[253,253,253],[211,211,211],[11,11,11],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[137,137,137],[235,235,235],[253,253,253],[194,194,194],[152,152,152],[237,237,237],[253,253,253],[179,179,179],[10,10,10],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[6,6,6],[167,167,167],[249,249,249],[253,253,253],[199,199,199],[15,15,15],[0,0,0],[70,70,70],[243,243,243],[253,253,253],[182,182,182],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[155,155,155],[253,253,253],[253,253,253],[253,253,253],[80,80,80],[0,0,0],[0,0,0],[0,0,0],[49,49,49],[233,233,233],[253,253,253],[98,98,98],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[38,38,38],[227,227,227],[255,255,255],[253,253,253],[229,229,229],[19,19,19],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[51,51,51],[253,253,253],[255,255,255],[17,17,17],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[193,193,193],[253,253,253],[253,253,253],[253,253,253],[132,132,132],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[18,18,18],[253,253,253],[253,253,253],[89,89,89],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[32,32,32],[245,245,245],[253,253,253],[163,163,163],[253,253,253],[171,171,171],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[2,2,2],[145,145,145],[253,253,253],[205,205,205],[5,5,5],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[108,108,108],[253,253,253],[245,245,245],[15,15,15],[233,233,233],[215,215,215],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[52,52,52],[253,253,253],[253,253,253],[36,36,36],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[192,192,192],[253,253,253],[98,98,98],[0,0,0],[42,42,42],[94,94,94],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[253,253,253],[253,253,253],[36,36,36],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[217,217,217],[253,253,253],[72,72,72],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[255,255,255],[253,253,253],[127,127,127],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[217,217,217],[253,253,253],[72,72,72],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[233,233,233],[253,253,253],[209,209,209],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[217,217,217],[253,253,253],[72,72,72],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[86,86,86],[253,253,253],[215,215,215],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[217,217,217],[253,253,253],[72,72,72],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[112,112,112],[253,253,253],[215,215,215],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[217,217,217],[253,253,253],[136,136,136],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[195,195,195],[253,253,253],[184,184,184],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[59,59,59],[237,237,237],[253,253,253],[26,26,26],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[255,255,255],[253,253,253],[69,69,69],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[180,180,180],[253,253,253],[126,126,126],[3,3,3],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[105,105,105],[253,253,253],[241,241,241],[27,27,27],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[28,28,28],[241,241,241],[253,253,253],[109,109,109],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[42,42,42],[243,243,243],[253,253,253],[76,76,76],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[71,71,71],[247,247,247],[241,241,241],[154,154,154],[38,38,38],[18,18,18],[45,45,45],[109,109,109],[122,122,122],[241,241,241],[253,253,253],[151,151,151],[3,3,3],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[105,105,105],[181,181,181],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[245,245,245],[129,129,129],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]]],\"type\":\"image\",\"xaxis\":\"x5\",\"yaxis\":\"y5\"},{\"z\":[[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[11,11,11],[7,7,7],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[51,51,51],[255,255,255],[25,25,25],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[87,87,87],[94,94,94],[50,50,50],[40,40,40],[25,25,25],[0,0,0],[0,0,0],[0,0,0],[33,33,33],[221,221,221],[253,253,253],[24,24,24],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[6,6,6],[118,118,118],[140,140,140],[174,174,174],[149,149,149],[217,217,217],[116,116,116],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[5,5,5],[154,154,154],[221,221,221],[82,82,82],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[124,124,124],[253,253,253],[219,219,219],[225,225,225],[84,84,84],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[119,119,119],[223,223,223],[38,38,38],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[30,30,30],[86,86,86],[11,11,11],[13,13,13],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[5,5,5],[149,149,149],[229,229,229],[64,64,64],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[3,3,3],[192,192,192],[253,253,253],[79,79,79],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[29,29,29],[165,165,165],[249,249,249],[115,115,115],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[130,130,130],[251,251,251],[152,152,152],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[101,101,101],[239,239,239],[201,201,201],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[76,76,76],[198,198,198],[124,124,124],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[183,183,183],[215,215,215],[15,15,15],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[6,6,6],[85,85,85],[122,122,122],[253,253,253],[253,253,253],[253,253,253],[46,46,46],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[99,99,99],[247,247,247],[109,109,109],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[72,72,72],[253,253,253],[253,253,253],[227,227,227],[155,155,155],[217,217,217],[128,128,128],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[176,176,176],[136,136,136],[1,1,1],[0,0,0],[0,0,0],[0,0,0],[41,41,41],[181,181,181],[251,251,251],[160,160,160],[56,56,56],[37,37,37],[10,10,10],[251,251,251],[91,91,91],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[19,19,19],[235,235,235],[99,99,99],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[181,181,181],[253,253,253],[194,194,194],[17,17,17],[0,0,0],[0,0,0],[90,90,90],[253,253,253],[46,46,46],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[95,95,95],[245,245,245],[26,26,26],[0,0,0],[0,0,0],[0,0,0],[94,94,94],[253,253,253],[116,116,116],[6,6,6],[0,0,0],[0,0,0],[49,49,49],[231,231,231],[187,187,187],[2,2,2],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[233,233,233],[181,181,181],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[69,69,69],[158,158,158],[17,17,17],[0,0,0],[0,0,0],[122,122,122],[229,229,229],[249,249,249],[65,65,65],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[233,233,233],[181,181,181],[0,0,0],[0,0,0],[0,0,0],[4,4,4],[136,136,136],[3,3,3],[0,0,0],[11,11,11],[128,128,128],[251,251,251],[247,247,247],[96,96,96],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[233,233,233],[229,229,229],[76,76,76],[0,0,0],[1,1,1],[148,148,148],[181,181,181],[68,68,68],[104,104,104],[215,215,215],[253,253,253],[253,253,253],[116,116,116],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[98,98,98],[253,253,253],[251,251,251],[217,217,217],[219,219,219],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[245,245,245],[99,99,99],[11,11,11],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[9,9,9],[181,181,181],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[233,233,233],[183,183,183],[26,26,26],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[13,13,13],[95,95,95],[217,217,217],[253,253,253],[253,253,253],[253,253,253],[194,194,194],[98,98,98],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]]],\"type\":\"image\",\"xaxis\":\"x6\",\"yaxis\":\"y6\"},{\"z\":[[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[64,64,64],[128,128,128],[128,128,128],[191,191,191],[128,128,128],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[64,64,64],[255,255,255],[255,255,255],[255,255,255],[255,255,255],[255,255,255],[255,255,255],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[128,128,128],[255,255,255],[255,255,255],[255,255,255],[255,255,255],[255,255,255],[255,255,255],[128,128,128],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[128,128,128],[255,255,255],[255,255,255],[255,255,255],[64,64,64],[0,0,0],[128,128,128],[255,255,255],[64,64,64],[64,64,64],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[191,191,191],[255,255,255],[191,191,191],[64,64,64],[0,0,0],[0,0,0],[64,64,64],[255,255,255],[255,255,255],[128,128,128],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[191,191,191],[255,255,255],[255,255,255],[128,128,128],[0,0,0],[0,0,0],[64,64,64],[255,255,255],[255,255,255],[255,255,255],[128,128,128],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[64,64,64],[255,255,255],[255,255,255],[255,255,255],[64,64,64],[64,64,64],[128,128,128],[255,255,255],[255,255,255],[255,255,255],[255,255,255],[128,128,128],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[128,128,128],[255,255,255],[255,255,255],[128,128,128],[128,128,128],[255,255,255],[255,255,255],[255,255,255],[255,255,255],[255,255,255],[255,255,255],[128,128,128],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[191,191,191],[255,255,255],[255,255,255],[255,255,255],[255,255,255],[255,255,255],[255,255,255],[255,255,255],[255,255,255],[255,255,255],[191,191,191],[64,64,64],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[128,128,128],[255,255,255],[255,255,255],[255,255,255],[255,255,255],[255,255,255],[255,255,255],[255,255,255],[255,255,255],[255,255,255],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[191,191,191],[255,255,255],[128,128,128],[128,128,128],[0,0,0],[64,64,64],[255,255,255],[255,255,255],[255,255,255],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[64,64,64],[255,255,255],[255,255,255],[64,64,64],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[191,191,191],[255,255,255],[191,191,191],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[64,64,64],[255,255,255],[255,255,255],[128,128,128],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[191,191,191],[255,255,255],[255,255,255],[64,64,64],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[64,64,64],[255,255,255],[255,255,255],[255,255,255],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[255,255,255],[255,255,255],[255,255,255],[64,64,64],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[64,64,64],[255,255,255],[255,255,255],[191,191,191],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[191,191,191],[255,255,255],[255,255,255],[255,255,255],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[64,64,64],[255,255,255],[191,191,191],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]]],\"type\":\"image\",\"xaxis\":\"x7\",\"yaxis\":\"y7\"},{\"z\":[[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[18,18,18],[223,223,223],[84,84,84],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[26,26,26],[72,72,72],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[66,66,66],[253,253,253],[38,38,38],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[133,133,133],[239,239,239],[1,1,1],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[45,45,45],[237,237,237],[192,192,192],[4,4,4],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[8,8,8],[162,162,162],[253,253,253],[92,92,92],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[17,17,17],[251,251,251],[165,165,165],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[50,50,50],[253,253,253],[249,249,249],[63,63,63],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[20,20,20],[198,198,198],[243,243,243],[52,52,52],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[62,62,62],[192,192,192],[253,253,253],[136,136,136],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[5,5,5],[166,166,166],[253,253,253],[74,74,74],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[72,72,72],[245,245,245],[253,253,253],[180,180,180],[7,7,7],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[136,136,136],[253,253,253],[235,235,235],[3,3,3],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[40,40,40],[152,152,152],[239,239,239],[253,253,253],[150,150,150],[101,101,101],[88,88,88],[88,88,88],[88,88,88],[88,88,88],[88,88,88],[175,175,175],[227,227,227],[227,227,227],[123,123,123],[39,39,39],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[2,2,2],[128,128,128],[243,243,243],[253,253,253],[253,253,253],[253,253,253],[245,245,245],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[255,255,255],[247,247,247],[241,241,241],[229,229,229],[35,35,35],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[108,108,108],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[205,205,205],[199,199,199],[199,199,199],[101,101,101],[92,92,92],[92,92,92],[221,221,221],[253,253,253],[253,253,253],[98,98,98],[92,92,92],[55,55,55],[79,79,79],[65,65,65],[13,13,13],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[147,147,147],[253,253,253],[213,213,213],[98,98,98],[54,54,54],[5,5,5],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[34,34,34],[221,221,221],[253,253,253],[137,137,137],[2,2,2],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[72,72,72],[92,92,92],[8,8,8],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[38,38,38],[221,221,221],[253,253,253],[207,207,207],[23,23,23],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[29,29,29],[184,184,184],[253,253,253],[207,207,207],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[213,213,213],[253,253,253],[219,219,219],[29,29,29],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[1,1,1],[95,95,95],[219,219,219],[251,251,251],[134,134,134],[6,6,6],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[114,114,114],[253,253,253],[231,231,231],[130,130,130],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[227,227,227],[253,253,253],[167,167,167],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[5,5,5],[149,149,149],[253,253,253],[192,192,192],[2,2,2],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[74,74,74],[253,253,253],[217,217,217],[28,28,28],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[106,106,106],[253,253,253],[181,181,181],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]]],\"type\":\"image\",\"xaxis\":\"x8\",\"yaxis\":\"y8\"},{\"z\":[[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[169,169,169],[253,253,253],[237,237,237],[148,148,148],[96,96,96],[13,13,13],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[113,113,113],[203,203,203],[253,253,253],[251,251,251],[251,251,251],[211,211,211],[102,102,102],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[8,8,8],[20,20,20],[100,100,100],[179,179,179],[245,245,245],[251,251,251],[150,150,150],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[56,56,56],[198,198,198],[239,239,239],[113,113,113],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[62,62,62],[253,253,253],[239,239,239],[16,16,16],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[148,148,148],[253,253,253],[38,38,38],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[148,148,148],[251,251,251],[126,126,126],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[8,8,8],[201,201,201],[251,251,251],[170,170,170],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[106,106,106],[253,253,253],[251,251,251],[56,56,56],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[150,150,150],[253,253,253],[251,251,251],[20,20,20],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[32,32,32],[43,43,43],[139,139,139],[253,253,253],[247,247,247],[97,97,97],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[57,57,57],[128,128,128],[191,191,191],[237,237,237],[251,251,251],[251,251,251],[251,251,251],[243,243,243],[83,83,83],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[3,3,3],[57,57,57],[223,223,223],[245,245,245],[243,243,243],[241,241,241],[237,237,237],[251,251,251],[251,251,251],[251,251,251],[253,253,253],[245,245,245],[18,18,18],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[26,26,26],[139,139,139],[251,251,251],[247,247,247],[119,119,119],[49,49,49],[46,46,46],[171,171,171],[251,251,251],[235,235,235],[101,101,101],[156,156,156],[251,251,251],[91,91,91],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[71,71,71],[211,211,211],[253,253,253],[217,217,217],[47,47,47],[0,0,0],[0,0,0],[175,175,175],[251,251,251],[243,243,243],[66,66,66],[0,0,0],[43,43,43],[251,251,251],[126,126,126],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[12,12,12],[217,217,217],[253,253,253],[229,229,229],[26,26,26],[0,0,0],[28,28,28],[175,175,175],[255,255,255],[239,239,239],[106,106,106],[0,0,0],[0,0,0],[105,105,105],[253,253,253],[65,65,65],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[136,136,136],[251,251,251],[173,173,173],[53,53,53],[0,0,0],[87,87,87],[231,231,231],[251,251,251],[239,239,239],[68,68,68],[0,0,0],[0,0,0],[0,0,0],[148,148,148],[153,153,153],[4,4,4],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[223,223,223],[251,251,251],[155,155,155],[39,39,39],[162,162,162],[251,251,251],[251,251,251],[243,243,243],[62,62,62],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[57,57,57],[4,4,4],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[101,101,101],[251,251,251],[251,251,251],[253,253,253],[251,251,251],[247,247,247],[162,162,162],[49,49,49],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[3,3,3],[113,113,113],[235,235,235],[235,235,235],[147,147,147],[47,47,47],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]]],\"type\":\"image\",\"xaxis\":\"x9\",\"yaxis\":\"y9\"},{\"z\":[[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[28,28,28],[229,229,229],[38,38,38],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[16,16,16],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[35,35,35],[237,237,237],[129,129,129],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[29,29,29],[209,209,209],[92,92,92],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[1,1,1],[197,197,197],[178,178,178],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[99,99,99],[253,253,253],[245,245,245],[38,38,38],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[50,50,50],[253,253,253],[237,237,237],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[99,99,99],[253,253,253],[253,253,253],[43,43,43],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[3,3,3],[161,161,161],[253,253,253],[237,237,237],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[99,99,99],[253,253,253],[253,253,253],[43,43,43],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[3,3,3],[161,161,161],[253,253,253],[253,253,253],[140,140,140],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[10,10,10],[241,241,241],[253,253,253],[43,43,43],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[149,149,149],[253,253,253],[253,253,253],[253,253,253],[63,63,63],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[136,136,136],[253,253,253],[111,111,111],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[71,71,71],[239,239,239],[253,253,253],[253,253,253],[105,105,105],[4,4,4],[0,0,0],[0,0,0],[7,7,7],[88,88,88],[129,129,129],[227,227,227],[253,253,253],[151,151,151],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[2,2,2],[201,201,201],[253,253,253],[253,253,253],[253,253,253],[155,155,155],[178,178,178],[233,233,233],[233,233,233],[235,235,235],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[245,245,245],[4,4,4],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[53,53,53],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[4,4,4],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[3,3,3],[217,217,217],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[253,253,253],[193,193,193],[162,162,162],[172,172,172],[253,253,253],[253,253,253],[4,4,4],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[75,75,75],[159,159,159],[186,186,186],[233,233,233],[149,149,149],[177,177,177],[140,140,140],[15,15,15],[15,15,15],[5,5,5],[0,0,0],[27,27,27],[253,253,253],[253,253,253],[13,13,13],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[27,27,27],[253,253,253],[253,253,253],[88,88,88],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[27,27,27],[253,253,253],[253,253,253],[4,4,4],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[27,27,27],[253,253,253],[253,253,253],[110,110,110],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[27,27,27],[255,255,255],[253,253,253],[114,114,114],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[27,27,27],[253,253,253],[253,253,253],[114,114,114],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[27,27,27],[253,253,253],[253,253,253],[53,53,53],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[27,27,27],[253,253,253],[225,225,225],[3,3,3],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[27,27,27],[207,207,207],[105,105,105],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]]],\"type\":\"image\",\"xaxis\":\"x10\",\"yaxis\":\"y10\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,0.184],\"showticklabels\":false},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.53,1.0],\"showticklabels\":false},\"xaxis2\":{\"anchor\":\"y2\",\"domain\":[0.204,0.388],\"showticklabels\":false},\"yaxis2\":{\"anchor\":\"x2\",\"domain\":[0.53,1.0],\"showticklabels\":false},\"xaxis3\":{\"anchor\":\"y3\",\"domain\":[0.408,0.592],\"showticklabels\":false},\"yaxis3\":{\"anchor\":\"x3\",\"domain\":[0.53,1.0],\"showticklabels\":false},\"xaxis4\":{\"anchor\":\"y4\",\"domain\":[0.6120000000000001,0.796],\"showticklabels\":false},\"yaxis4\":{\"anchor\":\"x4\",\"domain\":[0.53,1.0],\"showticklabels\":false},\"xaxis5\":{\"anchor\":\"y5\",\"domain\":[0.816,1.0],\"showticklabels\":false},\"yaxis5\":{\"anchor\":\"x5\",\"domain\":[0.53,1.0],\"showticklabels\":false},\"xaxis6\":{\"anchor\":\"y6\",\"domain\":[0.0,0.184],\"showticklabels\":false},\"yaxis6\":{\"anchor\":\"x6\",\"domain\":[0.0,0.47],\"showticklabels\":false},\"xaxis7\":{\"anchor\":\"y7\",\"domain\":[0.204,0.388],\"showticklabels\":false},\"yaxis7\":{\"anchor\":\"x7\",\"domain\":[0.0,0.47],\"showticklabels\":false},\"xaxis8\":{\"anchor\":\"y8\",\"domain\":[0.408,0.592],\"showticklabels\":false},\"yaxis8\":{\"anchor\":\"x8\",\"domain\":[0.0,0.47],\"showticklabels\":false},\"xaxis9\":{\"anchor\":\"y9\",\"domain\":[0.6120000000000001,0.796],\"showticklabels\":false},\"yaxis9\":{\"anchor\":\"x9\",\"domain\":[0.0,0.47],\"showticklabels\":false},\"xaxis10\":{\"anchor\":\"y10\",\"domain\":[0.816,1.0],\"showticklabels\":false},\"yaxis10\":{\"anchor\":\"x10\",\"domain\":[0.0,0.47],\"showticklabels\":false},\"annotations\":[{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"1\",\"x\":0.092,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"1\",\"x\":0.296,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"1\",\"x\":0.5,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"3\",\"x\":0.7040000000000001,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"0\",\"x\":0.9079999999999999,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"6\",\"x\":0.092,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.47,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"9\",\"x\":0.296,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.47,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"4\",\"x\":0.5,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.47,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"2\",\"x\":0.7040000000000001,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.47,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"4\",\"x\":0.9079999999999999,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.47,\"yanchor\":\"bottom\",\"yref\":\"paper\"}],\"margin\":{\"t\":40,\"b\":20,\"r\":20,\"l\":20},\"width\":1000},                        {\"displayModeBar\": false, \"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('f27232dd-77f1-453e-9e0f-d201408e96c5');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "train_loader, test_loader = get_mnist()\n",
        "visualize(train_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39wQVAmhqDjy"
      },
      "source": [
        "To finish the day, below is some code for a training/testing loop for MNIST images, which also logs & plots the results.\n",
        "\n",
        "Note, it's normal to encounter some bugs and glitches at this point - just go back and fix them until everything runs! Because backprop is annoying and fiddly and depends heavily on exactly how the implementation works (with too many edge cases to test all of them), you may have to resort to replacing your code with the reference solution until you find the source of the error - this is a bit frustrating, but we'd be lying if we said ML isn't without its share of slow debugging sessions!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyCg3Iq3qDjz",
        "outputId": "b54dc047-ada9-4bda-d0d7-9436b9909c3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train set: Avg loss: 2.034: 100%|██████████| 118/118 [00:02<00:00, 57.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set:  Avg loss: 1.955, Accuracy: 5416/10000 (54.2%)\n",
            "Epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train set: Avg loss: 1.199: 100%|██████████| 118/118 [00:04<00:00, 26.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set:  Avg loss: 1.164, Accuracy: 7500/10000 (75.0%)\n",
            "Epoch: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train set: Avg loss: 0.673: 100%|██████████| 118/118 [00:02<00:00, 54.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set:  Avg loss: 0.719, Accuracy: 8353/10000 (83.5%)\n",
            "Epoch: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train set: Avg loss: 0.559: 100%|██████████| 118/118 [00:02<00:00, 48.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set:  Avg loss: 0.545, Accuracy: 8643/10000 (86.4%)\n",
            "Epoch: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train set: Avg loss: 0.523: 100%|██████████| 118/118 [00:02<00:00, 55.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set:  Avg loss: 0.460, Accuracy: 8794/10000 (87.9%)\n",
            "\n",
            "Completed in  13.93s\n"
          ]
        }
      ],
      "source": [
        "def train(model: MLP, train_loader: DataLoader, optimizer: SGD, epoch: int, train_loss_list: list | None = None):\n",
        "    print(f\"Epoch: {epoch}\")\n",
        "    progress_bar = tqdm(train_loader)\n",
        "    for data, target in progress_bar:\n",
        "        data, target = Tensor(data.numpy()), Tensor(target.numpy())\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = cross_entropy(output, target).sum() / len(output)\n",
        "        loss.backward()\n",
        "        progress_bar.set_description(f\"Train set: Avg loss: {loss.item():.3f}\")\n",
        "        optimizer.step()\n",
        "        if train_loss_list is not None:\n",
        "            train_loss_list.append(loss.item())\n",
        "\n",
        "\n",
        "def test(model: MLP, test_loader: DataLoader, test_accuracy_list: list | None = None):\n",
        "    test_loss = 0\n",
        "    test_accuracy = 0\n",
        "    with NoGrad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = Tensor(data.numpy()), Tensor(target.numpy())\n",
        "            output: Tensor = model(data)\n",
        "            test_loss += cross_entropy(output, target).sum().item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            test_accuracy += (pred == target.reshape(pred.shape)).sum().item()\n",
        "    n_data = len(test_loader.dataset)\n",
        "    test_loss /= n_data\n",
        "    print(f\"Test set:  Avg loss: {test_loss:.3f}, Accuracy: {test_accuracy}/{n_data} ({test_accuracy / n_data:.1%})\")\n",
        "    if test_accuracy_list is not None:\n",
        "        test_accuracy_list.append(test_accuracy / n_data)\n",
        "\n",
        "\n",
        "num_epochs = 5\n",
        "model = MLP()\n",
        "start = time.time()\n",
        "train_loss_list = []\n",
        "test_accuracy_list = []\n",
        "optimizer = SGD(model.parameters(), 0.01)\n",
        "for epoch in range(num_epochs):\n",
        "    train(model, train_loader, optimizer, epoch, train_loss_list)\n",
        "    test(model, test_loader, test_accuracy_list)\n",
        "    optimizer.step()\n",
        "print(f\"\\nCompleted in {time.time() - start: .2f}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "UJk3UrrhqDjz",
        "outputId": "024dc767-b056-4d8d-e179-75cb2c585e4c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"8ad7b3f1-597a-4990-874a-48e1994159f0\" class=\"plotly-graph-div\" style=\"height:525px; width:800px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"8ad7b3f1-597a-4990-874a-48e1994159f0\")) {                    Plotly.newPlot(                        \"8ad7b3f1-597a-4990-874a-48e1994159f0\",                        [{\"name\":\"Cross entropy loss\",\"x\":[0.0,0.008488964346349746,0.01697792869269949,0.025466893039049237,0.03395585738539898,0.042444821731748725,0.050933786078098474,0.05942275042444822,0.06791171477079797,0.07640067911714771,0.08488964346349745,0.0933786078098472,0.10186757215619695,0.11035653650254669,0.11884550084889645,0.1273344651952462,0.13582342954159593,0.14431239388794567,0.15280135823429541,0.16129032258064516,0.1697792869269949,0.17826825127334467,0.1867572156196944,0.19524617996604415,0.2037351443123939,0.21222410865874364,0.22071307300509338,0.22920203735144312,0.2376910016977929,0.24617996604414263,0.2546689303904924,0.2631578947368421,0.27164685908319186,0.28013582342954163,0.28862478777589134,0.2971137521222411,0.30560271646859083,0.3140916808149406,0.3225806451612903,0.3310696095076401,0.3395585738539898,0.34804753820033957,0.35653650254668934,0.36502546689303905,0.3735144312393888,0.38200339558573854,0.3904923599320883,0.398981324278438,0.4074702886247878,0.41595925297113756,0.4244482173174873,0.43293718166383705,0.44142614601018676,0.44991511035653653,0.45840407470288624,0.466893039049236,0.4753820033955858,0.4838709677419355,0.49235993208828527,0.500848896434635,0.5093378607809848,0.5178268251273345,0.5263157894736842,0.534804753820034,0.5432937181663837,0.5517826825127334,0.5602716468590833,0.568760611205433,0.5772495755517827,0.5857385398981324,0.5942275042444822,0.6027164685908319,0.6112054329371817,0.6196943972835315,0.6281833616298812,0.6366723259762309,0.6451612903225806,0.6536502546689305,0.6621392190152802,0.6706281833616299,0.6791171477079796,0.6876061120543294,0.6960950764006791,0.7045840407470289,0.7130730050933787,0.7215619694397284,0.7300509337860781,0.7385398981324278,0.7470288624787776,0.7555178268251274,0.7640067911714771,0.7724957555178269,0.7809847198641766,0.7894736842105263,0.797962648556876,0.8064516129032259,0.8149405772495756,0.8234295415959253,0.8319185059422751,0.8404074702886248,0.8488964346349746,0.8573853989813243,0.8658743633276741,0.8743633276740238,0.8828522920203735,0.8913412563667233,0.8998302207130731,0.9083191850594228,0.9168081494057725,0.9252971137521223,0.933786078098472,0.9422750424448217,0.9507640067911716,0.9592529711375213,0.967741935483871,0.9762308998302207,0.9847198641765705,0.9932088285229203,1.00169779286927,1.0101867572156198,1.0186757215619695,1.0271646859083192,1.035653650254669,1.0441426146010186,1.0526315789473684,1.0611205432937183,1.069609507640068,1.0780984719864177,1.0865874363327674,1.0950764006791172,1.1035653650254669,1.1120543293718166,1.1205432937181665,1.1290322580645162,1.137521222410866,1.1460101867572157,1.1544991511035654,1.162988115449915,1.1714770797962648,1.1799660441426147,1.1884550084889645,1.1969439728353142,1.2054329371816639,1.2139219015280136,1.2224108658743633,1.230899830220713,1.239388794567063,1.2478777589134127,1.2563667232597624,1.2648556876061121,1.2733446519524618,1.2818336162988115,1.2903225806451613,1.2988115449915112,1.307300509337861,1.3157894736842106,1.3242784380305603,1.33276740237691,1.3412563667232598,1.3497453310696095,1.3582342954159592,1.3667232597623091,1.3752122241086588,1.3837011884550086,1.3921901528013583,1.400679117147708,1.4091680814940577,1.4176570458404074,1.4261460101867574,1.434634974533107,1.4431239388794568,1.4516129032258065,1.4601018675721562,1.468590831918506,1.4770797962648556,1.4855687606112056,1.4940577249575553,1.502546689303905,1.5110356536502547,1.5195246179966044,1.5280135823429541,1.5365025466893039,1.5449915110356538,1.5534804753820035,1.5619694397283532,1.570458404074703,1.5789473684210527,1.5874363327674024,1.595925297113752,1.604414261460102,1.6129032258064517,1.6213921901528014,1.6298811544991512,1.6383701188455009,1.6468590831918506,1.6553480475382003,1.6638370118845502,1.6723259762309,1.6808149405772497,1.6893039049235994,1.697792869269949,1.7062818336162988,1.7147707979626485,1.7232597623089985,1.7317487266553482,1.740237691001698,1.7487266553480476,1.7572156196943973,1.765704584040747,1.7741935483870968,1.7826825127334467,1.7911714770797964,1.7996604414261461,1.8081494057724958,1.8166383701188455,1.8251273344651953,1.833616298811545,1.842105263157895,1.8505942275042446,1.8590831918505943,1.867572156196944,1.8760611205432938,1.8845500848896435,1.8930390492359932,1.9015280135823431,1.9100169779286928,1.9185059422750426,1.9269949066213923,1.935483870967742,1.9439728353140917,1.9524617996604414,1.9609507640067911,1.969439728353141,1.9779286926994908,1.9864176570458405,1.9949066213921902,2.00339558573854,2.01188455008489,2.0203735144312396,2.0288624787775893,2.037351443123939,2.0458404074702887,2.0543293718166384,2.062818336162988,2.071307300509338,2.0797962648556876,2.0882852292020373,2.096774193548387,2.1052631578947367,2.113752122241087,2.1222410865874366,2.1307300509337863,2.139219015280136,2.1477079796264857,2.1561969439728355,2.164685908319185,2.173174872665535,2.1816638370118846,2.1901528013582343,2.198641765704584,2.2071307300509337,2.2156196943972835,2.224108658743633,2.232597623089983,2.241086587436333,2.2495755517826828,2.2580645161290325,2.266553480475382,2.275042444821732,2.2835314091680816,2.2920203735144313,2.300509337860781,2.3089983022071308,2.3174872665534805,2.32597623089983,2.33446519524618,2.3429541595925296,2.3514431239388793,2.3599320882852295,2.368421052631579,2.376910016977929,2.3853989813242786,2.3938879456706283,2.402376910016978,2.4108658743633278,2.4193548387096775,2.427843803056027,2.436332767402377,2.4448217317487266,2.4533106960950763,2.461799660441426,2.4702886247877758,2.478777589134126,2.4872665534804757,2.4957555178268254,2.504244482173175,2.512733446519525,2.5212224108658745,2.5297113752122242,2.538200339558574,2.5466893039049237,2.5551782682512734,2.563667232597623,2.572156196943973,2.5806451612903225,2.5891341256366722,2.5976230899830224,2.606112054329372,2.614601018675722,2.6230899830220715,2.6315789473684212,2.640067911714771,2.6485568760611207,2.6570458404074704,2.66553480475382,2.67402376910017,2.6825127334465195,2.6910016977928692,2.699490662139219,2.7079796264855687,2.7164685908319184,2.7249575551782685,2.7334465195246183,2.741935483870968,2.7504244482173177,2.7589134125636674,2.767402376910017,2.775891341256367,2.7843803056027165,2.7928692699490663,2.801358234295416,2.8098471986417657,2.8183361629881154,2.826825127334465,2.835314091680815,2.843803056027165,2.8522920203735147,2.8607809847198644,2.869269949066214,2.877758913412564,2.8862478777589136,2.8947368421052633,2.903225806451613,2.9117147707979627,2.9202037351443124,2.928692699490662,2.937181663837012,2.9456706281833616,2.9541595925297113,2.9626485568760614,2.971137521222411,2.979626485568761,2.9881154499151106,2.9966044142614603,3.00509337860781,3.0135823429541597,3.0220713073005094,3.030560271646859,3.039049235993209,3.0475382003395586,3.0560271646859083,3.064516129032258,3.0730050933786077,3.081494057724958,3.0899830220713076,3.0984719864176573,3.106960950764007,3.1154499151103567,3.1239388794567065,3.132427843803056,3.140916808149406,3.1494057724957556,3.1578947368421053,3.166383701188455,3.1748726655348047,3.1833616298811545,3.191850594227504,3.2003395585738543,3.208828522920204,3.2173174872665538,3.2258064516129035,3.234295415959253,3.242784380305603,3.2512733446519526,3.2597623089983023,3.268251273344652,3.2767402376910018,3.2852292020373515,3.293718166383701,3.302207130730051,3.3106960950764006,3.3191850594227503,3.3276740237691005,3.33616298811545,3.3446519524618,3.3531409168081496,3.3616298811544993,3.370118845500849,3.3786078098471988,3.3870967741935485,3.395585738539898,3.404074702886248,3.4125636672325976,3.4210526315789473,3.429541595925297,3.4380305602716468,3.446519524617997,3.4550084889643466,3.4634974533106964,3.471986417657046,3.480475382003396,3.4889643463497455,3.497453310696095,3.505942275042445,3.5144312393887946,3.5229202037351444,3.531409168081494,3.539898132427844,3.5483870967741935,3.556876061120543,3.5653650254668934,3.573853989813243,3.582342954159593,3.5908319185059425,3.5993208828522922,3.607809847198642,3.6162988115449917,3.6247877758913414,3.633276740237691,3.641765704584041,3.6502546689303905,3.6587436332767402,3.66723259762309,3.6757215619694397,3.68421052631579,3.6926994906621395,3.7011884550084893,3.709677419354839,3.7181663837011887,3.7266553480475384,3.735144312393888,3.743633276740238,3.7521222410865875,3.7606112054329373,3.769100169779287,3.7775891341256367,3.7860780984719864,3.794567062818336,3.8030560271646863,3.811544991511036,3.8200339558573857,3.8285229202037354,3.837011884550085,3.845500848896435,3.8539898132427846,3.8624787775891343,3.870967741935484,3.8794567062818337,3.8879456706281834,3.896434634974533,3.904923599320883,3.9134125636672326,3.9219015280135823,3.9303904923599324,3.938879456706282,3.947368421052632,3.9558573853989816,3.9643463497453313,3.972835314091681,3.9813242784380307,3.9898132427843804,3.99830220713073,4.00679117147708,4.01528013582343,4.02376910016978,4.032258064516129,4.040747028862479,4.049235993208828,4.057724957555179,4.066213921901528,4.074702886247878,4.083191850594227,4.0916808149405774,4.100169779286927,4.108658743633277,4.117147707979627,4.125636672325976,4.1341256366723265,4.142614601018676,4.151103565365026,4.159592529711375,4.168081494057725,4.176570458404075,4.185059422750425,4.193548387096774,4.202037351443124,4.2105263157894735,4.219015280135824,4.227504244482174,4.235993208828523,4.244482173174873,4.2529711375212225,4.261460101867573,4.269949066213922,4.278438030560272,4.286926994906621,4.2954159592529715,4.303904923599321,4.312393887945671,4.32088285229202,4.32937181663837,4.33786078098472,4.34634974533107,4.35483870967742,4.363327674023769,4.371816638370119,4.380305602716469,4.388794567062819,4.397283531409168,4.405772495755518,4.4142614601018675,4.422750424448218,4.431239388794567,4.439728353140917,4.448217317487266,4.4567062818336165,4.465195246179966,4.473684210526316,4.482173174872666,4.490662139219015,4.4991511035653655,4.507640067911715,4.516129032258065,4.524617996604414,4.533106960950764,4.541595925297114,4.550084889643464,4.558573853989813,4.567062818336163,4.5755517826825125,4.584040747028863,4.592529711375213,4.601018675721562,4.609507640067912,4.6179966044142615,4.626485568760612,4.634974533106961,4.643463497453311,4.65195246179966,4.6604414261460105,4.66893039049236,4.67741935483871,4.685908319185059,4.694397283531409,4.702886247877759,4.711375212224109,4.719864176570459,4.728353140916808,4.736842105263158,4.745331069609508,4.753820033955858,4.762308998302207,4.770797962648557,4.7792869269949065,4.787775891341257,4.796264855687606,4.804753820033956,4.813242784380305,4.8217317487266556,4.830220713073006,4.838709677419355,4.847198641765705,4.855687606112054,4.864176570458405,4.872665534804754,4.881154499151104,4.889643463497453,4.898132427843803,4.906621392190153,4.915110356536503,4.923599320882852,4.932088285229202,4.9405772495755516,4.949066213921902,4.957555178268252,4.966044142614601,4.974533106960951,4.983022071307301,4.991511035653651,5.0],\"y\":[2.3177671432495117,2.303535223007202,2.319876194000244,2.3154730796813965,2.307156562805176,2.3038127422332764,2.302155017852783,2.297121047973633,2.298686981201172,2.298323154449463,2.291994094848633,2.289134979248047,2.290827751159668,2.293710231781006,2.2937092781066895,2.2962350845336914,2.281921863555908,2.2889699935913086,2.2746589183807373,2.280656337738037,2.2868728637695312,2.2708868980407715,2.2692337036132812,2.2692699432373047,2.2836499214172363,2.2685635089874268,2.270796775817871,2.265509605407715,2.260850667953491,2.25313401222229,2.2659969329833984,2.253263235092163,2.2546048164367676,2.250788688659668,2.2523717880249023,2.2526535987854004,2.2471580505371094,2.2361717224121094,2.254627227783203,2.239593744277954,2.2427148818969727,2.240959644317627,2.238093852996826,2.2232210636138916,2.2267026901245117,2.230806350708008,2.2164855003356934,2.22953462600708,2.2209863662719727,2.217221975326538,2.220390558242798,2.220815658569336,2.204557418823242,2.196138381958008,2.1987290382385254,2.2016310691833496,2.2020087242126465,2.1989200115203857,2.201028823852539,2.2004432678222656,2.1850407123565674,2.2100327014923096,2.1838860511779785,2.177204132080078,2.1936469078063965,2.182516098022461,2.1723055839538574,2.171386480331421,2.1737825870513916,2.1731605529785156,2.1445350646972656,2.153135061264038,2.173703193664551,2.1622912883758545,2.1512134075164795,2.1363272666931152,2.147737741470337,2.1449759006500244,2.1332767009735107,2.1177799701690674,2.114712953567505,2.118699312210083,2.1135168075561523,2.100663900375366,2.108546257019043,2.1212995052337646,2.114197254180908,2.092432737350464,2.097688674926758,2.08357834815979,2.089367628097534,2.0872421264648438,2.072788953781128,2.0745973587036133,2.0682568550109863,2.0603127479553223,2.0598273277282715,2.0792582035064697,2.0690746307373047,2.0345406532287598,2.0499002933502197,2.0261974334716797,2.023745059967041,2.0231831073760986,2.022531032562256,2.018542766571045,2.015230655670166,2.0262672901153564,1.9905147552490234,2.0083706378936768,1.9830663204193115,1.986285924911499,2.0026934146881104,2.00370454788208,1.9675929546356201,1.959256649017334,1.991140604019165,2.0336804389953613,1.974399447441101,1.9703114032745361,1.9362819194793701,1.9410011768341064,1.9631145000457764,1.9412628412246704,1.9413235187530518,1.9029686450958252,1.9194386005401611,1.9125699996948242,1.9235479831695557,1.8942805528640747,1.8593519926071167,1.876461148262024,1.8935697078704834,1.8702588081359863,1.886505365371704,1.8484854698181152,1.8617725372314453,1.8455618619918823,1.8542324304580688,1.8590738773345947,1.796723484992981,1.8457660675048828,1.8123055696487427,1.8012819290161133,1.8181662559509277,1.8026669025421143,1.7892653942108154,1.8126837015151978,1.7517476081848145,1.78409743309021,1.7541508674621582,1.7335295677185059,1.7426728010177612,1.7663480043411255,1.742743730545044,1.7253917455673218,1.7108936309814453,1.756160020828247,1.7408379316329956,1.690854787826538,1.714897632598877,1.647976279258728,1.688746452331543,1.6747045516967773,1.6760042905807495,1.6926591396331787,1.6737295389175415,1.6828376054763794,1.614495038986206,1.6198418140411377,1.6061432361602783,1.6122925281524658,1.5980794429779053,1.5888383388519287,1.5782558917999268,1.5748341083526611,1.5752782821655273,1.5656123161315918,1.5396088361740112,1.5582654476165771,1.5523264408111572,1.5825822353363037,1.500467300415039,1.5129406452178955,1.5229930877685547,1.5201761722564697,1.515634536743164,1.4957826137542725,1.495610237121582,1.4720520973205566,1.4606587886810303,1.4831393957138062,1.474472999572754,1.4815118312835693,1.466650366783142,1.4619388580322266,1.4383782148361206,1.4676249027252197,1.436195731163025,1.4363350868225098,1.406424880027771,1.4025689363479614,1.36688232421875,1.390571117401123,1.3875102996826172,1.4031569957733154,1.377281904220581,1.3639085292816162,1.369344711303711,1.3469637632369995,1.2951592206954956,1.380014419555664,1.3256211280822754,1.3467559814453125,1.3381425142288208,1.3377007246017456,1.2989294528961182,1.331078290939331,1.2459115982055664,1.279244303703308,1.2658271789550781,1.2955501079559326,1.3353830575942993,1.308130145072937,1.2510788440704346,1.2794455289840698,1.1942873001098633,1.2752970457077026,1.2037444114685059,1.1923390626907349,1.2032569646835327,1.183516502380371,1.2278521060943604,1.2200196981430054,1.2162415981292725,1.199152946472168,1.1753989458084106,1.1373331546783447,1.0949764251708984,1.1759190559387207,1.1923155784606934,1.1744918823242188,1.1265044212341309,1.1282236576080322,1.128849983215332,1.1328932046890259,1.1426998376846313,1.0887173414230347,1.132474422454834,1.0779213905334473,1.083338975906372,1.0999598503112793,1.0869171619415283,1.1111116409301758,1.1041975021362305,1.0691213607788086,1.0642679929733276,1.045137882232666,1.0262577533721924,1.0386388301849365,1.0145727396011353,0.9770951867103577,1.0082831382751465,1.053436040878296,1.0099573135375977,0.9968018531799316,0.9864272475242615,1.0279457569122314,1.0424203872680664,1.0313735008239746,1.0026702880859375,0.9860100746154785,1.0068225860595703,1.0438950061798096,0.9713717699050903,1.0135853290557861,0.9616442918777466,0.9213228225708008,0.9615671634674072,0.9545618295669556,0.958469033241272,0.9467654228210449,0.9403666257858276,0.9408406615257263,0.9267956018447876,0.9555373191833496,0.9443590641021729,0.9008976817131042,0.9979004859924316,0.8826594352722168,0.8949685096740723,0.9368057250976562,0.9302949905395508,0.9165112376213074,0.8653074502944946,0.9136549234390259,0.861201822757721,0.8695088028907776,0.9122968912124634,0.9129246473312378,0.8566306829452515,0.8369262218475342,0.8740672469139099,0.8817833662033081,0.8682168126106262,0.8918626308441162,0.8611887693405151,0.8889130353927612,0.8851984739303589,0.8708724975585938,0.8215968012809753,0.9063428044319153,0.8578205108642578,0.8358734250068665,0.8677456378936768,0.8253685235977173,0.8631256818771362,0.8679278492927551,0.8047975301742554,0.8641785383224487,0.8305156826972961,0.7591225504875183,0.7946085333824158,0.852512538433075,0.7676088213920593,0.8287031650543213,0.8045006990432739,0.8695032596588135,0.8113871812820435,0.8236171007156372,0.7962435483932495,0.7996478080749512,0.7804977893829346,0.7921757698059082,0.7592605352401733,0.7856310606002808,0.7948851585388184,0.8418871760368347,0.786883533000946,0.7730406522750854,0.8149874210357666,0.7164623141288757,0.742509663105011,0.7490357160568237,0.7825990915298462,0.7622712850570679,0.7933360934257507,0.7884540557861328,0.7243462204933167,0.7378419041633606,0.7474139332771301,0.7823307514190674,0.7056926488876343,0.6729306578636169,0.7766108512878418,0.7379714250564575,0.6749484539031982,0.6987825632095337,0.7506582736968994,0.745855450630188,0.7183996438980103,0.7078009247779846,0.7490898966789246,0.7195682525634766,0.7363389134407043,0.7313008904457092,0.6945581436157227,0.6679689884185791,0.7518120408058167,0.6844308972358704,0.7004823684692383,0.7676286697387695,0.7012404799461365,0.6403461694717407,0.7261695861816406,0.6902409791946411,0.6775141954421997,0.6742855310440063,0.6733013391494751,0.6895992755889893,0.6872457265853882,0.7110446691513062,0.6841205358505249,0.7011545896530151,0.6536421775817871,0.7357137203216553,0.6603277921676636,0.6564502716064453,0.6977103352546692,0.6813601851463318,0.6699941158294678,0.6867520809173584,0.6834269762039185,0.6740374565124512,0.6545891761779785,0.5941759943962097,0.6864402294158936,0.6500341296195984,0.5995597839355469,0.6340494155883789,0.6585057973861694,0.6529862880706787,0.6676178574562073,0.6118042469024658,0.6353304386138916,0.7100636959075928,0.5976217985153198,0.6033994555473328,0.616693377494812,0.6723291277885437,0.6759514212608337,0.6420997381210327,0.5657764673233032,0.6896184086799622,0.5918219685554504,0.6055423617362976,0.6726373434066772,0.6498974561691284,0.6320350170135498,0.611484169960022,0.615321159362793,0.6223608255386353,0.6255704164505005,0.602392315864563,0.6230456829071045,0.6084474325180054,0.6030331254005432,0.612298846244812,0.6298890709877014,0.6372408866882324,0.5708783268928528,0.6491258144378662,0.6272530555725098,0.6130460500717163,0.6218396425247192,0.5976907014846802,0.618362307548523,0.5890517234802246,0.542481005191803,0.6137248277664185,0.6600096821784973,0.6004270911216736,0.5727986693382263,0.5603855848312378,0.5468245148658752,0.6080280542373657,0.6076214909553528,0.6243755221366882,0.6465156078338623,0.6073443293571472,0.6351146101951599,0.5769205689430237,0.5782684087753296,0.6012208461761475,0.5613752007484436,0.593808650970459,0.570581316947937,0.6284028887748718,0.5811001658439636,0.554932177066803,0.5740644931793213,0.5665327310562134,0.5376151204109192,0.5460377931594849,0.5943631529808044,0.5459054708480835,0.5645673274993896,0.5264374017715454,0.5309978723526001,0.5831180810928345,0.5743666887283325,0.5588163137435913,0.5185221433639526,0.5641570687294006,0.5487233400344849,0.5414994955062866,0.5693678259849548,0.5867244005203247,0.5588819980621338,0.5891944169998169,0.5456039905548096,0.5576201677322388,0.5656635761260986,0.5269394516944885,0.5488928556442261,0.5295201539993286,0.556612491607666,0.5579182505607605,0.5329858064651489,0.5438108444213867,0.5604385733604431,0.5348917245864868,0.5478363037109375,0.5132638216018677,0.5524308085441589,0.5527634620666504,0.5911906957626343,0.566874623298645,0.5206300020217896,0.5476405620574951,0.5712763071060181,0.5861588716506958,0.515859067440033,0.47093161940574646,0.5812276601791382,0.5578262805938721,0.5250232219696045,0.5417875051498413,0.49126705527305603,0.5517951250076294,0.5452521443367004,0.580039381980896,0.5814173221588135,0.5281194448471069,0.4682666063308716,0.6217644214630127,0.5329422950744629,0.5098540782928467,0.49606412649154663,0.5137243270874023,0.5233820676803589,0.5210479497909546,0.4992797374725342,0.5319956541061401,0.4419918358325958,0.542444109916687,0.5326535701751709,0.5257635116577148,0.49194592237472534,0.49030357599258423,0.45354336500167847,0.4897744655609131,0.5047693252563477,0.49159902334213257,0.5019582509994507,0.5442163944244385,0.5142374038696289,0.47193169593811035,0.4732919931411743,0.5354259014129639,0.4959886968135834,0.521472692489624,0.547785758972168,0.489712655544281,0.4951232969760895,0.48033562302589417,0.4950503706932068,0.5140247344970703,0.47634637355804443,0.464708149433136,0.5123791694641113,0.5121350288391113,0.45006877183914185,0.5601116418838501,0.5083978772163391,0.4810832738876343,0.49450618028640747,0.5440659523010254,0.503359854221344,0.46145081520080566,0.49416399002075195,0.5246511101722717,0.451515793800354,0.5261014103889465,0.4430653154850006,0.46407759189605713,0.4876345098018646,0.467435359954834,0.48432424664497375,0.46450138092041016,0.5058648586273193,0.4400654137134552,0.5266239643096924,0.46643298864364624,0.5368340611457825,0.5210506916046143,0.5231643915176392,0.4505239725112915,0.4667062759399414,0.49788519740104675,0.49072542786598206,0.4577432870864868,0.4828835427761078,0.5044114589691162,0.4840092658996582,0.5188013315200806,0.4922349154949188,0.504510223865509,0.48098212480545044,0.5233851075172424],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"name\":\"Test accuracy\",\"x\":[0.0,1.25,2.5,3.75,5.0],\"y\":[0.5416,0.75,0.8353,0.8643,0.8794],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y2\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,0.94],\"title\":{\"text\":\"Batches seen\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Cross entropy loss\"}},\"yaxis2\":{\"anchor\":\"x\",\"overlaying\":\"y\",\"side\":\"right\",\"title\":{\"text\":\"Test accuracy\"},\"range\":[0,1]},\"hovermode\":\"x unified\",\"title\":{\"text\":\"MLP training on MNIST from scratch!\"},\"width\":800},                        {\"displaylogo\": false, \"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('8ad7b3f1-597a-4990-874a-48e1994159f0');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "line(\n",
        "    [train_loss_list, test_accuracy_list],\n",
        "    x_max=num_epochs,\n",
        "    yaxis2_range=[0, 1],\n",
        "    use_secondary_yaxis=True,\n",
        "    labels={\"x\": \"Batches seen\", \"y1\": \"Cross entropy loss\", \"y2\": \"Test accuracy\"},\n",
        "    title=\"MLP training on MNIST from scratch!\",\n",
        "    width=800,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1umGEGz9qDjz"
      },
      "source": [
        "Note - this training loop (if done correctly) will look to the one we used in earlier sections is that we're using SGD rather than Adam. You can try adapting your Adam code from the previous day's exercises, and get the same results as you have in earlier sections.\n",
        "\n",
        "If it works then congratulations - you've implemented a fully-functional autograd system!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BEa1ZScqDjz"
      },
      "source": [
        "# Bonus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNcG6PJZqDjz"
      },
      "source": [
        "Congratulations on finishing the day's main content! Here are a few more bonus things for you to explore."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Znr1cLJDqDjz"
      },
      "source": [
        "### In-Place Operation Warnings\n",
        "\n",
        "The most severe issue with our current system is that it can silently compute the wrong gradients when in-place operations are used. Have a look at how [PyTorch handles it](https://pytorch.org/docs/stable/notes/autograd.html#in-place-operations-with-autograd) and implement a similar system yourself so that it either computes the right gradients, or raises a warning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ofeE1d_qDjz"
      },
      "source": [
        "### In-Place `ReLU`\n",
        "\n",
        "Instead of implementing ReLU in terms of maximum, implement your own forward and backward functions that support `inplace=True`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MwT7wwnqDjz"
      },
      "source": [
        "### Backward for `einsum`\n",
        "\n",
        "Write the backward pass for your equivalent of `torch.einsum`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIMoY2sXqDj0"
      },
      "source": [
        "### Reuse of Module during forward\n",
        "\n",
        "Consider the following MLP, where the same `nn.ReLU` instance is used twice in the forward pass. Without running the code, explain whether this works correctly or not with reference to the specifics of your implementation.\n",
        "\n",
        "```python\n",
        "class MyModule(Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear1 = Linear(28*28, 64)\n",
        "        self.linear2 = Linear(64, 64)\n",
        "        self.linear3 = Linear(64, 10)\n",
        "        self.relu = ReLU()\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.linear1(x))\n",
        "        x = self.relu(self.linear2(x))\n",
        "        return self.linear3(x)\n",
        "```\n",
        "\n",
        "<details>\n",
        "<summary>Answer (what you should find)</summary>\n",
        "\n",
        "This implementation will work correctly.\n",
        "\n",
        "The danger of reusing modules is that you'd be creating a cyclical computational graph (because the same parameters would appear twice), but the `ReLU` module doesn't have any parameters (or any internal state), so this isn't a problem. It's effectively just a wrapper for the `relu` function, and you could replace `self.relu` with applying the `relu` function directly without changing the model's behaviour.\n",
        "\n",
        "This is slightly different if we're thinking about adding **hooks** to our model. Hooks are functions that are called during the forward or backward pass, and they can be used to inspect the state of the model during training. We generally want each hook to be associated with a single position in the model, rather than being called at two different points.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Br-gLokYqDj0"
      },
      "source": [
        "### Convolutional layers\n",
        "\n",
        "Now that you've implemented a linear layer, it should be relatively straightforward to take your convolutions code from day 2 and use it to make a convolutional layer. How much better performance do you get on the MNIST task once you replace your first two linear layers with convolutions?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QN5SzUJhqDj0"
      },
      "source": [
        "### ResNet Support\n",
        "\n",
        "Make a list of the features that would need to be implemented to support ResNet inference, and training. It will probably take too long to do all of them, but pick some interesting features to start implementing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kd55xf7IqDj0"
      },
      "source": [
        "### Central Difference Checking\n",
        "\n",
        "Write a function that compares the gradients from your backprop to a central difference method. See [Wikipedia](https://en.wikipedia.org/wiki/Finite_difference) for more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9INJkB9qDj0"
      },
      "source": [
        "### Non-Differentiable Function Support\n",
        "\n",
        "Your `Tensor` does not currently support equivalents of `torch.all`, `torch.any`, `torch.floor`, `torch.less`, etc. which are non-differentiable functions of Tensors. Implement them so that they are usable in computational graphs, but gradients shouldn't flow through them (their contribution is zero)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pckt5FvmqDj0"
      },
      "source": [
        "### Differentiation wrt Keyword Arguments\n",
        "\n",
        "In the real PyTorch, you can sometimes pass tensors as keyword arguments and differentiation will work, as in `t.add(other=t.tensor([3,4]), input=t.tensor([1,2]))`. In other similar looking cases like `t.dot`, it raises an error that the argument must be passed positionally. Decide on a desired behavior in your system and implement and test it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8elOEXtLqDj0"
      },
      "source": [
        "### `torch.stack`\n",
        "\n",
        "So far we've registered a separate backwards for each input argument that could be a Tensor. This is problematic if the function can take any number of tensors like `torch.stack` or `numpy.stack`. Think of and implement the backward function for stack. It may require modification to your other code."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}